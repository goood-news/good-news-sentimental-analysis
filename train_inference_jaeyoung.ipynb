{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_inference_jaeyoung.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"9a28c34e865f48a2a1573dba5ee0c1e8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_479780f7957b4521b6e07808aa7026aa","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_95b16ef6acfb43c1b0b995e41ee6962e","IPY_MODEL_7ab962dc0f7c4889a5211c82e300156e"]}},"479780f7957b4521b6e07808aa7026aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"95b16ef6acfb43c1b0b995e41ee6962e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4a2a9f00600e4fafaba9f2bdbf55e150","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":2244861551,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2244861551,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6a8b505f65c745efbb719541bf494af0"}},"7ab962dc0f7c4889a5211c82e300156e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_38268af62ee943178225e226eb3b8673","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2.24G/2.24G [00:41&lt;00:00, 54.3MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8b9eb099ecd1495594d6a7bd278267ae"}},"4a2a9f00600e4fafaba9f2bdbf55e150":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6a8b505f65c745efbb719541bf494af0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"38268af62ee943178225e226eb3b8673":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8b9eb099ecd1495594d6a7bd278267ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d45d43811dc84561bcf88ea56a15033b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f6b2bcf9ec8f40068c9c83e2f0bd5b82","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f64af2e9460d4ab5b86a8e5c327e4744","IPY_MODEL_6bb1a68c0911478b9c1646a5e5a37284"]}},"f6b2bcf9ec8f40068c9c83e2f0bd5b82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f64af2e9460d4ab5b86a8e5c327e4744":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_bfa2097d9c5544709b681bac0a686d1c","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":513,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":513,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4e179c9d33424d189f571785c3538717"}},"6bb1a68c0911478b9c1646a5e5a37284":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4e67dbb28c484d62b87d7699416d0b17","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 513/513 [00:12&lt;00:00, 39.6B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_10b97ca6aca7451b80950efda4619a7e"}},"bfa2097d9c5544709b681bac0a686d1c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4e179c9d33424d189f571785c3538717":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4e67dbb28c484d62b87d7699416d0b17":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"10b97ca6aca7451b80950efda4619a7e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"34c9d405273b49c0afe747a4fc99e932":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6715d27dee0e4a118f4523b0d95d2a47","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_88648a7cc10c4d9582f0fe51e68a2ffb","IPY_MODEL_e73c8afb120340e5b782f8ecee83151f"]}},"6715d27dee0e4a118f4523b0d95d2a47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"88648a7cc10c4d9582f0fe51e68a2ffb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c8205c97127c4ae397e0a9eb2eec224d","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":5069051,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5069051,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_fd193de0a7414d28ace670a9f0eb6027"}},"e73c8afb120340e5b782f8ecee83151f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_77543ee5a5224d9b8f9b5f0efc7e608b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5.07M/5.07M [00:01&lt;00:00, 2.99MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_370479ef2ba44910b5bebb8bef8d1d9a"}},"c8205c97127c4ae397e0a9eb2eec224d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"fd193de0a7414d28ace670a9f0eb6027":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"77543ee5a5224d9b8f9b5f0efc7e608b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"370479ef2ba44910b5bebb8bef8d1d9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8bfa0f30b63149c3b1dd4134ab56ab37":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fcf74716b6fd4e4dbe2360fac14cff6f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c8684eca7a1d49dfbb58321390e7806f","IPY_MODEL_cbb3bc5ba56e4edaa0e0b8f86f1089e0"]}},"fcf74716b6fd4e4dbe2360fac14cff6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c8684eca7a1d49dfbb58321390e7806f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4154908224554006a46f8b6c62d4c567","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":9096718,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":9096718,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0e58ca2a015c4918a8c6537ac9ba753b"}},"cbb3bc5ba56e4edaa0e0b8f86f1089e0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_61c65e5394af4fbc8c88da93b7ef18a4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9.10M/9.10M [00:01&lt;00:00, 7.71MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1a1385b9a775467f9019a427ae2c0884"}},"4154908224554006a46f8b6c62d4c567":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0e58ca2a015c4918a8c6537ac9ba753b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"61c65e5394af4fbc8c88da93b7ef18a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1a1385b9a775467f9019a427ae2c0884":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"bu09q4eHofsR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624884070315,"user_tz":-540,"elapsed":570,"user":{"displayName":"jessie J","photoUrl":"","userId":"11579515478357014635"}},"outputId":"e8aa6edc-b68c-49df-98d7-03ef5a50c338"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":291},"id":"cv8Q6m_VIrgq","executionInfo":{"status":"ok","timestamp":1624884117315,"user_tz":-540,"elapsed":34731,"user":{"displayName":"jessie J","photoUrl":"","userId":"11579515478357014635"}},"outputId":"b30bf229-b8e3-4b51-f786-d278d6a484f9"},"source":[" # Install colab_ssh on google colab\n"," !pip install colab_ssh --upgrade\n","\t\n"," from colab_ssh import launch_ssh_cloudflared, init_git_cloudflared\n"," launch_ssh_cloudflared(password=\"tjrals\")\n","\t\n"," # Optional: if you want to clone a github repository\n"," # init_git_cloudflared(githubRepositoryUrl)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting colab_ssh\n","  Downloading https://files.pythonhosted.org/packages/74/c0/a2d6cc985d9496968b80203105f20c8d3845effa45dd4cb46c22879f1e44/colab_ssh-0.3.15-py3-none-any.whl\n","Installing collected packages: colab-ssh\n","Successfully installed colab-ssh-0.3.15\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<style>\n","*{\n","\toutline:none;\n","}\n","code{\n","\tdisplay:inline-block;\n","\tpadding:5px 10px;\n","\tbackground: #444;\n","\tborder-radius: 4px;\n","\twhite-space: pre-wrap;\n","\tposition:relative;\n","\tcolor:white;\n","}\n",".copy-code-button{\n","\tfloat:right;\n","\tbackground:#333;\n","\tcolor:white;\n","\tborder: none;\n","\tmargin: 0 0 0 10px;\n","\tcursor: pointer;\n","}\n","p, li{\n","\tmax-width:700px;\n","}\n",".choices{\n","\tdisplay:flex;\n","\tflex: 1 0 auto;\n","}\n",".choice-section{\n","\tborder:solid 1px #555;\n","\tborder-radius: 4px;\n","\tmin-width:300px;\n","\tmargin: 10px 15px 0 0;\n","\tpadding: 0 15px 15px 15px ;\n","}\n",".button{\n","\tpadding: 10px 15px;\n","\tbackground:#333;\n","\tborder-radius: 4px;\n","\tborder:solid 1px #555;\n","\tcolor:white;\n","\tfont-weight:bold;\n","\tcursor:pointer;\n","}\n",".pill{\n","\tpadding:2px 4px;\n","\tborder-radius: 100px;\n","\tbackground-color:#e65858;\n","\tfont-size:12px;\n","\tfont-weight:bold;\n","\tmargin: 0 15px;\n","\tcolor:white;\n","}\n","</style>\n","<details class=\"choice-section\">\n","\t<summary style=\"cursor:pointer\">\n","\t\t<h3 style=\"display:inline-block;margin-top:15px\">⚙️ Client machine configuration<span class=\"pill\">Required</span></h3>\n","\t</summary>\n","\t<p>Don't worry, you only have to do this <b>once per client machine</b>.</p>\n","\t<ol>\n","\t\t<li>Download <a href=\"https://developers.cloudflare.com/argo-tunnel/getting-started/installation\">Cloudflared (Argo Tunnel)</a>, then copy the absolute path to the cloudflare binary</li>\n","\t\t<li>Now, you have to append the following to your SSH config file (usually under ~/.ssh/config):</li>\n","\t</ol>\n","\t<code>Host *.trycloudflare.com\n","\tHostName %h\n","\tUser root\n","\tPort 22\n","\tProxyCommand &ltPUT_THE_ABSOLUTE_CLOUDFLARE_PATH_HERE&gt access ssh --hostname %h\n","\t</code>\n","</details>\n","<div class=\"choices\">\n","\t<div class=\"choice-section\">\n","\t\t<h4>SSH Terminal</h4>\n","\t\t<p>To connect using your terminal, type this command:</p>\n","\t\t<code>ssh ratio-arbitration-declare-deutsche.trycloudflare.com</code>\n","\t</div>\n","\t<div class=\"choice-section\">\n","\t\t<h4>VSCode Remote SSH</h4>\n","\t\t<p>You can also connect with VSCode Remote SSH (Ctrl+Shift+P and type \"Connect to Host...\"). Then, paste the following hostname in the opened command palette:</p>\n","\t\t<code>ratio-arbitration-declare-deutsche.trycloudflare.com</code>\n","\t</div>\n","</div>\n","\n","<script>\n","// Copy any string\n","function fallbackCopyTextToClipboard(text) {\n","  var textArea = document.createElement(\"textarea\");\n","  textArea.value = text;\n","  \n","  // Avoid scrolling to bottom\n","  textArea.style.top = \"0\";\n","  textArea.style.left = \"0\";\n","  textArea.style.position = \"fixed\";\n","\n","  document.body.appendChild(textArea);\n","  textArea.focus();\n","  textArea.select();\n","\n","  try {\n","    var successful = document.execCommand('copy');\n","    var msg = successful ? 'successful' : 'unsuccessful';\n","    console.log('Fallback: Copying text command was ' + msg);\n","  } catch (err) {\n","    console.error('Fallback: Oops, unable to copy', err);\n","  }\n","\n","  document.body.removeChild(textArea);\n","}\n","\n","// Show the copy button with every code tag\n","document.querySelectorAll('code').forEach(function (codeBlock) {\n","\tconst codeToCopy= codeBlock.innerText;\n","\tvar pre = document.createElement('pre');\n","\tpre.innerText = codeToCopy;\n","    var button = document.createElement('button');\n","    button.className = 'copy-code-button';\n","    button.type = 'button';\n","    button.innerText = 'Copy';\n","\tbutton.onclick = function(){\n","\t\tfallbackCopyTextToClipboard(codeToCopy);\n","\t\tbutton.innerText = 'Copied'\n","\t\tsetTimeout(()=>{\n","\t\t\tbutton.innerText = 'Copy'\n","\t\t},2000)\n","\t}\n","\tcodeBlock.children = pre;\n","\tcodeBlock.prepend(button)\n","});\n","</script>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"0BAS9CGxLa7Y","executionInfo":{"status":"ok","timestamp":1624884168020,"user_tz":-540,"elapsed":4958,"user":{"displayName":"jessie J","photoUrl":"","userId":"11579515478357014635"}}},"source":["import numpy as np; np.random.seed(1234)\n","import pandas as pd\n","\n","\n","ntrain = 150000\n","\n","data = pd.read_csv('/content/drive/MyDrive/sentimental_analisis/ratings.txt', sep='\\t', quoting=3)\n","data = pd.DataFrame(np.random.permutation(data))\n","trn, tst = data[:ntrain], data[ntrain:]\n","\n","header = 'id document label'.split()\n","trn.to_csv('/content/drive/MyDrive/sentimental_analisis/ratings_train.txt', sep='\\t', index=False, header=header)\n","tst.to_csv('/content/drive/MyDrive/sentimental_analisis/ratings_test.txt', sep='\\t', index=False, header=header)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZL9m57ewYUky"},"source":["df = pd.read_csv('/content/drive/MyDrive/sentimental_analisis/ratings.txt', sep='\\t')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-3505HNspdWk","executionInfo":{"status":"ok","timestamp":1624884258868,"user_tz":-540,"elapsed":534,"user":{"displayName":"jessie J","photoUrl":"","userId":"11579515478357014635"}}},"source":["# !pip install transformers"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"XCYFIVQmahfz","executionInfo":{"status":"ok","timestamp":1624884266561,"user_tz":-540,"elapsed":2986,"user":{"displayName":"jessie J","photoUrl":"","userId":"11579515478357014635"}}},"source":["import transformers\n","from transformers import AutoTokenizer"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GbqcvY4WzTFV","executionInfo":{"status":"ok","timestamp":1622417930794,"user_tz":-540,"elapsed":40124958,"user":{"displayName":"서석민_T1092","photoUrl":"","userId":"11541575103523287051"}},"outputId":"96a642ec-21dd-411b-e612-e04448eaba36"},"source":["!python /content/drive/MyDrive/sentimental_analisis/good-news-sentimental-analysis/train.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n","Epoch[3/10](5620/14999) || training loss 0.1003 || training accuracy 95.62% || lr 5.317974446451267e-06\n","Epoch[3/10](5640/14999) || training loss 0.2196 || training accuracy 89.38% || lr 5.3169041407452e-06\n","Epoch[3/10](5660/14999) || training loss 0.1221 || training accuracy 94.38% || lr 5.315833835039133e-06\n","Epoch[3/10](5680/14999) || training loss 0.1446 || training accuracy 95.62% || lr 5.314763529333065e-06\n","Epoch[3/10](5700/14999) || training loss 0.1669 || training accuracy 94.38% || lr 5.3136932236269985e-06\n","Epoch[3/10](5720/14999) || training loss 0.1472 || training accuracy 93.75% || lr 5.312622917920931e-06\n","Epoch[3/10](5740/14999) || training loss 0.1431 || training accuracy 93.75% || lr 5.311552612214863e-06\n","Epoch[3/10](5760/14999) || training loss 0.1804 || training accuracy 92.50% || lr 5.310482306508796e-06\n","Epoch[3/10](5780/14999) || training loss 0.1197 || training accuracy 95.00% || lr 5.30941200080273e-06\n","Epoch[3/10](5800/14999) || training loss 0.1397 || training accuracy 93.12% || lr 5.308341695096662e-06\n","Epoch[3/10](5820/14999) || training loss 0.1967 || training accuracy 92.50% || lr 5.307271389390594e-06\n","Epoch[3/10](5840/14999) || training loss 0.1198 || training accuracy 95.00% || lr 5.306201083684527e-06\n","Epoch[3/10](5860/14999) || training loss 0.0987 || training accuracy 95.00% || lr 5.30513077797846e-06\n","Epoch[3/10](5880/14999) || training loss 0.1676 || training accuracy 93.12% || lr 5.304060472272393e-06\n","Epoch[3/10](5900/14999) || training loss 0.1011 || training accuracy 96.25% || lr 5.302990166566325e-06\n","Epoch[3/10](5920/14999) || training loss 0.2635 || training accuracy 93.12% || lr 5.301919860860258e-06\n","Epoch[3/10](5940/14999) || training loss 0.1269 || training accuracy 95.00% || lr 5.30084955515419e-06\n","Epoch[3/10](5960/14999) || training loss 0.1485 || training accuracy 94.38% || lr 5.299779249448123e-06\n","Epoch[3/10](5980/14999) || training loss 0.1066 || training accuracy 96.88% || lr 5.2987089437420565e-06\n","Epoch[3/10](6000/14999) || training loss 0.1811 || training accuracy 93.75% || lr 5.297638638035989e-06\n","Epoch[3/10](6020/14999) || training loss 0.1477 || training accuracy 96.25% || lr 5.296568332329921e-06\n","Epoch[3/10](6040/14999) || training loss 0.2267 || training accuracy 89.38% || lr 5.295498026623854e-06\n","Epoch[3/10](6060/14999) || training loss 0.2171 || training accuracy 91.88% || lr 5.2944277209177876e-06\n","Epoch[3/10](6080/14999) || training loss 0.1315 || training accuracy 91.88% || lr 5.2933574152117195e-06\n","Epoch[3/10](6100/14999) || training loss 0.263 || training accuracy 89.38% || lr 5.292287109505652e-06\n","Epoch[3/10](6120/14999) || training loss 0.1527 || training accuracy 94.38% || lr 5.291216803799585e-06\n","Epoch[3/10](6140/14999) || training loss 0.1262 || training accuracy 93.12% || lr 5.290146498093517e-06\n","Epoch[3/10](6160/14999) || training loss 0.1595 || training accuracy 93.12% || lr 5.289076192387451e-06\n","Epoch[3/10](6180/14999) || training loss 0.1082 || training accuracy 95.62% || lr 5.288005886681383e-06\n","Epoch[3/10](6200/14999) || training loss 0.0946 || training accuracy 96.25% || lr 5.286935580975316e-06\n","Epoch[3/10](6220/14999) || training loss 0.203 || training accuracy 93.75% || lr 5.285865275269248e-06\n","Epoch[3/10](6240/14999) || training loss 0.1907 || training accuracy 94.38% || lr 5.284794969563182e-06\n","Epoch[3/10](6260/14999) || training loss 0.1626 || training accuracy 93.75% || lr 5.2837246638571145e-06\n","Epoch[3/10](6280/14999) || training loss 0.1999 || training accuracy 91.88% || lr 5.282654358151046e-06\n","Epoch[3/10](6300/14999) || training loss 0.1383 || training accuracy 93.75% || lr 5.281584052444979e-06\n","Epoch[3/10](6320/14999) || training loss 0.1656 || training accuracy 93.75% || lr 5.280513746738912e-06\n","Epoch[3/10](6340/14999) || training loss 0.1687 || training accuracy 93.12% || lr 5.279443441032845e-06\n","Epoch[3/10](6360/14999) || training loss 0.121 || training accuracy 94.38% || lr 5.2783731353267775e-06\n","Epoch[3/10](6380/14999) || training loss 0.1303 || training accuracy 94.38% || lr 5.27730282962071e-06\n","Epoch[3/10](6400/14999) || training loss 0.1285 || training accuracy 95.62% || lr 5.276232523914643e-06\n","Epoch[3/10](6420/14999) || training loss 0.08697 || training accuracy 96.88% || lr 5.275162218208575e-06\n","Epoch[3/10](6440/14999) || training loss 0.2118 || training accuracy 92.50% || lr 5.2740919125025086e-06\n","Epoch[3/10](6460/14999) || training loss 0.1696 || training accuracy 91.25% || lr 5.273021606796441e-06\n","Epoch[3/10](6480/14999) || training loss 0.1109 || training accuracy 95.00% || lr 5.271951301090373e-06\n","Epoch[3/10](6500/14999) || training loss 0.1368 || training accuracy 94.38% || lr 5.270880995384306e-06\n","Epoch[3/10](6520/14999) || training loss 0.1836 || training accuracy 95.00% || lr 5.26981068967824e-06\n","Epoch[3/10](6540/14999) || training loss 0.1678 || training accuracy 95.62% || lr 5.2687403839721724e-06\n","Epoch[3/10](6560/14999) || training loss 0.2192 || training accuracy 90.62% || lr 5.267670078266104e-06\n","Epoch[3/10](6580/14999) || training loss 0.1652 || training accuracy 93.75% || lr 5.266599772560037e-06\n","Epoch[3/10](6600/14999) || training loss 0.1823 || training accuracy 93.12% || lr 5.26552946685397e-06\n","Epoch[3/10](6620/14999) || training loss 0.1976 || training accuracy 91.25% || lr 5.264459161147903e-06\n","Epoch[3/10](6640/14999) || training loss 0.1345 || training accuracy 95.62% || lr 5.2633888554418355e-06\n","Epoch[3/10](6660/14999) || training loss 0.2484 || training accuracy 91.25% || lr 5.262318549735768e-06\n","Epoch[3/10](6680/14999) || training loss 0.1605 || training accuracy 93.75% || lr 5.2612482440297e-06\n","Epoch[3/10](6700/14999) || training loss 0.1857 || training accuracy 95.00% || lr 5.260177938323633e-06\n","Epoch[3/10](6720/14999) || training loss 0.1833 || training accuracy 95.00% || lr 5.2591076326175666e-06\n","Epoch[3/10](6740/14999) || training loss 0.1994 || training accuracy 92.50% || lr 5.258037326911499e-06\n","Epoch[3/10](6760/14999) || training loss 0.2175 || training accuracy 93.12% || lr 5.256967021205431e-06\n","Epoch[3/10](6780/14999) || training loss 0.1632 || training accuracy 95.00% || lr 5.255896715499364e-06\n","Epoch[3/10](6800/14999) || training loss 0.1129 || training accuracy 96.88% || lr 5.254826409793298e-06\n","Epoch[3/10](6820/14999) || training loss 0.1286 || training accuracy 93.75% || lr 5.2537561040872296e-06\n","Epoch[3/10](6840/14999) || training loss 0.1332 || training accuracy 95.00% || lr 5.252685798381162e-06\n","Epoch[3/10](6860/14999) || training loss 0.07163 || training accuracy 96.88% || lr 5.251615492675095e-06\n","Epoch[3/10](6880/14999) || training loss 0.1562 || training accuracy 92.50% || lr 5.250545186969027e-06\n","Epoch[3/10](6900/14999) || training loss 0.1284 || training accuracy 94.38% || lr 5.249474881262961e-06\n","Epoch[3/10](6920/14999) || training loss 0.1486 || training accuracy 93.12% || lr 5.2484045755568934e-06\n","Epoch[3/10](6940/14999) || training loss 0.1565 || training accuracy 95.00% || lr 5.247334269850826e-06\n","Epoch[3/10](6960/14999) || training loss 0.2099 || training accuracy 92.50% || lr 5.246263964144758e-06\n","Epoch[3/10](6980/14999) || training loss 0.1569 || training accuracy 92.50% || lr 5.245193658438691e-06\n","Epoch[3/10](7000/14999) || training loss 0.1868 || training accuracy 91.88% || lr 5.2441233527326245e-06\n","Epoch[3/10](7020/14999) || training loss 0.08936 || training accuracy 97.50% || lr 5.2430530470265565e-06\n","Epoch[3/10](7040/14999) || training loss 0.216 || training accuracy 91.88% || lr 5.241982741320489e-06\n","Epoch[3/10](7060/14999) || training loss 0.1121 || training accuracy 96.88% || lr 5.240912435614422e-06\n","Epoch[3/10](7080/14999) || training loss 0.1037 || training accuracy 95.62% || lr 5.239842129908356e-06\n","Epoch[3/10](7100/14999) || training loss 0.1652 || training accuracy 94.38% || lr 5.2387718242022876e-06\n","Epoch[3/10](7120/14999) || training loss 0.232 || training accuracy 91.88% || lr 5.23770151849622e-06\n","Epoch[3/10](7140/14999) || training loss 0.1596 || training accuracy 95.00% || lr 5.236631212790153e-06\n","Epoch[3/10](7160/14999) || training loss 0.08766 || training accuracy 97.50% || lr 5.235560907084085e-06\n","Epoch[3/10](7180/14999) || training loss 0.1563 || training accuracy 94.38% || lr 5.234490601378019e-06\n","Epoch[3/10](7200/14999) || training loss 0.2125 || training accuracy 94.38% || lr 5.2334202956719514e-06\n","Epoch[3/10](7220/14999) || training loss 0.2716 || training accuracy 90.62% || lr 5.232349989965883e-06\n","Epoch[3/10](7240/14999) || training loss 0.126 || training accuracy 96.25% || lr 5.231279684259816e-06\n","Epoch[3/10](7260/14999) || training loss 0.1272 || training accuracy 92.50% || lr 5.23020937855375e-06\n","Epoch[3/10](7280/14999) || training loss 0.2086 || training accuracy 91.88% || lr 5.2291390728476825e-06\n","Epoch[3/10](7300/14999) || training loss 0.246 || training accuracy 92.50% || lr 5.2280687671416144e-06\n","Epoch[3/10](7320/14999) || training loss 0.1219 || training accuracy 95.00% || lr 5.226998461435547e-06\n","Epoch[3/10](7340/14999) || training loss 0.2084 || training accuracy 93.12% || lr 5.22592815572948e-06\n","Epoch[3/10](7360/14999) || training loss 0.1702 || training accuracy 93.75% || lr 5.224857850023413e-06\n","Epoch[3/10](7380/14999) || training loss 0.1641 || training accuracy 93.75% || lr 5.2237875443173455e-06\n","Epoch[3/10](7400/14999) || training loss 0.1699 || training accuracy 93.12% || lr 5.222717238611278e-06\n","Epoch[3/10](7420/14999) || training loss 0.1557 || training accuracy 95.62% || lr 5.22164693290521e-06\n","Epoch[3/10](7440/14999) || training loss 0.1517 || training accuracy 93.75% || lr 5.220576627199143e-06\n","Epoch[3/10](7460/14999) || training loss 0.08199 || training accuracy 97.50% || lr 5.219506321493077e-06\n","Epoch[3/10](7480/14999) || training loss 0.1522 || training accuracy 93.12% || lr 5.218436015787009e-06\n","Epoch[3/10](7500/14999) || training loss 0.1172 || training accuracy 97.50% || lr 5.217365710080941e-06\n","Epoch[3/10](7520/14999) || training loss 0.1454 || training accuracy 92.50% || lr 5.216295404374874e-06\n","Epoch[3/10](7540/14999) || training loss 0.1945 || training accuracy 92.50% || lr 5.215225098668808e-06\n","Epoch[3/10](7560/14999) || training loss 0.1718 || training accuracy 94.38% || lr 5.21415479296274e-06\n","Epoch[3/10](7580/14999) || training loss 0.08385 || training accuracy 96.88% || lr 5.2130844872566724e-06\n","Epoch[3/10](7600/14999) || training loss 0.09241 || training accuracy 95.00% || lr 5.212014181550605e-06\n","Epoch[3/10](7620/14999) || training loss 0.1153 || training accuracy 97.50% || lr 5.210943875844537e-06\n","Epoch[3/10](7640/14999) || training loss 0.08995 || training accuracy 97.50% || lr 5.209873570138471e-06\n","Epoch[3/10](7660/14999) || training loss 0.09666 || training accuracy 97.50% || lr 5.2088032644324035e-06\n","Epoch[3/10](7680/14999) || training loss 0.244 || training accuracy 93.75% || lr 5.207732958726336e-06\n","Epoch[3/10](7700/14999) || training loss 0.08705 || training accuracy 97.50% || lr 5.206662653020268e-06\n","Epoch[3/10](7720/14999) || training loss 0.138 || training accuracy 96.88% || lr 5.205592347314201e-06\n","Epoch[3/10](7740/14999) || training loss 0.1737 || training accuracy 93.12% || lr 5.204522041608135e-06\n","Epoch[3/10](7760/14999) || training loss 0.1089 || training accuracy 95.62% || lr 5.2034517359020665e-06\n","Epoch[3/10](7780/14999) || training loss 0.1543 || training accuracy 94.38% || lr 5.202381430195999e-06\n","Epoch[3/10](7800/14999) || training loss 0.1157 || training accuracy 95.62% || lr 5.201311124489932e-06\n","Epoch[3/10](7820/14999) || training loss 0.1312 || training accuracy 95.00% || lr 5.200240818783866e-06\n","Epoch[3/10](7840/14999) || training loss 0.07912 || training accuracy 97.50% || lr 5.199170513077798e-06\n","Epoch[3/10](7860/14999) || training loss 0.156 || training accuracy 95.00% || lr 5.19810020737173e-06\n","Epoch[3/10](7880/14999) || training loss 0.1924 || training accuracy 93.75% || lr 5.197029901665663e-06\n","Epoch[3/10](7900/14999) || training loss 0.1466 || training accuracy 95.62% || lr 5.195959595959595e-06\n","Epoch[3/10](7920/14999) || training loss 0.2008 || training accuracy 94.38% || lr 5.194889290253529e-06\n","Epoch[3/10](7940/14999) || training loss 0.1388 || training accuracy 94.38% || lr 5.1938189845474615e-06\n","Epoch[3/10](7960/14999) || training loss 0.1546 || training accuracy 94.38% || lr 5.1927486788413934e-06\n","Epoch[3/10](7980/14999) || training loss 0.1533 || training accuracy 93.12% || lr 5.191678373135326e-06\n","Epoch[3/10](8000/14999) || training loss 0.1328 || training accuracy 93.12% || lr 5.190608067429259e-06\n","Epoch[3/10](8020/14999) || training loss 0.1147 || training accuracy 95.62% || lr 5.189537761723193e-06\n","Epoch[3/10](8040/14999) || training loss 0.1197 || training accuracy 95.00% || lr 5.1884674560171245e-06\n","Epoch[3/10](8060/14999) || training loss 0.1576 || training accuracy 95.62% || lr 5.187397150311057e-06\n","Epoch[3/10](8080/14999) || training loss 0.1703 || training accuracy 91.88% || lr 5.18632684460499e-06\n","Epoch[3/10](8100/14999) || training loss 0.1617 || training accuracy 95.00% || lr 5.185256538898923e-06\n","Epoch[3/10](8120/14999) || training loss 0.1946 || training accuracy 93.75% || lr 5.184186233192856e-06\n","Epoch[3/10](8140/14999) || training loss 0.2102 || training accuracy 91.88% || lr 5.183115927486788e-06\n","Epoch[3/10](8160/14999) || training loss 0.1513 || training accuracy 93.12% || lr 5.18204562178072e-06\n","Epoch[3/10](8180/14999) || training loss 0.1686 || training accuracy 93.75% || lr 5.180975316074653e-06\n","Epoch[3/10](8200/14999) || training loss 0.1969 || training accuracy 91.25% || lr 5.179905010368587e-06\n","Epoch[3/10](8220/14999) || training loss 0.1137 || training accuracy 95.00% || lr 5.1788347046625195e-06\n","Epoch[3/10](8240/14999) || training loss 0.1528 || training accuracy 94.38% || lr 5.177764398956451e-06\n","Epoch[3/10](8260/14999) || training loss 0.1382 || training accuracy 95.00% || lr 5.176694093250384e-06\n","Epoch[3/10](8280/14999) || training loss 0.1668 || training accuracy 91.25% || lr 5.175623787544317e-06\n","Epoch[3/10](8300/14999) || training loss 0.2356 || training accuracy 93.12% || lr 5.17455348183825e-06\n","Epoch[3/10](8320/14999) || training loss 0.1904 || training accuracy 93.12% || lr 5.1734831761321825e-06\n","Epoch[3/10](8340/14999) || training loss 0.0993 || training accuracy 97.50% || lr 5.172412870426115e-06\n","Epoch[3/10](8360/14999) || training loss 0.1784 || training accuracy 93.12% || lr 5.171342564720048e-06\n","Epoch[3/10](8380/14999) || training loss 0.16 || training accuracy 93.75% || lr 5.170272259013981e-06\n","Epoch[3/10](8400/14999) || training loss 0.1832 || training accuracy 93.75% || lr 5.169201953307914e-06\n","Epoch[3/10](8420/14999) || training loss 0.1359 || training accuracy 95.62% || lr 5.168131647601846e-06\n","Epoch[3/10](8440/14999) || training loss 0.1005 || training accuracy 95.62% || lr 5.167061341895778e-06\n","Epoch[3/10](8460/14999) || training loss 0.1312 || training accuracy 94.38% || lr 5.165991036189711e-06\n","Epoch[3/10](8480/14999) || training loss 0.1572 || training accuracy 93.75% || lr 5.164920730483645e-06\n","Epoch[3/10](8500/14999) || training loss 0.1414 || training accuracy 94.38% || lr 5.163850424777577e-06\n","Epoch[3/10](8520/14999) || training loss 0.2115 || training accuracy 92.50% || lr 5.162780119071509e-06\n","Epoch[3/10](8540/14999) || training loss 0.2338 || training accuracy 88.12% || lr 5.161709813365442e-06\n","Epoch[3/10](8560/14999) || training loss 0.1522 || training accuracy 95.00% || lr 5.160639507659376e-06\n","Epoch[3/10](8580/14999) || training loss 0.1734 || training accuracy 93.12% || lr 5.159569201953308e-06\n","Epoch[3/10](8600/14999) || training loss 0.1606 || training accuracy 94.38% || lr 5.1584988962472405e-06\n","Epoch[3/10](8620/14999) || training loss 0.1604 || training accuracy 93.12% || lr 5.157428590541173e-06\n","Epoch[3/10](8640/14999) || training loss 0.2331 || training accuracy 91.88% || lr 5.156358284835105e-06\n","Epoch[3/10](8660/14999) || training loss 0.101 || training accuracy 95.00% || lr 5.155287979129039e-06\n","Epoch[3/10](8680/14999) || training loss 0.1559 || training accuracy 94.38% || lr 5.154217673422972e-06\n","Epoch[3/10](8700/14999) || training loss 0.147 || training accuracy 95.00% || lr 5.1531473677169035e-06\n","Epoch[3/10](8720/14999) || training loss 0.1862 || training accuracy 93.12% || lr 5.152077062010836e-06\n","Epoch[3/10](8740/14999) || training loss 0.1362 || training accuracy 93.75% || lr 5.151006756304769e-06\n","Epoch[3/10](8760/14999) || training loss 0.1132 || training accuracy 96.25% || lr 5.149936450598703e-06\n","Epoch[3/10](8780/14999) || training loss 0.1474 || training accuracy 92.50% || lr 5.148866144892635e-06\n","Epoch[3/10](8800/14999) || training loss 0.1315 || training accuracy 94.38% || lr 5.147795839186567e-06\n","Epoch[3/10](8820/14999) || training loss 0.09029 || training accuracy 97.50% || lr 5.1467255334805e-06\n","Epoch[3/10](8840/14999) || training loss 0.1952 || training accuracy 93.75% || lr 5.145655227774432e-06\n","Epoch[3/10](8860/14999) || training loss 0.2179 || training accuracy 88.75% || lr 5.144584922068366e-06\n","Epoch[3/10](8880/14999) || training loss 0.09586 || training accuracy 95.62% || lr 5.1435146163622985e-06\n","Epoch[3/10](8900/14999) || training loss 0.1053 || training accuracy 96.88% || lr 5.14244431065623e-06\n","Epoch[3/10](8920/14999) || training loss 0.1471 || training accuracy 93.75% || lr 5.141374004950163e-06\n","Epoch[3/10](8940/14999) || training loss 0.1743 || training accuracy 91.25% || lr 5.140303699244097e-06\n","Epoch[3/10](8960/14999) || training loss 0.1464 || training accuracy 94.38% || lr 5.13923339353803e-06\n","Epoch[3/10](8980/14999) || training loss 0.1817 || training accuracy 93.12% || lr 5.1381630878319615e-06\n","Epoch[3/10](9000/14999) || training loss 0.2006 || training accuracy 95.00% || lr 5.137092782125894e-06\n","Epoch[3/10](9020/14999) || training loss 0.1541 || training accuracy 95.62% || lr 5.136022476419827e-06\n","Epoch[3/10](9040/14999) || training loss 0.2072 || training accuracy 93.12% || lr 5.13495217071376e-06\n","Epoch[3/10](9060/14999) || training loss 0.1419 || training accuracy 95.00% || lr 5.133881865007693e-06\n","Epoch[3/10](9080/14999) || training loss 0.2209 || training accuracy 93.75% || lr 5.132811559301625e-06\n","Epoch[3/10](9100/14999) || training loss 0.1889 || training accuracy 94.38% || lr 5.131741253595558e-06\n","Epoch[3/10](9120/14999) || training loss 0.1472 || training accuracy 95.62% || lr 5.130670947889491e-06\n","Epoch[3/10](9140/14999) || training loss 0.2322 || training accuracy 91.88% || lr 5.129600642183424e-06\n","Epoch[3/10](9160/14999) || training loss 0.1245 || training accuracy 95.62% || lr 5.1285303364773565e-06\n","Epoch[3/10](9180/14999) || training loss 0.1391 || training accuracy 93.12% || lr 5.127460030771288e-06\n","Epoch[3/10](9200/14999) || training loss 0.1807 || training accuracy 91.25% || lr 5.126389725065221e-06\n","Epoch[3/10](9220/14999) || training loss 0.1112 || training accuracy 96.88% || lr 5.125319419359155e-06\n","Epoch[3/10](9240/14999) || training loss 0.1328 || training accuracy 95.62% || lr 5.124249113653087e-06\n","Epoch[3/10](9260/14999) || training loss 0.1713 || training accuracy 92.50% || lr 5.1231788079470195e-06\n","Epoch[3/10](9280/14999) || training loss 0.1425 || training accuracy 96.25% || lr 5.122108502240952e-06\n","Epoch[3/10](9300/14999) || training loss 0.1193 || training accuracy 95.00% || lr 5.121038196534885e-06\n","Epoch[3/10](9320/14999) || training loss 0.199 || training accuracy 93.75% || lr 5.119967890828818e-06\n","Epoch[3/10](9340/14999) || training loss 0.2287 || training accuracy 92.50% || lr 5.118897585122751e-06\n","Epoch[3/10](9360/14999) || training loss 0.1113 || training accuracy 95.00% || lr 5.117827279416683e-06\n","Epoch[3/10](9380/14999) || training loss 0.1716 || training accuracy 93.75% || lr 5.116756973710615e-06\n","Epoch[3/10](9400/14999) || training loss 0.1008 || training accuracy 96.88% || lr 5.115686668004549e-06\n","Epoch[3/10](9420/14999) || training loss 0.1876 || training accuracy 93.12% || lr 5.114616362298482e-06\n","Epoch[3/10](9440/14999) || training loss 0.1842 || training accuracy 91.25% || lr 5.113546056592414e-06\n","Epoch[3/10](9460/14999) || training loss 0.1259 || training accuracy 96.25% || lr 5.112475750886346e-06\n","Epoch[3/10](9480/14999) || training loss 0.1705 || training accuracy 94.38% || lr 5.111405445180279e-06\n","Epoch[3/10](9500/14999) || training loss 0.1174 || training accuracy 95.62% || lr 5.110335139474213e-06\n","Epoch[3/10](9520/14999) || training loss 0.1464 || training accuracy 94.38% || lr 5.109264833768145e-06\n","Epoch[3/10](9540/14999) || training loss 0.2061 || training accuracy 93.12% || lr 5.1081945280620775e-06\n","Epoch[3/10](9560/14999) || training loss 0.1643 || training accuracy 93.75% || lr 5.10712422235601e-06\n","Epoch[3/10](9580/14999) || training loss 0.1941 || training accuracy 94.38% || lr 5.106053916649942e-06\n","Epoch[3/10](9600/14999) || training loss 0.1694 || training accuracy 91.88% || lr 5.104983610943876e-06\n","Epoch[3/10](9620/14999) || training loss 0.1065 || training accuracy 95.62% || lr 5.1039133052378086e-06\n","Epoch[3/10](9640/14999) || training loss 0.1524 || training accuracy 93.75% || lr 5.102842999531741e-06\n","Epoch[3/10](9660/14999) || training loss 0.1521 || training accuracy 95.00% || lr 5.101772693825673e-06\n","Epoch[3/10](9680/14999) || training loss 0.1552 || training accuracy 92.50% || lr 5.100702388119607e-06\n","Epoch[3/10](9700/14999) || training loss 0.1585 || training accuracy 93.75% || lr 5.09963208241354e-06\n","Epoch[3/10](9720/14999) || training loss 0.1877 || training accuracy 92.50% || lr 5.098561776707472e-06\n","Epoch[3/10](9740/14999) || training loss 0.155 || training accuracy 93.12% || lr 5.097491471001404e-06\n","Epoch[3/10](9760/14999) || training loss 0.1043 || training accuracy 95.62% || lr 5.096421165295337e-06\n","Epoch[3/10](9780/14999) || training loss 0.1314 || training accuracy 95.62% || lr 5.09535085958927e-06\n","Epoch[3/10](9800/14999) || training loss 0.1753 || training accuracy 93.75% || lr 5.094280553883203e-06\n","Epoch[3/10](9820/14999) || training loss 0.1802 || training accuracy 91.88% || lr 5.0932102481771355e-06\n","Epoch[3/10](9840/14999) || training loss 0.1963 || training accuracy 91.25% || lr 5.092139942471068e-06\n","Epoch[3/10](9860/14999) || training loss 0.1057 || training accuracy 96.88% || lr 5.091069636765e-06\n","Epoch[3/10](9880/14999) || training loss 0.1559 || training accuracy 94.38% || lr 5.089999331058934e-06\n","Epoch[3/10](9900/14999) || training loss 0.1216 || training accuracy 93.75% || lr 5.0889290253528666e-06\n","Epoch[3/10](9920/14999) || training loss 0.1586 || training accuracy 93.12% || lr 5.0878587196467985e-06\n","Epoch[3/10](9940/14999) || training loss 0.1288 || training accuracy 95.00% || lr 5.086788413940731e-06\n","Epoch[3/10](9960/14999) || training loss 0.2228 || training accuracy 93.75% || lr 5.085718108234665e-06\n","Epoch[3/10](9980/14999) || training loss 0.1469 || training accuracy 93.75% || lr 5.084647802528597e-06\n","Epoch[3/10](10000/14999) || training loss 0.1466 || training accuracy 92.50% || lr 5.0835774968225296e-06\n","Epoch[3/10](10020/14999) || training loss 0.1206 || training accuracy 96.25% || lr 5.082507191116462e-06\n","Epoch[3/10](10040/14999) || training loss 0.1252 || training accuracy 96.88% || lr 5.081436885410395e-06\n","Epoch[3/10](10060/14999) || training loss 0.1406 || training accuracy 94.38% || lr 5.080366579704328e-06\n","Epoch[3/10](10080/14999) || training loss 0.116 || training accuracy 94.38% || lr 5.079296273998261e-06\n","Epoch[3/10](10100/14999) || training loss 0.2203 || training accuracy 91.25% || lr 5.0782259682921934e-06\n","Epoch[3/10](10120/14999) || training loss 0.1894 || training accuracy 93.75% || lr 5.077155662586125e-06\n","Epoch[3/10](10140/14999) || training loss 0.165 || training accuracy 91.88% || lr 5.076085356880059e-06\n","Epoch[3/10](10160/14999) || training loss 0.1539 || training accuracy 94.38% || lr 5.075015051173992e-06\n","Epoch[3/10](10180/14999) || training loss 0.1855 || training accuracy 93.12% || lr 5.073944745467924e-06\n","Epoch[3/10](10200/14999) || training loss 0.1077 || training accuracy 96.25% || lr 5.0728744397618565e-06\n","Epoch[3/10](10220/14999) || training loss 0.1768 || training accuracy 94.38% || lr 5.071804134055789e-06\n","Epoch[3/10](10240/14999) || training loss 0.1908 || training accuracy 91.88% || lr 5.070733828349723e-06\n","Epoch[3/10](10260/14999) || training loss 0.1236 || training accuracy 93.75% || lr 5.069663522643655e-06\n","Epoch[3/10](10280/14999) || training loss 0.1262 || training accuracy 93.75% || lr 5.0685932169375876e-06\n","Epoch[3/10](10300/14999) || training loss 0.2314 || training accuracy 90.62% || lr 5.06752291123152e-06\n","Epoch[3/10](10320/14999) || training loss 0.2574 || training accuracy 91.25% || lr 5.066452605525452e-06\n","Epoch[3/10](10340/14999) || training loss 0.2056 || training accuracy 91.25% || lr 5.065382299819386e-06\n","Epoch[3/10](10360/14999) || training loss 0.1439 || training accuracy 93.75% || lr 5.064311994113319e-06\n","Epoch[3/10](10380/14999) || training loss 0.1176 || training accuracy 95.00% || lr 5.0632416884072514e-06\n","Epoch[3/10](10400/14999) || training loss 0.1155 || training accuracy 94.38% || lr 5.062171382701183e-06\n","Epoch[3/10](10420/14999) || training loss 0.1068 || training accuracy 95.62% || lr 5.061101076995117e-06\n","Epoch[3/10](10440/14999) || training loss 0.1207 || training accuracy 95.62% || lr 5.06003077128905e-06\n","Epoch[3/10](10460/14999) || training loss 0.1684 || training accuracy 93.75% || lr 5.058960465582982e-06\n","Epoch[3/10](10480/14999) || training loss 0.1977 || training accuracy 91.25% || lr 5.0578901598769144e-06\n","Epoch[3/10](10500/14999) || training loss 0.1608 || training accuracy 94.38% || lr 5.056819854170847e-06\n","Epoch[3/10](10520/14999) || training loss 0.1809 || training accuracy 93.75% || lr 5.05574954846478e-06\n","Epoch[3/10](10540/14999) || training loss 0.226 || training accuracy 91.88% || lr 5.054679242758713e-06\n","Epoch[3/10](10560/14999) || training loss 0.1586 || training accuracy 95.62% || lr 5.0536089370526455e-06\n","Epoch[3/10](10580/14999) || training loss 0.2082 || training accuracy 91.25% || lr 5.052538631346578e-06\n","Epoch[3/10](10600/14999) || training loss 0.1752 || training accuracy 93.75% || lr 5.05146832564051e-06\n","Epoch[3/10](10620/14999) || training loss 0.08781 || training accuracy 96.88% || lr 5.050398019934444e-06\n","Epoch[3/10](10640/14999) || training loss 0.1253 || training accuracy 95.62% || lr 5.049327714228377e-06\n","Epoch[3/10](10660/14999) || training loss 0.1192 || training accuracy 94.38% || lr 5.0482574085223086e-06\n","Epoch[3/10](10680/14999) || training loss 0.1053 || training accuracy 96.25% || lr 5.047187102816241e-06\n","Epoch[3/10](10700/14999) || training loss 0.2125 || training accuracy 91.25% || lr 5.046116797110175e-06\n","Epoch[3/10](10720/14999) || training loss 0.1483 || training accuracy 93.12% || lr 5.045046491404107e-06\n","Epoch[3/10](10740/14999) || training loss 0.1373 || training accuracy 95.62% || lr 5.04397618569804e-06\n","Epoch[3/10](10760/14999) || training loss 0.1852 || training accuracy 90.62% || lr 5.0429058799919724e-06\n","Epoch[3/10](10780/14999) || training loss 0.1315 || training accuracy 95.00% || lr 5.041835574285905e-06\n","Epoch[3/10](10800/14999) || training loss 0.1389 || training accuracy 94.38% || lr 5.040765268579838e-06\n","Epoch[3/10](10820/14999) || training loss 0.1085 || training accuracy 95.00% || lr 5.039694962873771e-06\n","Epoch[3/10](10840/14999) || training loss 0.06022 || training accuracy 98.12% || lr 5.0386246571677035e-06\n","Epoch[3/10](10860/14999) || training loss 0.2428 || training accuracy 90.00% || lr 5.0375543514616355e-06\n","Epoch[3/10](10880/14999) || training loss 0.1904 || training accuracy 93.12% || lr 5.036484045755568e-06\n","Epoch[3/10](10900/14999) || training loss 0.1321 || training accuracy 93.75% || lr 5.035413740049502e-06\n","Epoch[3/10](10920/14999) || training loss 0.1687 || training accuracy 93.75% || lr 5.034343434343435e-06\n","Epoch[3/10](10940/14999) || training loss 0.221 || training accuracy 92.50% || lr 5.0332731286373665e-06\n","Epoch[3/10](10960/14999) || training loss 0.1664 || training accuracy 95.00% || lr 5.032202822931299e-06\n","Epoch[3/10](10980/14999) || training loss 0.1877 || training accuracy 91.88% || lr 5.031132517225233e-06\n","Epoch[3/10](11000/14999) || training loss 0.1525 || training accuracy 95.00% || lr 5.030062211519165e-06\n","Epoch[3/10](11020/14999) || training loss 0.1383 || training accuracy 95.00% || lr 5.028991905813098e-06\n","Epoch[3/10](11040/14999) || training loss 0.2043 || training accuracy 93.12% || lr 5.02792160010703e-06\n","Epoch[3/10](11060/14999) || training loss 0.1983 || training accuracy 92.50% || lr 5.026851294400962e-06\n","Epoch[3/10](11080/14999) || training loss 0.1412 || training accuracy 93.75% || lr 5.025780988694896e-06\n","Epoch[3/10](11100/14999) || training loss 0.1741 || training accuracy 93.75% || lr 5.024710682988829e-06\n","Epoch[3/10](11120/14999) || training loss 0.09952 || training accuracy 97.50% || lr 5.0236403772827615e-06\n","Epoch[3/10](11140/14999) || training loss 0.1208 || training accuracy 95.00% || lr 5.0225700715766934e-06\n","Epoch[3/10](11160/14999) || training loss 0.1956 || training accuracy 93.12% || lr 5.021499765870626e-06\n","Epoch[3/10](11180/14999) || training loss 0.1475 || training accuracy 94.38% || lr 5.02042946016456e-06\n","Epoch[3/10](11200/14999) || training loss 0.1554 || training accuracy 92.50% || lr 5.019359154458492e-06\n","Epoch[3/10](11220/14999) || training loss 0.1607 || training accuracy 92.50% || lr 5.0182888487524245e-06\n","Epoch[3/10](11240/14999) || training loss 0.1249 || training accuracy 95.00% || lr 5.017218543046357e-06\n","Epoch[3/10](11260/14999) || training loss 0.1753 || training accuracy 93.75% || lr 5.01614823734029e-06\n","Epoch[3/10](11280/14999) || training loss 0.2421 || training accuracy 91.88% || lr 5.015077931634223e-06\n","Epoch[3/10](11300/14999) || training loss 0.1273 || training accuracy 95.00% || lr 5.014007625928156e-06\n","Epoch[3/10](11320/14999) || training loss 0.1388 || training accuracy 94.38% || lr 5.012937320222088e-06\n","Epoch[3/10](11340/14999) || training loss 0.2097 || training accuracy 92.50% || lr 5.01186701451602e-06\n","Epoch[3/10](11360/14999) || training loss 0.1698 || training accuracy 91.88% || lr 5.010796708809954e-06\n","Epoch[3/10](11380/14999) || training loss 0.1419 || training accuracy 93.75% || lr 5.009726403103887e-06\n","Epoch[3/10](11400/14999) || training loss 0.1496 || training accuracy 93.12% || lr 5.008656097397819e-06\n","Epoch[3/10](11420/14999) || training loss 0.1476 || training accuracy 93.75% || lr 5.007585791691751e-06\n","Epoch[3/10](11440/14999) || training loss 0.1275 || training accuracy 96.88% || lr 5.006515485985685e-06\n","Epoch[3/10](11460/14999) || training loss 0.1761 || training accuracy 95.00% || lr 5.005445180279617e-06\n","Epoch[3/10](11480/14999) || training loss 0.21 || training accuracy 92.50% || lr 5.00437487457355e-06\n","Epoch[3/10](11500/14999) || training loss 0.1574 || training accuracy 94.38% || lr 5.0033045688674825e-06\n","Epoch[3/10](11520/14999) || training loss 0.1345 || training accuracy 95.62% || lr 5.002234263161415e-06\n","Epoch[3/10](11540/14999) || training loss 0.1201 || training accuracy 95.62% || lr 5.001163957455348e-06\n","Epoch[3/10](11560/14999) || training loss 0.2326 || training accuracy 90.00% || lr 5.000093651749281e-06\n","Epoch[3/10](11580/14999) || training loss 0.1095 || training accuracy 95.62% || lr 4.999023346043214e-06\n","Epoch[3/10](11600/14999) || training loss 0.121 || training accuracy 95.00% || lr 4.9979530403371455e-06\n","Epoch[3/10](11620/14999) || training loss 0.144 || training accuracy 95.62% || lr 4.996882734631078e-06\n","Epoch[3/10](11640/14999) || training loss 0.132 || training accuracy 95.00% || lr 4.995812428925012e-06\n","Epoch[3/10](11660/14999) || training loss 0.1365 || training accuracy 95.00% || lr 4.994742123218945e-06\n","Epoch[3/10](11680/14999) || training loss 0.1465 || training accuracy 95.00% || lr 4.993671817512877e-06\n","Epoch[3/10](11700/14999) || training loss 0.2056 || training accuracy 92.50% || lr 4.992601511806809e-06\n","Epoch[3/10](11720/14999) || training loss 0.09002 || training accuracy 95.62% || lr 4.991531206100743e-06\n","Epoch[3/10](11740/14999) || training loss 0.1631 || training accuracy 92.50% || lr 4.990460900394675e-06\n","Epoch[3/10](11760/14999) || training loss 0.2191 || training accuracy 90.62% || lr 4.989390594688608e-06\n","Epoch[3/10](11780/14999) || training loss 0.1956 || training accuracy 93.12% || lr 4.9883202889825405e-06\n","Epoch[3/10](11800/14999) || training loss 0.2386 || training accuracy 91.25% || lr 4.9872499832764724e-06\n","Epoch[3/10](11820/14999) || training loss 0.1295 || training accuracy 95.62% || lr 4.986179677570406e-06\n","Epoch[3/10](11840/14999) || training loss 0.16 || training accuracy 96.25% || lr 4.985109371864339e-06\n","Epoch[3/10](11860/14999) || training loss 0.1847 || training accuracy 94.38% || lr 4.984039066158272e-06\n","Epoch[3/10](11880/14999) || training loss 0.1255 || training accuracy 95.62% || lr 4.9829687604522035e-06\n","Epoch[3/10](11900/14999) || training loss 0.1557 || training accuracy 93.75% || lr 4.981898454746136e-06\n","Epoch[3/10](11920/14999) || training loss 0.1909 || training accuracy 91.88% || lr 4.98082814904007e-06\n","Epoch[3/10](11940/14999) || training loss 0.1888 || training accuracy 93.12% || lr 4.979757843334002e-06\n","Epoch[3/10](11960/14999) || training loss 0.1636 || training accuracy 93.75% || lr 4.978687537627935e-06\n","Epoch[3/10](11980/14999) || training loss 0.1598 || training accuracy 95.00% || lr 4.977617231921867e-06\n","Epoch[3/10](12000/14999) || training loss 0.15 || training accuracy 94.38% || lr 4.9765469262158e-06\n","Epoch[3/10](12020/14999) || training loss 0.1846 || training accuracy 95.62% || lr 4.975476620509733e-06\n","Epoch[3/10](12040/14999) || training loss 0.2093 || training accuracy 91.25% || lr 4.974406314803666e-06\n","Epoch[3/10](12060/14999) || training loss 0.1847 || training accuracy 90.62% || lr 4.9733360090975985e-06\n","Epoch[3/10](12080/14999) || training loss 0.1398 || training accuracy 93.12% || lr 4.97226570339153e-06\n","Epoch[3/10](12100/14999) || training loss 0.1487 || training accuracy 95.62% || lr 4.971195397685464e-06\n","Epoch[3/10](12120/14999) || training loss 0.1263 || training accuracy 95.00% || lr 4.970125091979397e-06\n","Epoch[3/10](12140/14999) || training loss 0.2122 || training accuracy 92.50% || lr 4.969054786273329e-06\n","Epoch[3/10](12160/14999) || training loss 0.1332 || training accuracy 93.75% || lr 4.9679844805672615e-06\n","Epoch[3/10](12180/14999) || training loss 0.1988 || training accuracy 92.50% || lr 4.966914174861194e-06\n","Epoch[3/10](12200/14999) || training loss 0.101 || training accuracy 96.25% || lr 4.965843869155128e-06\n","Epoch[3/10](12220/14999) || training loss 0.2118 || training accuracy 91.25% || lr 4.96477356344906e-06\n","Epoch[3/10](12240/14999) || training loss 0.1605 || training accuracy 95.62% || lr 4.963703257742993e-06\n","Epoch[3/10](12260/14999) || training loss 0.2393 || training accuracy 88.75% || lr 4.962632952036925e-06\n","Epoch[3/10](12280/14999) || training loss 0.07803 || training accuracy 96.88% || lr 4.961562646330858e-06\n","Epoch[3/10](12300/14999) || training loss 0.1664 || training accuracy 91.88% || lr 4.960492340624791e-06\n","Epoch[3/10](12320/14999) || training loss 0.1584 || training accuracy 91.88% || lr 4.959422034918724e-06\n","Epoch[3/10](12340/14999) || training loss 0.1313 || training accuracy 94.38% || lr 4.958351729212656e-06\n","Epoch[3/10](12360/14999) || training loss 0.1302 || training accuracy 96.25% || lr 4.957281423506588e-06\n","Epoch[3/10](12380/14999) || training loss 0.1763 || training accuracy 92.50% || lr 4.956211117800522e-06\n","Epoch[3/10](12400/14999) || training loss 0.1979 || training accuracy 93.12% || lr 4.955140812094455e-06\n","Epoch[3/10](12420/14999) || training loss 0.1442 || training accuracy 95.62% || lr 4.954070506388387e-06\n","Epoch[3/10](12440/14999) || training loss 0.1958 || training accuracy 93.75% || lr 4.9530002006823195e-06\n","Epoch[3/10](12460/14999) || training loss 0.1873 || training accuracy 92.50% || lr 4.951929894976253e-06\n","Epoch[3/10](12480/14999) || training loss 0.2447 || training accuracy 91.25% || lr 4.950859589270185e-06\n","Epoch[3/10](12500/14999) || training loss 0.1594 || training accuracy 91.88% || lr 4.949789283564118e-06\n","Epoch[3/10](12520/14999) || training loss 0.2195 || training accuracy 91.88% || lr 4.948718977858051e-06\n","Epoch[3/10](12540/14999) || training loss 0.1327 || training accuracy 93.75% || lr 4.9476486721519825e-06\n","Epoch[3/10](12560/14999) || training loss 0.2174 || training accuracy 92.50% || lr 4.946578366445916e-06\n","Epoch[3/10](12580/14999) || training loss 0.1355 || training accuracy 94.38% || lr 4.945508060739849e-06\n","Epoch[3/10](12600/14999) || training loss 0.1118 || training accuracy 94.38% || lr 4.944437755033782e-06\n","Epoch[3/10](12620/14999) || training loss 0.1682 || training accuracy 94.38% || lr 4.943367449327714e-06\n","Epoch[3/10](12640/14999) || training loss 0.1322 || training accuracy 93.12% || lr 4.942297143621646e-06\n","Epoch[3/10](12660/14999) || training loss 0.08045 || training accuracy 96.25% || lr 4.94122683791558e-06\n","Epoch[3/10](12680/14999) || training loss 0.1761 || training accuracy 92.50% || lr 4.940156532209512e-06\n","Epoch[3/10](12700/14999) || training loss 0.2088 || training accuracy 92.50% || lr 4.939086226503445e-06\n","Epoch[3/10](12720/14999) || training loss 0.162 || training accuracy 95.00% || lr 4.9380159207973775e-06\n","Epoch[3/10](12740/14999) || training loss 0.08668 || training accuracy 98.12% || lr 4.936945615091309e-06\n","Epoch[3/10](12760/14999) || training loss 0.1907 || training accuracy 93.12% || lr 4.935875309385243e-06\n","Epoch[3/10](12780/14999) || training loss 0.1699 || training accuracy 93.12% || lr 4.934805003679176e-06\n","Epoch[3/10](12800/14999) || training loss 0.1503 || training accuracy 92.50% || lr 4.9337346979731086e-06\n","Epoch[3/10](12820/14999) || training loss 0.213 || training accuracy 91.88% || lr 4.9326643922670405e-06\n","Epoch[3/10](12840/14999) || training loss 0.1805 || training accuracy 93.75% || lr 4.931594086560974e-06\n","Epoch[3/10](12860/14999) || training loss 0.1917 || training accuracy 92.50% || lr 4.930523780854907e-06\n","Epoch[3/10](12880/14999) || training loss 0.1484 || training accuracy 95.00% || lr 4.929453475148839e-06\n","Epoch[3/10](12900/14999) || training loss 0.1558 || training accuracy 93.75% || lr 4.928383169442772e-06\n","Epoch[3/10](12920/14999) || training loss 0.1161 || training accuracy 94.38% || lr 4.927312863736704e-06\n","Epoch[3/10](12940/14999) || training loss 0.1619 || training accuracy 92.50% || lr 4.926242558030638e-06\n","Epoch[3/10](12960/14999) || training loss 0.1005 || training accuracy 96.88% || lr 4.92517225232457e-06\n","Epoch[3/10](12980/14999) || training loss 0.1592 || training accuracy 93.75% || lr 4.924101946618503e-06\n","Epoch[3/10](13000/14999) || training loss 0.1364 || training accuracy 93.12% || lr 4.9230316409124355e-06\n","Epoch[3/10](13020/14999) || training loss 0.2558 || training accuracy 90.62% || lr 4.921961335206367e-06\n","Epoch[3/10](13040/14999) || training loss 0.1778 || training accuracy 93.75% || lr 4.920891029500301e-06\n","Epoch[3/10](13060/14999) || training loss 0.1199 || training accuracy 95.00% || lr 4.919820723794234e-06\n","Epoch[3/10](13080/14999) || training loss 0.1685 || training accuracy 93.75% || lr 4.918750418088166e-06\n","Epoch[3/10](13100/14999) || training loss 0.1896 || training accuracy 94.38% || lr 4.9176801123820985e-06\n","Epoch[3/10](13120/14999) || training loss 0.2165 || training accuracy 90.62% || lr 4.916609806676032e-06\n","Epoch[3/10](13140/14999) || training loss 0.1784 || training accuracy 91.88% || lr 4.915539500969965e-06\n","Epoch[3/10](13160/14999) || training loss 0.08482 || training accuracy 95.62% || lr 4.914469195263897e-06\n","Epoch[3/10](13180/14999) || training loss 0.1774 || training accuracy 93.75% || lr 4.9133988895578296e-06\n","Epoch[3/10](13200/14999) || training loss 0.1237 || training accuracy 95.62% || lr 4.912328583851762e-06\n","Epoch[3/10](13220/14999) || training loss 0.227 || training accuracy 90.00% || lr 4.911258278145695e-06\n","Epoch[3/10](13240/14999) || training loss 0.2057 || training accuracy 91.88% || lr 4.910187972439628e-06\n","Epoch[3/10](13260/14999) || training loss 0.1114 || training accuracy 95.62% || lr 4.909117666733561e-06\n","Epoch[3/10](13280/14999) || training loss 0.1195 || training accuracy 94.38% || lr 4.908047361027493e-06\n","Epoch[3/10](13300/14999) || training loss 0.08397 || training accuracy 97.50% || lr 4.906977055321426e-06\n","Epoch[3/10](13320/14999) || training loss 0.1657 || training accuracy 93.12% || lr 4.905906749615359e-06\n","Epoch[3/10](13340/14999) || training loss 0.1591 || training accuracy 93.12% || lr 4.904836443909292e-06\n","Epoch[3/10](13360/14999) || training loss 0.166 || training accuracy 93.12% || lr 4.903766138203224e-06\n","Epoch[3/10](13380/14999) || training loss 0.1597 || training accuracy 91.88% || lr 4.9026958324971565e-06\n","Epoch[3/10](13400/14999) || training loss 0.1394 || training accuracy 93.75% || lr 4.90162552679109e-06\n","Epoch[3/10](13420/14999) || training loss 0.1554 || training accuracy 91.88% || lr 4.900555221085022e-06\n","Epoch[3/10](13440/14999) || training loss 0.1694 || training accuracy 95.00% || lr 4.899484915378955e-06\n","Epoch[3/10](13460/14999) || training loss 0.1227 || training accuracy 95.62% || lr 4.8984146096728876e-06\n","Epoch[3/10](13480/14999) || training loss 0.1516 || training accuracy 94.38% || lr 4.89734430396682e-06\n","Epoch[3/10](13500/14999) || training loss 0.1303 || training accuracy 95.62% || lr 4.896273998260753e-06\n","Epoch[3/10](13520/14999) || training loss 0.2148 || training accuracy 93.12% || lr 4.895203692554686e-06\n","Epoch[3/10](13540/14999) || training loss 0.1788 || training accuracy 93.75% || lr 4.894133386848619e-06\n","Epoch[3/10](13560/14999) || training loss 0.1359 || training accuracy 95.00% || lr 4.893063081142551e-06\n","Epoch[3/10](13580/14999) || training loss 0.1546 || training accuracy 95.00% || lr 4.891992775436484e-06\n","Epoch[3/10](13600/14999) || training loss 0.2228 || training accuracy 90.00% || lr 4.890922469730417e-06\n","Epoch[3/10](13620/14999) || training loss 0.1419 || training accuracy 96.25% || lr 4.889852164024349e-06\n","Epoch[3/10](13640/14999) || training loss 0.1664 || training accuracy 94.38% || lr 4.888781858318282e-06\n","Epoch[3/10](13660/14999) || training loss 0.127 || training accuracy 96.88% || lr 4.8877115526122144e-06\n","Epoch[3/10](13680/14999) || training loss 0.1408 || training accuracy 95.62% || lr 4.886641246906148e-06\n","Epoch[3/10](13700/14999) || training loss 0.1374 || training accuracy 96.25% || lr 4.88557094120008e-06\n","Epoch[3/10](13720/14999) || training loss 0.1017 || training accuracy 96.25% || lr 4.884500635494013e-06\n","Epoch[3/10](13740/14999) || training loss 0.1678 || training accuracy 94.38% || lr 4.8834303297879455e-06\n","Epoch[3/10](13760/14999) || training loss 0.1211 || training accuracy 95.00% || lr 4.8823600240818775e-06\n","Epoch[3/10](13780/14999) || training loss 0.1423 || training accuracy 94.38% || lr 4.881289718375811e-06\n","Epoch[3/10](13800/14999) || training loss 0.2085 || training accuracy 90.62% || lr 4.880219412669744e-06\n","Epoch[3/10](13820/14999) || training loss 0.1047 || training accuracy 96.25% || lr 4.879149106963676e-06\n","Epoch[3/10](13840/14999) || training loss 0.08748 || training accuracy 96.25% || lr 4.8780788012576086e-06\n","Epoch[3/10](13860/14999) || training loss 0.1735 || training accuracy 94.38% || lr 4.877008495551542e-06\n","Epoch[3/10](13880/14999) || training loss 0.08372 || training accuracy 95.62% || lr 4.875938189845475e-06\n","Epoch[3/10](13900/14999) || training loss 0.1586 || training accuracy 93.12% || lr 4.874867884139407e-06\n","Epoch[3/10](13920/14999) || training loss 0.1564 || training accuracy 94.38% || lr 4.87379757843334e-06\n","Epoch[3/10](13940/14999) || training loss 0.1807 || training accuracy 92.50% || lr 4.8727272727272724e-06\n","Epoch[3/10](13960/14999) || training loss 0.2074 || training accuracy 92.50% || lr 4.871656967021205e-06\n","Epoch[3/10](13980/14999) || training loss 0.2006 || training accuracy 92.50% || lr 4.870586661315138e-06\n","Epoch[3/10](14000/14999) || training loss 0.1324 || training accuracy 94.38% || lr 4.869516355609071e-06\n","Epoch[3/10](14020/14999) || training loss 0.1717 || training accuracy 94.38% || lr 4.868446049903003e-06\n","Epoch[3/10](14040/14999) || training loss 0.1306 || training accuracy 94.38% || lr 4.8673757441969355e-06\n","Epoch[3/10](14060/14999) || training loss 0.1511 || training accuracy 94.38% || lr 4.866305438490869e-06\n","Epoch[3/10](14080/14999) || training loss 0.2026 || training accuracy 91.88% || lr 4.865235132784802e-06\n","Epoch[3/10](14100/14999) || training loss 0.1854 || training accuracy 94.38% || lr 4.864164827078734e-06\n","Epoch[3/10](14120/14999) || training loss 0.1678 || training accuracy 93.75% || lr 4.8630945213726665e-06\n","Epoch[3/10](14140/14999) || training loss 0.1062 || training accuracy 94.38% || lr 4.8620242156666e-06\n","Epoch[3/10](14160/14999) || training loss 0.1476 || training accuracy 93.12% || lr 4.860953909960532e-06\n","Epoch[3/10](14180/14999) || training loss 0.1695 || training accuracy 95.00% || lr 4.859883604254465e-06\n","Epoch[3/10](14200/14999) || training loss 0.154 || training accuracy 95.00% || lr 4.858813298548398e-06\n","Epoch[3/10](14220/14999) || training loss 0.2115 || training accuracy 92.50% || lr 4.85774299284233e-06\n","Epoch[3/10](14240/14999) || training loss 0.1649 || training accuracy 93.75% || lr 4.856672687136263e-06\n","Epoch[3/10](14260/14999) || training loss 0.2291 || training accuracy 90.00% || lr 4.855602381430196e-06\n","Epoch[3/10](14280/14999) || training loss 0.1174 || training accuracy 96.25% || lr 4.854532075724129e-06\n","Epoch[3/10](14300/14999) || training loss 0.1828 || training accuracy 91.25% || lr 4.853461770018061e-06\n","Epoch[3/10](14320/14999) || training loss 0.1478 || training accuracy 92.50% || lr 4.852391464311994e-06\n","Epoch[3/10](14340/14999) || training loss 0.1474 || training accuracy 93.75% || lr 4.851321158605927e-06\n","Epoch[3/10](14360/14999) || training loss 0.1075 || training accuracy 95.00% || lr 4.850250852899859e-06\n","Epoch[3/10](14380/14999) || training loss 0.1489 || training accuracy 95.00% || lr 4.849180547193792e-06\n","Epoch[3/10](14400/14999) || training loss 0.1149 || training accuracy 95.62% || lr 4.8481102414877245e-06\n","Epoch[3/10](14420/14999) || training loss 0.08166 || training accuracy 96.88% || lr 4.847039935781658e-06\n","Epoch[3/10](14440/14999) || training loss 0.1516 || training accuracy 95.62% || lr 4.84596963007559e-06\n","Epoch[3/10](14460/14999) || training loss 0.1516 || training accuracy 95.00% || lr 4.844899324369523e-06\n","Epoch[3/10](14480/14999) || training loss 0.0967 || training accuracy 95.62% || lr 4.843829018663456e-06\n","Epoch[3/10](14500/14999) || training loss 0.1445 || training accuracy 93.75% || lr 4.8427587129573876e-06\n","Epoch[3/10](14520/14999) || training loss 0.2661 || training accuracy 90.00% || lr 4.841688407251321e-06\n","Epoch[3/10](14540/14999) || training loss 0.1899 || training accuracy 92.50% || lr 4.840618101545254e-06\n","Epoch[3/10](14560/14999) || training loss 0.187 || training accuracy 94.38% || lr 4.839547795839186e-06\n","Epoch[3/10](14580/14999) || training loss 0.1534 || training accuracy 95.00% || lr 4.838477490133119e-06\n","Epoch[3/10](14600/14999) || training loss 0.1661 || training accuracy 93.12% || lr 4.837407184427052e-06\n","Epoch[3/10](14620/14999) || training loss 0.1503 || training accuracy 93.75% || lr 4.836336878720985e-06\n","Epoch[3/10](14640/14999) || training loss 0.1523 || training accuracy 95.00% || lr 4.835266573014917e-06\n","Epoch[3/10](14660/14999) || training loss 0.1969 || training accuracy 93.75% || lr 4.83419626730885e-06\n","Epoch[3/10](14680/14999) || training loss 0.1224 || training accuracy 95.62% || lr 4.8331259616027825e-06\n","Epoch[3/10](14700/14999) || training loss 0.1052 || training accuracy 96.88% || lr 4.832055655896715e-06\n","Epoch[3/10](14720/14999) || training loss 0.199 || training accuracy 93.12% || lr 4.830985350190648e-06\n","Epoch[3/10](14740/14999) || training loss 0.1844 || training accuracy 92.50% || lr 4.829915044484581e-06\n","Epoch[3/10](14760/14999) || training loss 0.1007 || training accuracy 95.00% || lr 4.828844738778514e-06\n","Epoch[3/10](14780/14999) || training loss 0.1596 || training accuracy 93.12% || lr 4.8277744330724455e-06\n","Epoch[3/10](14800/14999) || training loss 0.1466 || training accuracy 95.62% || lr 4.826704127366379e-06\n","Epoch[3/10](14820/14999) || training loss 0.1812 || training accuracy 92.50% || lr 4.825633821660312e-06\n","Epoch[3/10](14840/14999) || training loss 0.1493 || training accuracy 95.00% || lr 4.824563515954244e-06\n","Epoch[3/10](14860/14999) || training loss 0.1312 || training accuracy 94.38% || lr 4.823493210248177e-06\n","Epoch[3/10](14880/14999) || training loss 0.1399 || training accuracy 95.62% || lr 4.82242290454211e-06\n","Epoch[3/10](14900/14999) || training loss 0.1187 || training accuracy 94.38% || lr 4.821352598836042e-06\n","Epoch[3/10](14920/14999) || training loss 0.1878 || training accuracy 93.75% || lr 4.820282293129975e-06\n","Epoch[3/10](14940/14999) || training loss 0.1507 || training accuracy 95.00% || lr 4.819211987423908e-06\n","Epoch[3/10](14960/14999) || training loss 0.1515 || training accuracy 94.38% || lr 4.8181416817178405e-06\n","Epoch[3/10](14980/14999) || training loss 0.168 || training accuracy 94.38% || lr 4.817071376011773e-06\n","Calculating validation results...\n","100% 235/235 [02:22<00:00,  1.65it/s]\n","[Val] acc : 90.82%, loss: 0.2477, F1 : 0.9082 || best acc : 90.92%, best loss: 0.2442\n","Time elapsed:  266.90 min\n","\n","Epoch[4/10](20/14999) || training loss 0.1141 || training accuracy 94.38% || lr 4.814984279884942e-06\n","Epoch[4/10](40/14999) || training loss 0.1089 || training accuracy 95.00% || lr 4.8139139741788746e-06\n","Epoch[4/10](60/14999) || training loss 0.1081 || training accuracy 96.25% || lr 4.812843668472807e-06\n","Epoch[4/10](80/14999) || training loss 0.1759 || training accuracy 92.50% || lr 4.811773362766739e-06\n","Epoch[4/10](100/14999) || training loss 0.08537 || training accuracy 96.88% || lr 4.810703057060673e-06\n","Epoch[4/10](120/14999) || training loss 0.0456 || training accuracy 98.12% || lr 4.809632751354606e-06\n","Epoch[4/10](140/14999) || training loss 0.1326 || training accuracy 93.12% || lr 4.8085624456485384e-06\n","Epoch[4/10](160/14999) || training loss 0.09767 || training accuracy 95.62% || lr 4.80749213994247e-06\n","Epoch[4/10](180/14999) || training loss 0.0644 || training accuracy 98.75% || lr 4.806421834236404e-06\n","Epoch[4/10](200/14999) || training loss 0.08053 || training accuracy 97.50% || lr 4.805351528530337e-06\n","Epoch[4/10](220/14999) || training loss 0.08575 || training accuracy 95.62% || lr 4.804281222824269e-06\n","Epoch[4/10](240/14999) || training loss 0.1137 || training accuracy 94.38% || lr 4.8032109171182015e-06\n","Epoch[4/10](260/14999) || training loss 0.1463 || training accuracy 93.12% || lr 4.802140611412134e-06\n","Epoch[4/10](280/14999) || training loss 0.1199 || training accuracy 95.00% || lr 4.801070305706067e-06\n","Epoch[4/10](300/14999) || training loss 0.1061 || training accuracy 97.50% || lr 4.8e-06\n","Epoch[4/10](320/14999) || training loss 0.2097 || training accuracy 93.12% || lr 4.7989296942939326e-06\n","Epoch[4/10](340/14999) || training loss 0.1635 || training accuracy 92.50% || lr 4.797859388587865e-06\n","Epoch[4/10](360/14999) || training loss 0.05914 || training accuracy 98.12% || lr 4.796789082881797e-06\n","Epoch[4/10](380/14999) || training loss 0.09294 || training accuracy 97.50% || lr 4.795718777175731e-06\n","Epoch[4/10](400/14999) || training loss 0.1545 || training accuracy 95.00% || lr 4.794648471469664e-06\n","Epoch[4/10](420/14999) || training loss 0.1293 || training accuracy 95.62% || lr 4.793578165763596e-06\n","Epoch[4/10](440/14999) || training loss 0.04564 || training accuracy 99.38% || lr 4.792507860057528e-06\n","Epoch[4/10](460/14999) || training loss 0.1283 || training accuracy 95.62% || lr 4.791437554351462e-06\n","Epoch[4/10](480/14999) || training loss 0.1421 || training accuracy 93.75% || lr 4.790367248645395e-06\n","Epoch[4/10](500/14999) || training loss 0.1594 || training accuracy 92.50% || lr 4.789296942939327e-06\n","Epoch[4/10](520/14999) || training loss 0.1008 || training accuracy 93.75% || lr 4.7882266372332594e-06\n","Epoch[4/10](540/14999) || training loss 0.1192 || training accuracy 96.25% || lr 4.787156331527192e-06\n","Epoch[4/10](560/14999) || training loss 0.1316 || training accuracy 95.00% || lr 4.786086025821125e-06\n","Epoch[4/10](580/14999) || training loss 0.1277 || training accuracy 92.50% || lr 4.785015720115058e-06\n","Epoch[4/10](600/14999) || training loss 0.1702 || training accuracy 93.12% || lr 4.7839454144089905e-06\n","Epoch[4/10](620/14999) || training loss 0.1406 || training accuracy 93.12% || lr 4.7828751087029225e-06\n","Epoch[4/10](640/14999) || training loss 0.07048 || training accuracy 96.88% || lr 4.781804802996855e-06\n","Epoch[4/10](660/14999) || training loss 0.1303 || training accuracy 95.00% || lr 4.780734497290789e-06\n","Epoch[4/10](680/14999) || training loss 0.1282 || training accuracy 96.88% || lr 4.779664191584722e-06\n","Epoch[4/10](700/14999) || training loss 0.119 || training accuracy 95.00% || lr 4.7785938858786536e-06\n","Epoch[4/10](720/14999) || training loss 0.04674 || training accuracy 98.75% || lr 4.777523580172586e-06\n","Epoch[4/10](740/14999) || training loss 0.168 || training accuracy 93.75% || lr 4.77645327446652e-06\n","Epoch[4/10](760/14999) || training loss 0.126 || training accuracy 97.50% || lr 4.775382968760452e-06\n","Epoch[4/10](780/14999) || training loss 0.056 || training accuracy 98.75% || lr 4.774312663054385e-06\n","Epoch[4/10](800/14999) || training loss 0.1178 || training accuracy 96.88% || lr 4.7732423573483174e-06\n","Epoch[4/10](820/14999) || training loss 0.103 || training accuracy 97.50% || lr 4.772172051642249e-06\n","Epoch[4/10](840/14999) || training loss 0.184 || training accuracy 96.25% || lr 4.771101745936183e-06\n","Epoch[4/10](860/14999) || training loss 0.1183 || training accuracy 96.25% || lr 4.770031440230116e-06\n","Epoch[4/10](880/14999) || training loss 0.08973 || training accuracy 96.88% || lr 4.7689611345240485e-06\n","Epoch[4/10](900/14999) || training loss 0.08544 || training accuracy 97.50% || lr 4.7678908288179805e-06\n","Epoch[4/10](920/14999) || training loss 0.1102 || training accuracy 96.25% || lr 4.766820523111913e-06\n","Epoch[4/10](940/14999) || training loss 0.09639 || training accuracy 96.88% || lr 4.765750217405847e-06\n","Epoch[4/10](960/14999) || training loss 0.1513 || training accuracy 92.50% || lr 4.764679911699779e-06\n","Epoch[4/10](980/14999) || training loss 0.1164 || training accuracy 96.25% || lr 4.7636096059937115e-06\n","Epoch[4/10](1000/14999) || training loss 0.09432 || training accuracy 98.12% || lr 4.762539300287644e-06\n","Epoch[4/10](1020/14999) || training loss 0.0603 || training accuracy 97.50% || lr 4.761468994581578e-06\n","Epoch[4/10](1040/14999) || training loss 0.2052 || training accuracy 93.12% || lr 4.76039868887551e-06\n","Epoch[4/10](1060/14999) || training loss 0.1352 || training accuracy 95.00% || lr 4.759328383169443e-06\n","Epoch[4/10](1080/14999) || training loss 0.08471 || training accuracy 96.88% || lr 4.758258077463375e-06\n","Epoch[4/10](1100/14999) || training loss 0.09624 || training accuracy 96.25% || lr 4.757187771757307e-06\n","Epoch[4/10](1120/14999) || training loss 0.0727 || training accuracy 96.88% || lr 4.756117466051241e-06\n","Epoch[4/10](1140/14999) || training loss 0.0528 || training accuracy 98.12% || lr 4.755047160345174e-06\n","Epoch[4/10](1160/14999) || training loss 0.1002 || training accuracy 94.38% || lr 4.753976854639106e-06\n","Epoch[4/10](1180/14999) || training loss 0.08166 || training accuracy 96.88% || lr 4.7529065489330384e-06\n","Epoch[4/10](1200/14999) || training loss 0.08383 || training accuracy 96.88% || lr 4.751836243226971e-06\n","Epoch[4/10](1220/14999) || training loss 0.1752 || training accuracy 94.38% || lr 4.750765937520905e-06\n","Epoch[4/10](1240/14999) || training loss 0.1156 || training accuracy 97.50% || lr 4.749695631814837e-06\n","Epoch[4/10](1260/14999) || training loss 0.09034 || training accuracy 96.25% || lr 4.7486253261087695e-06\n","Epoch[4/10](1280/14999) || training loss 0.156 || training accuracy 95.62% || lr 4.747555020402702e-06\n","Epoch[4/10](1300/14999) || training loss 0.06755 || training accuracy 98.12% || lr 4.746484714696635e-06\n","Epoch[4/10](1320/14999) || training loss 0.1126 || training accuracy 96.25% || lr 4.745414408990568e-06\n","Epoch[4/10](1340/14999) || training loss 0.2059 || training accuracy 94.38% || lr 4.744344103284501e-06\n","Epoch[4/10](1360/14999) || training loss 0.08399 || training accuracy 97.50% || lr 4.7432737975784326e-06\n","Epoch[4/10](1380/14999) || training loss 0.05691 || training accuracy 98.12% || lr 4.742203491872365e-06\n","Epoch[4/10](1400/14999) || training loss 0.08935 || training accuracy 95.62% || lr 4.741133186166299e-06\n","Epoch[4/10](1420/14999) || training loss 0.1144 || training accuracy 96.25% || lr 4.740062880460232e-06\n","Epoch[4/10](1440/14999) || training loss 0.1281 || training accuracy 94.38% || lr 4.738992574754164e-06\n","Epoch[4/10](1460/14999) || training loss 0.1147 || training accuracy 95.62% || lr 4.737922269048096e-06\n","Epoch[4/10](1480/14999) || training loss 0.1911 || training accuracy 91.88% || lr 4.73685196334203e-06\n","Epoch[4/10](1500/14999) || training loss 0.1559 || training accuracy 95.00% || lr 4.735781657635962e-06\n","Epoch[4/10](1520/14999) || training loss 0.112 || training accuracy 96.25% || lr 4.734711351929895e-06\n","Epoch[4/10](1540/14999) || training loss 0.1167 || training accuracy 94.38% || lr 4.7336410462238275e-06\n","Epoch[4/10](1560/14999) || training loss 0.07075 || training accuracy 97.50% || lr 4.7325707405177594e-06\n","Epoch[4/10](1580/14999) || training loss 0.1687 || training accuracy 93.75% || lr 4.731500434811693e-06\n","Epoch[4/10](1600/14999) || training loss 0.1437 || training accuracy 95.62% || lr 4.730430129105626e-06\n","Epoch[4/10](1620/14999) || training loss 0.08548 || training accuracy 96.25% || lr 4.729359823399559e-06\n","Epoch[4/10](1640/14999) || training loss 0.1048 || training accuracy 96.88% || lr 4.7282895176934905e-06\n","Epoch[4/10](1660/14999) || training loss 0.09637 || training accuracy 96.25% || lr 4.727219211987423e-06\n","Epoch[4/10](1680/14999) || training loss 0.1811 || training accuracy 93.75% || lr 4.726148906281357e-06\n","Epoch[4/10](1700/14999) || training loss 0.07586 || training accuracy 96.88% || lr 4.725078600575289e-06\n","Epoch[4/10](1720/14999) || training loss 0.1137 || training accuracy 95.00% || lr 4.724008294869222e-06\n","Epoch[4/10](1740/14999) || training loss 0.1552 || training accuracy 91.88% || lr 4.722937989163154e-06\n","Epoch[4/10](1760/14999) || training loss 0.1245 || training accuracy 95.00% || lr 4.721867683457088e-06\n","Epoch[4/10](1780/14999) || training loss 0.1135 || training accuracy 96.88% || lr 4.72079737775102e-06\n","Epoch[4/10](1800/14999) || training loss 0.06803 || training accuracy 96.88% || lr 4.719727072044953e-06\n","Epoch[4/10](1820/14999) || training loss 0.1774 || training accuracy 93.75% || lr 4.7186567663388855e-06\n","Epoch[4/10](1840/14999) || training loss 0.1669 || training accuracy 93.12% || lr 4.7175864606328174e-06\n","Epoch[4/10](1860/14999) || training loss 0.1204 || training accuracy 96.88% || lr 4.716516154926751e-06\n","Epoch[4/10](1880/14999) || training loss 0.1084 || training accuracy 95.00% || lr 4.715445849220684e-06\n","Epoch[4/10](1900/14999) || training loss 0.139 || training accuracy 95.62% || lr 4.714375543514616e-06\n","Epoch[4/10](1920/14999) || training loss 0.1432 || training accuracy 95.62% || lr 4.7133052378085485e-06\n","Epoch[4/10](1940/14999) || training loss 0.1129 || training accuracy 97.50% || lr 4.712234932102481e-06\n","Epoch[4/10](1960/14999) || training loss 0.1358 || training accuracy 96.25% || lr 4.711164626396415e-06\n","Epoch[4/10](1980/14999) || training loss 0.1195 || training accuracy 93.75% || lr 4.710094320690347e-06\n","Epoch[4/10](2000/14999) || training loss 0.09929 || training accuracy 96.25% || lr 4.70902401498428e-06\n","Epoch[4/10](2020/14999) || training loss 0.186 || training accuracy 95.00% || lr 4.707953709278212e-06\n","Epoch[4/10](2040/14999) || training loss 0.1389 || training accuracy 96.25% || lr 4.706883403572145e-06\n","Epoch[4/10](2060/14999) || training loss 0.1126 || training accuracy 96.25% || lr 4.705813097866078e-06\n","Epoch[4/10](2080/14999) || training loss 0.04447 || training accuracy 98.75% || lr 4.704742792160011e-06\n","Epoch[4/10](2100/14999) || training loss 0.1094 || training accuracy 95.00% || lr 4.703672486453943e-06\n","Epoch[4/10](2120/14999) || training loss 0.1684 || training accuracy 92.50% || lr 4.702602180747875e-06\n","Epoch[4/10](2140/14999) || training loss 0.1314 || training accuracy 95.00% || lr 4.701531875041809e-06\n","Epoch[4/10](2160/14999) || training loss 0.09236 || training accuracy 95.62% || lr 4.700461569335742e-06\n","Epoch[4/10](2180/14999) || training loss 0.1007 || training accuracy 96.88% || lr 4.699391263629674e-06\n","Epoch[4/10](2200/14999) || training loss 0.1104 || training accuracy 96.25% || lr 4.6983209579236065e-06\n","Epoch[4/10](2220/14999) || training loss 0.1384 || training accuracy 96.25% || lr 4.697250652217539e-06\n","Epoch[4/10](2240/14999) || training loss 0.1651 || training accuracy 95.00% || lr 4.696180346511472e-06\n","Epoch[4/10](2260/14999) || training loss 0.135 || training accuracy 94.38% || lr 4.695110040805405e-06\n","Epoch[4/10](2280/14999) || training loss 0.1066 || training accuracy 96.88% || lr 4.694039735099338e-06\n","Epoch[4/10](2300/14999) || training loss 0.08505 || training accuracy 96.88% || lr 4.69296942939327e-06\n","Epoch[4/10](2320/14999) || training loss 0.1012 || training accuracy 97.50% || lr 4.691899123687203e-06\n","Epoch[4/10](2340/14999) || training loss 0.1431 || training accuracy 95.62% || lr 4.690828817981136e-06\n","Epoch[4/10](2360/14999) || training loss 0.1118 || training accuracy 95.62% || lr 4.689758512275069e-06\n","Epoch[4/10](2380/14999) || training loss 0.1611 || training accuracy 91.88% || lr 4.688688206569001e-06\n","Epoch[4/10](2400/14999) || training loss 0.1016 || training accuracy 95.00% || lr 4.687617900862933e-06\n","Epoch[4/10](2420/14999) || training loss 0.06567 || training accuracy 97.50% || lr 4.686547595156867e-06\n","Epoch[4/10](2440/14999) || training loss 0.1692 || training accuracy 94.38% || lr 4.685477289450799e-06\n","Epoch[4/10](2460/14999) || training loss 0.224 || training accuracy 91.25% || lr 4.684406983744732e-06\n","Epoch[4/10](2480/14999) || training loss 0.0623 || training accuracy 98.75% || lr 4.6833366780386645e-06\n","Epoch[4/10](2500/14999) || training loss 0.09053 || training accuracy 96.25% || lr 4.682266372332598e-06\n","Epoch[4/10](2520/14999) || training loss 0.1058 || training accuracy 96.25% || lr 4.68119606662653e-06\n","Epoch[4/10](2540/14999) || training loss 0.1412 || training accuracy 95.62% || lr 4.680125760920463e-06\n","Epoch[4/10](2560/14999) || training loss 0.1566 || training accuracy 95.00% || lr 4.679055455214396e-06\n","Epoch[4/10](2580/14999) || training loss 0.09627 || training accuracy 96.88% || lr 4.6779851495083275e-06\n","Epoch[4/10](2600/14999) || training loss 0.08956 || training accuracy 97.50% || lr 4.676914843802261e-06\n","Epoch[4/10](2620/14999) || training loss 0.226 || training accuracy 91.88% || lr 4.675844538096194e-06\n","Epoch[4/10](2640/14999) || training loss 0.1846 || training accuracy 92.50% || lr 4.674774232390126e-06\n","Epoch[4/10](2660/14999) || training loss 0.1388 || training accuracy 95.00% || lr 4.673703926684059e-06\n","Epoch[4/10](2680/14999) || training loss 0.1397 || training accuracy 92.50% || lr 4.672633620977991e-06\n","Epoch[4/10](2700/14999) || training loss 0.1564 || training accuracy 95.62% || lr 4.671563315271925e-06\n","Epoch[4/10](2720/14999) || training loss 0.1896 || training accuracy 91.88% || lr 4.670493009565857e-06\n","Epoch[4/10](2740/14999) || training loss 0.1379 || training accuracy 94.38% || lr 4.66942270385979e-06\n","Epoch[4/10](2760/14999) || training loss 0.1317 || training accuracy 94.38% || lr 4.6683523981537225e-06\n","Epoch[4/10](2780/14999) || training loss 0.1155 || training accuracy 96.88% || lr 4.667282092447654e-06\n","Epoch[4/10](2800/14999) || training loss 0.1148 || training accuracy 96.88% || lr 4.666211786741588e-06\n","Epoch[4/10](2820/14999) || training loss 0.1647 || training accuracy 95.00% || lr 4.665141481035521e-06\n","Epoch[4/10](2840/14999) || training loss 0.07062 || training accuracy 96.88% || lr 4.664071175329453e-06\n","Epoch[4/10](2860/14999) || training loss 0.1513 || training accuracy 93.75% || lr 4.6630008696233855e-06\n","Epoch[4/10](2880/14999) || training loss 0.1239 || training accuracy 93.75% || lr 4.661930563917319e-06\n","Epoch[4/10](2900/14999) || training loss 0.1536 || training accuracy 94.38% || lr 4.660860258211252e-06\n","Epoch[4/10](2920/14999) || training loss 0.1091 || training accuracy 96.25% || lr 4.659789952505184e-06\n","Epoch[4/10](2940/14999) || training loss 0.145 || training accuracy 95.00% || lr 4.658719646799117e-06\n","Epoch[4/10](2960/14999) || training loss 0.105 || training accuracy 95.62% || lr 4.657649341093049e-06\n","Epoch[4/10](2980/14999) || training loss 0.08812 || training accuracy 98.12% || lr 4.656579035386982e-06\n","Epoch[4/10](3000/14999) || training loss 0.1003 || training accuracy 95.62% || lr 4.655508729680915e-06\n","Epoch[4/10](3020/14999) || training loss 0.183 || training accuracy 93.75% || lr 4.654438423974848e-06\n","Epoch[4/10](3040/14999) || training loss 0.08376 || training accuracy 96.88% || lr 4.6533681182687805e-06\n","Epoch[4/10](3060/14999) || training loss 0.09303 || training accuracy 96.25% || lr 4.652297812562713e-06\n","Epoch[4/10](3080/14999) || training loss 0.1576 || training accuracy 94.38% || lr 4.651227506856646e-06\n","Epoch[4/10](3100/14999) || training loss 0.137 || training accuracy 95.62% || lr 4.650157201150579e-06\n","Epoch[4/10](3120/14999) || training loss 0.09217 || training accuracy 97.50% || lr 4.649086895444511e-06\n","Epoch[4/10](3140/14999) || training loss 0.1961 || training accuracy 94.38% || lr 4.6480165897384435e-06\n","Epoch[4/10](3160/14999) || training loss 0.07846 || training accuracy 97.50% || lr 4.646946284032377e-06\n","Epoch[4/10](3180/14999) || training loss 0.08452 || training accuracy 96.25% || lr 4.645875978326309e-06\n","Epoch[4/10](3200/14999) || training loss 0.09486 || training accuracy 96.25% || lr 4.644805672620242e-06\n","Epoch[4/10](3220/14999) || training loss 0.1962 || training accuracy 93.75% || lr 4.6437353669141746e-06\n","Epoch[4/10](3240/14999) || training loss 0.2274 || training accuracy 92.50% || lr 4.642665061208107e-06\n","Epoch[4/10](3260/14999) || training loss 0.1131 || training accuracy 95.62% || lr 4.64159475550204e-06\n","Epoch[4/10](3280/14999) || training loss 0.1136 || training accuracy 95.62% || lr 4.640524449795973e-06\n","Epoch[4/10](3300/14999) || training loss 0.07747 || training accuracy 96.25% || lr 4.639454144089906e-06\n","Epoch[4/10](3320/14999) || training loss 0.1655 || training accuracy 95.62% || lr 4.638383838383838e-06\n","Epoch[4/10](3340/14999) || training loss 0.165 || training accuracy 95.62% || lr 4.637313532677771e-06\n","Epoch[4/10](3360/14999) || training loss 0.1413 || training accuracy 96.88% || lr 4.636243226971704e-06\n","Epoch[4/10](3380/14999) || training loss 0.1471 || training accuracy 95.62% || lr 4.635172921265636e-06\n","Epoch[4/10](3400/14999) || training loss 0.08995 || training accuracy 97.50% || lr 4.634102615559569e-06\n","Epoch[4/10](3420/14999) || training loss 0.125 || training accuracy 95.00% || lr 4.6330323098535015e-06\n","Epoch[4/10](3440/14999) || training loss 0.1847 || training accuracy 91.25% || lr 4.631962004147435e-06\n","Epoch[4/10](3460/14999) || training loss 0.1498 || training accuracy 94.38% || lr 4.630891698441367e-06\n","Epoch[4/10](3480/14999) || training loss 0.1176 || training accuracy 95.00% || lr 4.6298213927353e-06\n","Epoch[4/10](3500/14999) || training loss 0.1019 || training accuracy 96.88% || lr 4.6287510870292326e-06\n","Epoch[4/10](3520/14999) || training loss 0.1383 || training accuracy 93.12% || lr 4.6276807813231645e-06\n","Epoch[4/10](3540/14999) || training loss 0.1045 || training accuracy 96.88% || lr 4.626610475617098e-06\n","Epoch[4/10](3560/14999) || training loss 0.09901 || training accuracy 96.88% || lr 4.625540169911031e-06\n","Epoch[4/10](3580/14999) || training loss 0.1932 || training accuracy 91.25% || lr 4.624469864204964e-06\n","Epoch[4/10](3600/14999) || training loss 0.1017 || training accuracy 95.00% || lr 4.6233995584988956e-06\n","Epoch[4/10](3620/14999) || training loss 0.1496 || training accuracy 95.00% || lr 4.622329252792829e-06\n","Epoch[4/10](3640/14999) || training loss 0.1483 || training accuracy 93.75% || lr 4.621258947086762e-06\n","Epoch[4/10](3660/14999) || training loss 0.1436 || training accuracy 93.75% || lr 4.620188641380694e-06\n","Epoch[4/10](3680/14999) || training loss 0.1363 || training accuracy 95.00% || lr 4.619118335674627e-06\n","Epoch[4/10](3700/14999) || training loss 0.1477 || training accuracy 93.75% || lr 4.6180480299685594e-06\n","Epoch[4/10](3720/14999) || training loss 0.09738 || training accuracy 97.50% || lr 4.616977724262492e-06\n","Epoch[4/10](3740/14999) || training loss 0.0649 || training accuracy 98.75% || lr 4.615907418556425e-06\n","Epoch[4/10](3760/14999) || training loss 0.04361 || training accuracy 98.75% || lr 4.614837112850358e-06\n","Epoch[4/10](3780/14999) || training loss 0.1874 || training accuracy 96.25% || lr 4.6137668071442905e-06\n","Epoch[4/10](3800/14999) || training loss 0.09546 || training accuracy 95.62% || lr 4.6126965014382225e-06\n","Epoch[4/10](3820/14999) || training loss 0.1064 || training accuracy 95.62% || lr 4.611626195732156e-06\n","Epoch[4/10](3840/14999) || training loss 0.1044 || training accuracy 96.25% || lr 4.610555890026089e-06\n","Epoch[4/10](3860/14999) || training loss 0.07365 || training accuracy 96.88% || lr 4.609485584320021e-06\n","Epoch[4/10](3880/14999) || training loss 0.08801 || training accuracy 97.50% || lr 4.6084152786139536e-06\n","Epoch[4/10](3900/14999) || training loss 0.06763 || training accuracy 96.88% || lr 4.607344972907887e-06\n","Epoch[4/10](3920/14999) || training loss 0.0926 || training accuracy 98.75% || lr 4.606274667201819e-06\n","Epoch[4/10](3940/14999) || training loss 0.1494 || training accuracy 95.62% || lr 4.605204361495752e-06\n","Epoch[4/10](3960/14999) || training loss 0.08819 || training accuracy 97.50% || lr 4.604134055789685e-06\n","Epoch[4/10](3980/14999) || training loss 0.06905 || training accuracy 97.50% || lr 4.6030637500836174e-06\n","Epoch[4/10](4000/14999) || training loss 0.124 || training accuracy 95.62% || lr 4.60199344437755e-06\n","Epoch[4/10](4020/14999) || training loss 0.08451 || training accuracy 96.25% || lr 4.600923138671483e-06\n","Epoch[4/10](4040/14999) || training loss 0.1076 || training accuracy 95.62% || lr 4.599852832965416e-06\n","Epoch[4/10](4060/14999) || training loss 0.1037 || training accuracy 96.88% || lr 4.598782527259348e-06\n","Epoch[4/10](4080/14999) || training loss 0.1445 || training accuracy 94.38% || lr 4.5977122215532804e-06\n","Epoch[4/10](4100/14999) || training loss 0.127 || training accuracy 94.38% || lr 4.596641915847214e-06\n","Epoch[4/10](4120/14999) || training loss 0.05944 || training accuracy 98.12% || lr 4.595571610141146e-06\n","Epoch[4/10](4140/14999) || training loss 0.1247 || training accuracy 96.88% || lr 4.594501304435079e-06\n","Epoch[4/10](4160/14999) || training loss 0.1723 || training accuracy 93.12% || lr 4.5934309987290115e-06\n","Epoch[4/10](4180/14999) || training loss 0.1103 || training accuracy 95.62% || lr 4.592360693022945e-06\n","Epoch[4/10](4200/14999) || training loss 0.09281 || training accuracy 96.88% || lr 4.591290387316877e-06\n","Epoch[4/10](4220/14999) || training loss 0.118 || training accuracy 95.00% || lr 4.59022008161081e-06\n","Epoch[4/10](4240/14999) || training loss 0.1478 || training accuracy 93.75% || lr 4.589149775904743e-06\n","Epoch[4/10](4260/14999) || training loss 0.1111 || training accuracy 95.62% || lr 4.5880794701986746e-06\n","Epoch[4/10](4280/14999) || training loss 0.1159 || training accuracy 95.62% || lr 4.587009164492608e-06\n","Epoch[4/10](4300/14999) || training loss 0.04089 || training accuracy 100.00% || lr 4.585938858786541e-06\n","Epoch[4/10](4320/14999) || training loss 0.1511 || training accuracy 92.50% || lr 4.584868553080474e-06\n","Epoch[4/10](4340/14999) || training loss 0.1995 || training accuracy 94.38% || lr 4.583798247374406e-06\n","Epoch[4/10](4360/14999) || training loss 0.1024 || training accuracy 96.25% || lr 4.582727941668339e-06\n","Epoch[4/10](4380/14999) || training loss 0.09692 || training accuracy 96.25% || lr 4.581657635962272e-06\n","Epoch[4/10](4400/14999) || training loss 0.1342 || training accuracy 95.00% || lr 4.580587330256204e-06\n","Epoch[4/10](4420/14999) || training loss 0.08067 || training accuracy 98.75% || lr 4.579517024550137e-06\n","Epoch[4/10](4440/14999) || training loss 0.1175 || training accuracy 97.50% || lr 4.5784467188440695e-06\n","Epoch[4/10](4460/14999) || training loss 0.09406 || training accuracy 97.50% || lr 4.577376413138002e-06\n","Epoch[4/10](4480/14999) || training loss 0.1314 || training accuracy 96.25% || lr 4.576306107431935e-06\n","Epoch[4/10](4500/14999) || training loss 0.1083 || training accuracy 95.00% || lr 4.575235801725868e-06\n","Epoch[4/10](4520/14999) || training loss 0.07306 || training accuracy 96.88% || lr 4.574165496019801e-06\n","Epoch[4/10](4540/14999) || training loss 0.143 || training accuracy 93.75% || lr 4.5730951903137325e-06\n","Epoch[4/10](4560/14999) || training loss 0.1067 || training accuracy 96.25% || lr 4.572024884607666e-06\n","Epoch[4/10](4580/14999) || training loss 0.1563 || training accuracy 95.00% || lr 4.570954578901599e-06\n","Epoch[4/10](4600/14999) || training loss 0.06733 || training accuracy 97.50% || lr 4.569884273195531e-06\n","Epoch[4/10](4620/14999) || training loss 0.1582 || training accuracy 93.75% || lr 4.568813967489464e-06\n","Epoch[4/10](4640/14999) || training loss 0.09777 || training accuracy 96.25% || lr 4.567743661783397e-06\n","Epoch[4/10](4660/14999) || training loss 0.142 || training accuracy 93.75% || lr 4.566673356077329e-06\n","Epoch[4/10](4680/14999) || training loss 0.1122 || training accuracy 95.00% || lr 4.565603050371262e-06\n","Epoch[4/10](4700/14999) || training loss 0.1442 || training accuracy 94.38% || lr 4.564532744665195e-06\n","Epoch[4/10](4720/14999) || training loss 0.07152 || training accuracy 98.12% || lr 4.5634624389591275e-06\n","Epoch[4/10](4740/14999) || training loss 0.1534 || training accuracy 93.12% || lr 4.56239213325306e-06\n","Epoch[4/10](4760/14999) || training loss 0.1164 || training accuracy 96.88% || lr 4.561321827546993e-06\n","Epoch[4/10](4780/14999) || training loss 0.09663 || training accuracy 96.88% || lr 4.560251521840926e-06\n","Epoch[4/10](4800/14999) || training loss 0.1645 || training accuracy 95.00% || lr 4.559181216134858e-06\n","Epoch[4/10](4820/14999) || training loss 0.1727 || training accuracy 95.62% || lr 4.5581109104287905e-06\n","Epoch[4/10](4840/14999) || training loss 0.1033 || training accuracy 96.88% || lr 4.557040604722724e-06\n","Epoch[4/10](4860/14999) || training loss 0.1304 || training accuracy 95.62% || lr 4.555970299016657e-06\n","Epoch[4/10](4880/14999) || training loss 0.1938 || training accuracy 93.12% || lr 4.554899993310589e-06\n","Epoch[4/10](4900/14999) || training loss 0.1619 || training accuracy 92.50% || lr 4.553829687604522e-06\n","Epoch[4/10](4920/14999) || training loss 0.1617 || training accuracy 95.62% || lr 4.552759381898455e-06\n","Epoch[4/10](4940/14999) || training loss 0.181 || training accuracy 93.75% || lr 4.551689076192387e-06\n","Epoch[4/10](4960/14999) || training loss 0.1859 || training accuracy 95.00% || lr 4.55061877048632e-06\n","Epoch[4/10](4980/14999) || training loss 0.09848 || training accuracy 96.25% || lr 4.549548464780253e-06\n","Epoch[4/10](5000/14999) || training loss 0.106 || training accuracy 95.62% || lr 4.548478159074185e-06\n","Epoch[4/10](5020/14999) || training loss 0.07442 || training accuracy 96.88% || lr 4.547407853368118e-06\n","Epoch[4/10](5040/14999) || training loss 0.1359 || training accuracy 95.00% || lr 4.546337547662051e-06\n","Epoch[4/10](5060/14999) || training loss 0.1288 || training accuracy 96.25% || lr 4.545267241955984e-06\n","Epoch[4/10](5080/14999) || training loss 0.1328 || training accuracy 95.62% || lr 4.544196936249916e-06\n","Epoch[4/10](5100/14999) || training loss 0.1372 || training accuracy 96.25% || lr 4.5431266305438485e-06\n","Epoch[4/10](5120/14999) || training loss 0.1481 || training accuracy 95.62% || lr 4.542056324837782e-06\n","Epoch[4/10](5140/14999) || training loss 0.1998 || training accuracy 94.38% || lr 4.540986019131714e-06\n","Epoch[4/10](5160/14999) || training loss 0.08 || training accuracy 96.88% || lr 4.539915713425647e-06\n","Epoch[4/10](5180/14999) || training loss 0.1475 || training accuracy 95.00% || lr 4.53884540771958e-06\n","Epoch[4/10](5200/14999) || training loss 0.1396 || training accuracy 94.38% || lr 4.537775102013512e-06\n","Epoch[4/10](5220/14999) || training loss 0.08718 || training accuracy 96.25% || lr 4.536704796307445e-06\n","Epoch[4/10](5240/14999) || training loss 0.1285 || training accuracy 96.25% || lr 4.535634490601378e-06\n","Epoch[4/10](5260/14999) || training loss 0.1458 || training accuracy 94.38% || lr 4.534564184895311e-06\n","Epoch[4/10](5280/14999) || training loss 0.1207 || training accuracy 93.75% || lr 4.533493879189243e-06\n","Epoch[4/10](5300/14999) || training loss 0.08332 || training accuracy 96.88% || lr 4.532423573483176e-06\n","Epoch[4/10](5320/14999) || training loss 0.1267 || training accuracy 96.25% || lr 4.531353267777109e-06\n","Epoch[4/10](5340/14999) || training loss 0.09133 || training accuracy 95.62% || lr 4.530282962071041e-06\n","Epoch[4/10](5360/14999) || training loss 0.143 || training accuracy 96.25% || lr 4.529212656364974e-06\n","Epoch[4/10](5380/14999) || training loss 0.1726 || training accuracy 95.00% || lr 4.528142350658907e-06\n","Epoch[4/10](5400/14999) || training loss 0.159 || training accuracy 93.12% || lr 4.527072044952839e-06\n","Epoch[4/10](5420/14999) || training loss 0.1689 || training accuracy 91.88% || lr 4.526001739246772e-06\n","Epoch[4/10](5440/14999) || training loss 0.08576 || training accuracy 96.25% || lr 4.524931433540705e-06\n","Epoch[4/10](5460/14999) || training loss 0.1923 || training accuracy 94.38% || lr 4.523861127834638e-06\n","Epoch[4/10](5480/14999) || training loss 0.1313 || training accuracy 96.25% || lr 4.52279082212857e-06\n","Epoch[4/10](5500/14999) || training loss 0.1699 || training accuracy 93.12% || lr 4.521720516422503e-06\n","Epoch[4/10](5520/14999) || training loss 0.1671 || training accuracy 93.12% || lr 4.520650210716436e-06\n","Epoch[4/10](5540/14999) || training loss 0.1227 || training accuracy 95.00% || lr 4.519579905010368e-06\n","Epoch[4/10](5560/14999) || training loss 0.1548 || training accuracy 95.00% || lr 4.518509599304301e-06\n","Epoch[4/10](5580/14999) || training loss 0.1465 || training accuracy 95.62% || lr 4.517439293598234e-06\n","Epoch[4/10](5600/14999) || training loss 0.08957 || training accuracy 97.50% || lr 4.516368987892167e-06\n","Epoch[4/10](5620/14999) || training loss 0.1224 || training accuracy 95.62% || lr 4.515298682186099e-06\n","Epoch[4/10](5640/14999) || training loss 0.131 || training accuracy 94.38% || lr 4.514228376480032e-06\n","Epoch[4/10](5660/14999) || training loss 0.1598 || training accuracy 93.75% || lr 4.513158070773965e-06\n","Epoch[4/10](5680/14999) || training loss 0.07892 || training accuracy 97.50% || lr 4.512087765067897e-06\n","Epoch[4/10](5700/14999) || training loss 0.1819 || training accuracy 96.25% || lr 4.51101745936183e-06\n","Epoch[4/10](5720/14999) || training loss 0.1804 || training accuracy 93.12% || lr 4.509947153655763e-06\n","Epoch[4/10](5740/14999) || training loss 0.1071 || training accuracy 95.62% || lr 4.508876847949695e-06\n","Epoch[4/10](5760/14999) || training loss 0.1461 || training accuracy 95.00% || lr 4.507806542243628e-06\n","Epoch[4/10](5780/14999) || training loss 0.07072 || training accuracy 97.50% || lr 4.506736236537561e-06\n","Epoch[4/10](5800/14999) || training loss 0.1476 || training accuracy 96.25% || lr 4.505665930831494e-06\n","Epoch[4/10](5820/14999) || training loss 0.1508 || training accuracy 95.62% || lr 4.504595625125426e-06\n","Epoch[4/10](5840/14999) || training loss 0.1509 || training accuracy 94.38% || lr 4.503525319419359e-06\n","Epoch[4/10](5860/14999) || training loss 0.1328 || training accuracy 94.38% || lr 4.502455013713292e-06\n","Epoch[4/10](5880/14999) || training loss 0.1062 || training accuracy 95.62% || lr 4.501384708007224e-06\n","Epoch[4/10](5900/14999) || training loss 0.1058 || training accuracy 95.62% || lr 4.500314402301157e-06\n","Epoch[4/10](5920/14999) || training loss 0.0999 || training accuracy 95.62% || lr 4.49924409659509e-06\n","Epoch[4/10](5940/14999) || training loss 0.0954 || training accuracy 96.25% || lr 4.4981737908890225e-06\n","Epoch[4/10](5960/14999) || training loss 0.1297 || training accuracy 93.12% || lr 4.497103485182955e-06\n","Epoch[4/10](5980/14999) || training loss 0.1507 || training accuracy 93.12% || lr 4.496033179476888e-06\n","Epoch[4/10](6000/14999) || training loss 0.08424 || training accuracy 96.88% || lr 4.494962873770821e-06\n","Epoch[4/10](6020/14999) || training loss 0.1235 || training accuracy 96.25% || lr 4.493892568064753e-06\n","Epoch[4/10](6040/14999) || training loss 0.1039 || training accuracy 96.25% || lr 4.492822262358686e-06\n","Epoch[4/10](6060/14999) || training loss 0.07077 || training accuracy 98.12% || lr 4.491751956652619e-06\n","Epoch[4/10](6080/14999) || training loss 0.1174 || training accuracy 96.25% || lr 4.490681650946551e-06\n","Epoch[4/10](6100/14999) || training loss 0.1314 || training accuracy 93.75% || lr 4.489611345240484e-06\n","Epoch[4/10](6120/14999) || training loss 0.1022 || training accuracy 95.62% || lr 4.488541039534417e-06\n","Epoch[4/10](6140/14999) || training loss 0.1606 || training accuracy 93.75% || lr 4.48747073382835e-06\n","Epoch[4/10](6160/14999) || training loss 0.03895 || training accuracy 98.12% || lr 4.486400428122282e-06\n","Epoch[4/10](6180/14999) || training loss 0.1084 || training accuracy 95.62% || lr 4.485330122416215e-06\n","Epoch[4/10](6200/14999) || training loss 0.06576 || training accuracy 98.12% || lr 4.484259816710148e-06\n","Epoch[4/10](6220/14999) || training loss 0.08896 || training accuracy 96.88% || lr 4.4831895110040805e-06\n","Epoch[4/10](6240/14999) || training loss 0.1436 || training accuracy 95.00% || lr 4.482119205298013e-06\n","Epoch[4/10](6260/14999) || training loss 0.2006 || training accuracy 95.62% || lr 4.481048899591946e-06\n","Epoch[4/10](6280/14999) || training loss 0.1784 || training accuracy 92.50% || lr 4.479978593885878e-06\n","Epoch[4/10](6300/14999) || training loss 0.2196 || training accuracy 91.25% || lr 4.478908288179811e-06\n","Epoch[4/10](6320/14999) || training loss 0.05643 || training accuracy 98.75% || lr 4.477837982473744e-06\n","Epoch[4/10](6340/14999) || training loss 0.08376 || training accuracy 96.88% || lr 4.476767676767677e-06\n","Epoch[4/10](6360/14999) || training loss 0.06798 || training accuracy 97.50% || lr 4.475697371061609e-06\n","Epoch[4/10](6380/14999) || training loss 0.08277 || training accuracy 97.50% || lr 4.474627065355542e-06\n","Epoch[4/10](6400/14999) || training loss 0.141 || training accuracy 94.38% || lr 4.4735567596494746e-06\n","Epoch[4/10](6420/14999) || training loss 0.1065 || training accuracy 95.62% || lr 4.472486453943407e-06\n","Epoch[4/10](6440/14999) || training loss 0.1249 || training accuracy 96.88% || lr 4.47141614823734e-06\n","Epoch[4/10](6460/14999) || training loss 0.1368 || training accuracy 93.75% || lr 4.470345842531273e-06\n","Epoch[4/10](6480/14999) || training loss 0.1135 || training accuracy 93.75% || lr 4.469275536825205e-06\n","Epoch[4/10](6500/14999) || training loss 0.1364 || training accuracy 93.12% || lr 4.4682052311191384e-06\n","Epoch[4/10](6520/14999) || training loss 0.1829 || training accuracy 93.12% || lr 4.467134925413071e-06\n","Epoch[4/10](6540/14999) || training loss 0.2044 || training accuracy 93.75% || lr 4.466064619707004e-06\n","Epoch[4/10](6560/14999) || training loss 0.1398 || training accuracy 95.00% || lr 4.464994314000936e-06\n","Epoch[4/10](6580/14999) || training loss 0.1054 || training accuracy 96.88% || lr 4.463924008294869e-06\n","Epoch[4/10](6600/14999) || training loss 0.1176 || training accuracy 96.88% || lr 4.462853702588802e-06\n","Epoch[4/10](6620/14999) || training loss 0.119 || training accuracy 95.62% || lr 4.461783396882734e-06\n","Epoch[4/10](6640/14999) || training loss 0.09111 || training accuracy 95.62% || lr 4.460713091176667e-06\n","Epoch[4/10](6660/14999) || training loss 0.1061 || training accuracy 96.25% || lr 4.4596427854706e-06\n","Epoch[4/10](6680/14999) || training loss 0.102 || training accuracy 97.50% || lr 4.458572479764532e-06\n","Epoch[4/10](6700/14999) || training loss 0.1561 || training accuracy 93.12% || lr 4.457502174058465e-06\n","Epoch[4/10](6720/14999) || training loss 0.1781 || training accuracy 92.50% || lr 4.456431868352398e-06\n","Epoch[4/10](6740/14999) || training loss 0.1304 || training accuracy 95.00% || lr 4.455361562646331e-06\n","Epoch[4/10](6760/14999) || training loss 0.1518 || training accuracy 95.62% || lr 4.454291256940263e-06\n","Epoch[4/10](6780/14999) || training loss 0.1334 || training accuracy 95.62% || lr 4.453220951234196e-06\n","Epoch[4/10](6800/14999) || training loss 0.1389 || training accuracy 94.38% || lr 4.452150645528129e-06\n","Epoch[4/10](6820/14999) || training loss 0.07519 || training accuracy 96.88% || lr 4.451080339822061e-06\n","Epoch[4/10](6840/14999) || training loss 0.09982 || training accuracy 97.50% || lr 4.450010034115994e-06\n","Epoch[4/10](6860/14999) || training loss 0.1976 || training accuracy 93.12% || lr 4.448939728409927e-06\n","Epoch[4/10](6880/14999) || training loss 0.1352 || training accuracy 95.00% || lr 4.44786942270386e-06\n","Epoch[4/10](6900/14999) || training loss 0.08614 || training accuracy 96.88% || lr 4.446799116997792e-06\n","Epoch[4/10](6920/14999) || training loss 0.04402 || training accuracy 98.75% || lr 4.445728811291725e-06\n","Epoch[4/10](6940/14999) || training loss 0.1025 || training accuracy 97.50% || lr 4.444658505585658e-06\n","Epoch[4/10](6960/14999) || training loss 0.1187 || training accuracy 96.25% || lr 4.44358819987959e-06\n","Epoch[4/10](6980/14999) || training loss 0.1274 || training accuracy 96.25% || lr 4.442517894173523e-06\n","Epoch[4/10](7000/14999) || training loss 0.1067 || training accuracy 96.88% || lr 4.441447588467456e-06\n","Epoch[4/10](7020/14999) || training loss 0.132 || training accuracy 95.00% || lr 4.440377282761388e-06\n","Epoch[4/10](7040/14999) || training loss 0.1081 || training accuracy 96.88% || lr 4.439306977055321e-06\n","Epoch[4/10](7060/14999) || training loss 0.1985 || training accuracy 93.75% || lr 4.438236671349254e-06\n","Epoch[4/10](7080/14999) || training loss 0.1375 || training accuracy 95.62% || lr 4.437166365643187e-06\n","Epoch[4/10](7100/14999) || training loss 0.112 || training accuracy 95.00% || lr 4.436096059937119e-06\n","Epoch[4/10](7120/14999) || training loss 0.1431 || training accuracy 93.75% || lr 4.435025754231052e-06\n","Epoch[4/10](7140/14999) || training loss 0.1499 || training accuracy 95.62% || lr 4.433955448524985e-06\n","Epoch[4/10](7160/14999) || training loss 0.1561 || training accuracy 92.50% || lr 4.4328851428189174e-06\n","Epoch[4/10](7180/14999) || training loss 0.1606 || training accuracy 93.75% || lr 4.43181483711285e-06\n","Epoch[4/10](7200/14999) || training loss 0.1077 || training accuracy 95.62% || lr 4.430744531406783e-06\n","Epoch[4/10](7220/14999) || training loss 0.1262 || training accuracy 95.62% || lr 4.429674225700715e-06\n","Epoch[4/10](7240/14999) || training loss 0.1058 || training accuracy 96.25% || lr 4.4286039199946485e-06\n","Epoch[4/10](7260/14999) || training loss 0.1838 || training accuracy 93.12% || lr 4.427533614288581e-06\n","Epoch[4/10](7280/14999) || training loss 0.1637 || training accuracy 93.75% || lr 4.426463308582514e-06\n","Epoch[4/10](7300/14999) || training loss 0.134 || training accuracy 95.62% || lr 4.425393002876446e-06\n","Epoch[4/10](7320/14999) || training loss 0.1255 || training accuracy 95.62% || lr 4.424322697170379e-06\n","Epoch[4/10](7340/14999) || training loss 0.1069 || training accuracy 95.00% || lr 4.423252391464312e-06\n","Epoch[4/10](7360/14999) || training loss 0.1121 || training accuracy 96.25% || lr 4.422182085758244e-06\n","Epoch[4/10](7380/14999) || training loss 0.1446 || training accuracy 96.25% || lr 4.421111780052177e-06\n","Epoch[4/10](7400/14999) || training loss 0.1143 || training accuracy 95.62% || lr 4.42004147434611e-06\n","Epoch[4/10](7420/14999) || training loss 0.1438 || training accuracy 94.38% || lr 4.418971168640043e-06\n","Epoch[4/10](7440/14999) || training loss 0.2088 || training accuracy 91.88% || lr 4.417900862933975e-06\n","Epoch[4/10](7460/14999) || training loss 0.1582 || training accuracy 93.12% || lr 4.416830557227908e-06\n","Epoch[4/10](7480/14999) || training loss 0.1806 || training accuracy 92.50% || lr 4.415760251521841e-06\n","Epoch[4/10](7500/14999) || training loss 0.1772 || training accuracy 93.12% || lr 4.414689945815773e-06\n","Epoch[4/10](7520/14999) || training loss 0.08621 || training accuracy 96.88% || lr 4.4136196401097065e-06\n","Epoch[4/10](7540/14999) || training loss 0.1495 || training accuracy 93.75% || lr 4.412549334403639e-06\n","Epoch[4/10](7560/14999) || training loss 0.149 || training accuracy 94.38% || lr 4.411479028697571e-06\n","Epoch[4/10](7580/14999) || training loss 0.1697 || training accuracy 92.50% || lr 4.410408722991504e-06\n","Epoch[4/10](7600/14999) || training loss 0.08868 || training accuracy 97.50% || lr 4.409338417285437e-06\n","Epoch[4/10](7620/14999) || training loss 0.1508 || training accuracy 94.38% || lr 4.40826811157937e-06\n","Epoch[4/10](7640/14999) || training loss 0.145 || training accuracy 92.50% || lr 4.407197805873302e-06\n","Epoch[4/10](7660/14999) || training loss 0.1483 || training accuracy 93.75% || lr 4.406127500167235e-06\n","Epoch[4/10](7680/14999) || training loss 0.1477 || training accuracy 93.75% || lr 4.405057194461168e-06\n","Epoch[4/10](7700/14999) || training loss 0.07704 || training accuracy 98.12% || lr 4.4039868887551e-06\n","Epoch[4/10](7720/14999) || training loss 0.09535 || training accuracy 95.00% || lr 4.402916583049033e-06\n","Epoch[4/10](7740/14999) || training loss 0.09729 || training accuracy 96.88% || lr 4.401846277342966e-06\n","Epoch[4/10](7760/14999) || training loss 0.1196 || training accuracy 95.62% || lr 4.400775971636898e-06\n","Epoch[4/10](7780/14999) || training loss 0.075 || training accuracy 98.12% || lr 4.399705665930831e-06\n","Epoch[4/10](7800/14999) || training loss 0.05744 || training accuracy 97.50% || lr 4.3986353602247645e-06\n","Epoch[4/10](7820/14999) || training loss 0.1661 || training accuracy 93.75% || lr 4.397565054518697e-06\n","Epoch[4/10](7840/14999) || training loss 0.1721 || training accuracy 92.50% || lr 4.396494748812629e-06\n","Epoch[4/10](7860/14999) || training loss 0.1258 || training accuracy 94.38% || lr 4.395424443106562e-06\n","Epoch[4/10](7880/14999) || training loss 0.1363 || training accuracy 92.50% || lr 4.394354137400495e-06\n","Epoch[4/10](7900/14999) || training loss 0.1243 || training accuracy 95.00% || lr 4.3932838316944275e-06\n","Epoch[4/10](7920/14999) || training loss 0.08267 || training accuracy 96.88% || lr 4.39221352598836e-06\n","Epoch[4/10](7940/14999) || training loss 0.1057 || training accuracy 96.88% || lr 4.391143220282293e-06\n","Epoch[4/10](7960/14999) || training loss 0.1382 || training accuracy 94.38% || lr 4.390072914576225e-06\n","Epoch[4/10](7980/14999) || training loss 0.1241 || training accuracy 94.38% || lr 4.389002608870158e-06\n","Epoch[4/10](8000/14999) || training loss 0.07403 || training accuracy 98.12% || lr 4.387932303164091e-06\n","Epoch[4/10](8020/14999) || training loss 0.1068 || training accuracy 95.00% || lr 4.386861997458024e-06\n","Epoch[4/10](8040/14999) || training loss 0.131 || training accuracy 95.62% || lr 4.385791691751956e-06\n","Epoch[4/10](8060/14999) || training loss 0.1171 || training accuracy 95.62% || lr 4.384721386045889e-06\n","Epoch[4/10](8080/14999) || training loss 0.1791 || training accuracy 91.25% || lr 4.3836510803398225e-06\n","Epoch[4/10](8100/14999) || training loss 0.1207 || training accuracy 96.25% || lr 4.382580774633754e-06\n","Epoch[4/10](8120/14999) || training loss 0.1484 || training accuracy 94.38% || lr 4.381510468927687e-06\n","Epoch[4/10](8140/14999) || training loss 0.12 || training accuracy 96.88% || lr 4.38044016322162e-06\n","Epoch[4/10](8160/14999) || training loss 0.09403 || training accuracy 96.88% || lr 4.379369857515553e-06\n","Epoch[4/10](8180/14999) || training loss 0.123 || training accuracy 96.25% || lr 4.3782995518094855e-06\n","Epoch[4/10](8200/14999) || training loss 0.1446 || training accuracy 95.00% || lr 4.377229246103418e-06\n","Epoch[4/10](8220/14999) || training loss 0.1904 || training accuracy 93.75% || lr 4.376158940397351e-06\n","Epoch[4/10](8240/14999) || training loss 0.08705 || training accuracy 96.88% || lr 4.375088634691283e-06\n","Epoch[4/10](8260/14999) || training loss 0.2012 || training accuracy 91.88% || lr 4.374018328985217e-06\n","Epoch[4/10](8280/14999) || training loss 0.09074 || training accuracy 96.25% || lr 4.372948023279149e-06\n","Epoch[4/10](8300/14999) || training loss 0.1438 || training accuracy 94.38% || lr 4.371877717573081e-06\n","Epoch[4/10](8320/14999) || training loss 0.1073 || training accuracy 96.88% || lr 4.370807411867014e-06\n","Epoch[4/10](8340/14999) || training loss 0.116 || training accuracy 95.62% || lr 4.369737106160947e-06\n","Epoch[4/10](8360/14999) || training loss 0.1504 || training accuracy 95.00% || lr 4.3686668004548805e-06\n","Epoch[4/10](8380/14999) || training loss 0.1422 || training accuracy 94.38% || lr 4.367596494748812e-06\n","Epoch[4/10](8400/14999) || training loss 0.08044 || training accuracy 97.50% || lr 4.366526189042745e-06\n","Epoch[4/10](8420/14999) || training loss 0.1359 || training accuracy 96.88% || lr 4.365455883336678e-06\n","Epoch[4/10](8440/14999) || training loss 0.135 || training accuracy 94.38% || lr 4.36438557763061e-06\n","Epoch[4/10](8460/14999) || training loss 0.1212 || training accuracy 96.25% || lr 4.3633152719245435e-06\n","Epoch[4/10](8480/14999) || training loss 0.09931 || training accuracy 96.88% || lr 4.362244966218476e-06\n","Epoch[4/10](8500/14999) || training loss 0.1844 || training accuracy 93.75% || lr 4.361174660512408e-06\n","Epoch[4/10](8520/14999) || training loss 0.1552 || training accuracy 93.75% || lr 4.360104354806341e-06\n","Epoch[4/10](8540/14999) || training loss 0.151 || training accuracy 93.75% || lr 4.3590340491002746e-06\n","Epoch[4/10](8560/14999) || training loss 0.1587 || training accuracy 95.00% || lr 4.357963743394207e-06\n","Epoch[4/10](8580/14999) || training loss 0.09749 || training accuracy 96.25% || lr 4.356893437688139e-06\n","Epoch[4/10](8600/14999) || training loss 0.1227 || training accuracy 95.62% || lr 4.355823131982072e-06\n","Epoch[4/10](8620/14999) || training loss 0.07658 || training accuracy 98.75% || lr 4.354752826276005e-06\n","Epoch[4/10](8640/14999) || training loss 0.1449 || training accuracy 93.75% || lr 4.353682520569938e-06\n","Epoch[4/10](8660/14999) || training loss 0.1901 || training accuracy 93.12% || lr 4.35261221486387e-06\n","Epoch[4/10](8680/14999) || training loss 0.1235 || training accuracy 95.62% || lr 4.351541909157803e-06\n","Epoch[4/10](8700/14999) || training loss 0.09398 || training accuracy 97.50% || lr 4.350471603451736e-06\n","Epoch[4/10](8720/14999) || training loss 0.104 || training accuracy 95.00% || lr 4.349401297745668e-06\n","Epoch[4/10](8740/14999) || training loss 0.1397 || training accuracy 95.62% || lr 4.3483309920396015e-06\n","Epoch[4/10](8760/14999) || training loss 0.1008 || training accuracy 95.00% || lr 4.347260686333534e-06\n","Epoch[4/10](8780/14999) || training loss 0.07282 || training accuracy 96.88% || lr 4.346190380627466e-06\n","Epoch[4/10](8800/14999) || training loss 0.1154 || training accuracy 96.88% || lr 4.345120074921399e-06\n","Epoch[4/10](8820/14999) || training loss 0.08877 || training accuracy 95.62% || lr 4.3440497692153326e-06\n","Epoch[4/10](8840/14999) || training loss 0.2299 || training accuracy 92.50% || lr 4.3429794635092645e-06\n","Epoch[4/10](8860/14999) || training loss 0.09821 || training accuracy 96.88% || lr 4.341909157803197e-06\n","Epoch[4/10](8880/14999) || training loss 0.1087 || training accuracy 95.00% || lr 4.34083885209713e-06\n","Epoch[4/10](8900/14999) || training loss 0.1445 || training accuracy 92.50% || lr 4.339768546391063e-06\n","Epoch[4/10](8920/14999) || training loss 0.1388 || training accuracy 95.62% || lr 4.338698240684996e-06\n","Epoch[4/10](8940/14999) || training loss 0.1545 || training accuracy 91.88% || lr 4.337627934978928e-06\n","Epoch[4/10](8960/14999) || training loss 0.1011 || training accuracy 95.62% || lr 4.336557629272861e-06\n","Epoch[4/10](8980/14999) || training loss 0.1138 || training accuracy 97.50% || lr 4.335487323566793e-06\n","Epoch[4/10](9000/14999) || training loss 0.07146 || training accuracy 96.88% || lr 4.334417017860726e-06\n","Epoch[4/10](9020/14999) || training loss 0.1197 || training accuracy 97.50% || lr 4.3333467121546594e-06\n","Epoch[4/10](9040/14999) || training loss 0.07453 || training accuracy 97.50% || lr 4.332276406448591e-06\n","Epoch[4/10](9060/14999) || training loss 0.0699 || training accuracy 98.12% || lr 4.331206100742524e-06\n","Epoch[4/10](9080/14999) || training loss 0.07032 || training accuracy 98.12% || lr 4.330135795036457e-06\n","Epoch[4/10](9100/14999) || training loss 0.09901 || training accuracy 96.25% || lr 4.3290654893303905e-06\n","Epoch[4/10](9120/14999) || training loss 0.1096 || training accuracy 96.88% || lr 4.3279951836243225e-06\n","Epoch[4/10](9140/14999) || training loss 0.2172 || training accuracy 93.12% || lr 4.326924877918255e-06\n","Epoch[4/10](9160/14999) || training loss 0.1004 || training accuracy 96.88% || lr 4.325854572212188e-06\n","Epoch[4/10](9180/14999) || training loss 0.1057 || training accuracy 96.25% || lr 4.32478426650612e-06\n","Epoch[4/10](9200/14999) || training loss 0.1544 || training accuracy 95.62% || lr 4.3237139608000536e-06\n","Epoch[4/10](9220/14999) || training loss 0.1711 || training accuracy 93.75% || lr 4.322643655093986e-06\n","Epoch[4/10](9240/14999) || training loss 0.1309 || training accuracy 95.00% || lr 4.321573349387918e-06\n","Epoch[4/10](9260/14999) || training loss 0.1606 || training accuracy 93.12% || lr 4.320503043681851e-06\n","Epoch[4/10](9280/14999) || training loss 0.08446 || training accuracy 98.12% || lr 4.319432737975784e-06\n","Epoch[4/10](9300/14999) || training loss 0.08686 || training accuracy 97.50% || lr 4.3183624322697174e-06\n","Epoch[4/10](9320/14999) || training loss 0.1369 || training accuracy 95.00% || lr 4.317292126563649e-06\n","Epoch[4/10](9340/14999) || training loss 0.1464 || training accuracy 96.25% || lr 4.316221820857582e-06\n","Epoch[4/10](9360/14999) || training loss 0.1373 || training accuracy 96.25% || lr 4.315151515151515e-06\n","Epoch[4/10](9380/14999) || training loss 0.1329 || training accuracy 94.38% || lr 4.314081209445448e-06\n","Epoch[4/10](9400/14999) || training loss 0.08386 || training accuracy 98.12% || lr 4.3130109037393805e-06\n","Epoch[4/10](9420/14999) || training loss 0.1658 || training accuracy 93.12% || lr 4.311940598033313e-06\n","Epoch[4/10](9440/14999) || training loss 0.1052 || training accuracy 94.38% || lr 4.310870292327246e-06\n","Epoch[4/10](9460/14999) || training loss 0.1821 || training accuracy 91.88% || lr 4.309799986621178e-06\n","Epoch[4/10](9480/14999) || training loss 0.1346 || training accuracy 93.75% || lr 4.3087296809151115e-06\n","Epoch[4/10](9500/14999) || training loss 0.09967 || training accuracy 96.88% || lr 4.307659375209044e-06\n","Epoch[4/10](9520/14999) || training loss 0.1552 || training accuracy 94.38% || lr 4.306589069502976e-06\n","Epoch[4/10](9540/14999) || training loss 0.09949 || training accuracy 96.25% || lr 4.305518763796909e-06\n","Epoch[4/10](9560/14999) || training loss 0.1409 || training accuracy 94.38% || lr 4.304448458090843e-06\n","Epoch[4/10](9580/14999) || training loss 0.08488 || training accuracy 97.50% || lr 4.3033781523847746e-06\n","Epoch[4/10](9600/14999) || training loss 0.1419 || training accuracy 95.00% || lr 4.302307846678707e-06\n","Epoch[4/10](9620/14999) || training loss 0.1681 || training accuracy 93.75% || lr 4.30123754097264e-06\n","Epoch[4/10](9640/14999) || training loss 0.1435 || training accuracy 95.00% || lr 4.300167235266573e-06\n","Epoch[4/10](9660/14999) || training loss 0.1302 || training accuracy 95.62% || lr 4.299096929560506e-06\n","Epoch[4/10](9680/14999) || training loss 0.1615 || training accuracy 93.75% || lr 4.2980266238544384e-06\n","Epoch[4/10](9700/14999) || training loss 0.1545 || training accuracy 93.12% || lr 4.296956318148371e-06\n","Epoch[4/10](9720/14999) || training loss 0.1946 || training accuracy 93.75% || lr 4.295886012442303e-06\n","Epoch[4/10](9740/14999) || training loss 0.14 || training accuracy 95.62% || lr 4.294815706736236e-06\n","Epoch[4/10](9760/14999) || training loss 0.0926 || training accuracy 95.00% || lr 4.2937454010301695e-06\n","Epoch[4/10](9780/14999) || training loss 0.1503 || training accuracy 93.75% || lr 4.2926750953241015e-06\n","Epoch[4/10](9800/14999) || training loss 0.1468 || training accuracy 93.12% || lr 4.291604789618034e-06\n","Epoch[4/10](9820/14999) || training loss 0.1026 || training accuracy 96.88% || lr 4.290534483911967e-06\n","Epoch[4/10](9840/14999) || training loss 0.06945 || training accuracy 98.12% || lr 4.289464178205901e-06\n","Epoch[4/10](9860/14999) || training loss 0.1392 || training accuracy 94.38% || lr 4.2883938724998326e-06\n","Epoch[4/10](9880/14999) || training loss 0.101 || training accuracy 96.88% || lr 4.287323566793765e-06\n","Epoch[4/10](9900/14999) || training loss 0.1392 || training accuracy 94.38% || lr 4.286253261087698e-06\n","Epoch[4/10](9920/14999) || training loss 0.1487 || training accuracy 95.62% || lr 4.28518295538163e-06\n","Epoch[4/10](9940/14999) || training loss 0.1527 || training accuracy 95.00% || lr 4.284112649675564e-06\n","Epoch[4/10](9960/14999) || training loss 0.132 || training accuracy 95.00% || lr 4.283042343969496e-06\n","Epoch[4/10](9980/14999) || training loss 0.116 || training accuracy 96.25% || lr 4.281972038263429e-06\n","Epoch[4/10](10000/14999) || training loss 0.1112 || training accuracy 96.25% || lr 4.280901732557361e-06\n","Epoch[4/10](10020/14999) || training loss 0.1658 || training accuracy 93.75% || lr 4.279831426851294e-06\n","Epoch[4/10](10040/14999) || training loss 0.1256 || training accuracy 96.88% || lr 4.2787611211452275e-06\n","Epoch[4/10](10060/14999) || training loss 0.1624 || training accuracy 94.38% || lr 4.2776908154391594e-06\n","Epoch[4/10](10080/14999) || training loss 0.1178 || training accuracy 96.25% || lr 4.276620509733092e-06\n","Epoch[4/10](10100/14999) || training loss 0.07765 || training accuracy 98.12% || lr 4.275550204027025e-06\n","Epoch[4/10](10120/14999) || training loss 0.1302 || training accuracy 95.62% || lr 4.274479898320958e-06\n","Epoch[4/10](10140/14999) || training loss 0.1016 || training accuracy 95.62% || lr 4.2734095926148905e-06\n","Epoch[4/10](10160/14999) || training loss 0.1798 || training accuracy 93.75% || lr 4.272339286908823e-06\n","Epoch[4/10](10180/14999) || training loss 0.09223 || training accuracy 96.25% || lr 4.271268981202756e-06\n","Epoch[4/10](10200/14999) || training loss 0.07538 || training accuracy 97.50% || lr 4.270198675496688e-06\n","Epoch[4/10](10220/14999) || training loss 0.06797 || training accuracy 97.50% || lr 4.269128369790622e-06\n","Epoch[4/10](10240/14999) || training loss 0.1019 || training accuracy 96.88% || lr 4.268058064084554e-06\n","Epoch[4/10](10260/14999) || training loss 0.07559 || training accuracy 98.12% || lr 4.266987758378486e-06\n","Epoch[4/10](10280/14999) || training loss 0.1355 || training accuracy 95.62% || lr 4.265917452672419e-06\n","Epoch[4/10](10300/14999) || training loss 0.1028 || training accuracy 96.25% || lr 4.264847146966352e-06\n","Epoch[4/10](10320/14999) || training loss 0.122 || training accuracy 95.62% || lr 4.263776841260285e-06\n","Epoch[4/10](10340/14999) || training loss 0.1756 || training accuracy 93.75% || lr 4.2627065355542174e-06\n","Epoch[4/10](10360/14999) || training loss 0.1072 || training accuracy 95.00% || lr 4.26163622984815e-06\n","Epoch[4/10](10380/14999) || training loss 0.08005 || training accuracy 97.50% || lr 4.260565924142083e-06\n","Epoch[4/10](10400/14999) || training loss 0.09288 || training accuracy 96.25% || lr 4.259495618436016e-06\n","Epoch[4/10](10420/14999) || training loss 0.1488 || training accuracy 93.75% || lr 4.2584253127299485e-06\n","Epoch[4/10](10440/14999) || training loss 0.06938 || training accuracy 96.88% || lr 4.257355007023881e-06\n","Epoch[4/10](10460/14999) || training loss 0.1904 || training accuracy 93.75% || lr 4.256284701317813e-06\n","Epoch[4/10](10480/14999) || training loss 0.1133 || training accuracy 95.00% || lr 4.255214395611746e-06\n","Epoch[4/10](10500/14999) || training loss 0.1415 || training accuracy 95.62% || lr 4.25414408990568e-06\n","Epoch[4/10](10520/14999) || training loss 0.1062 || training accuracy 94.38% || lr 4.2530737841996115e-06\n","Epoch[4/10](10540/14999) || training loss 0.1059 || training accuracy 95.62% || lr 4.252003478493544e-06\n","Epoch[4/10](10560/14999) || training loss 0.1316 || training accuracy 93.75% || lr 4.250933172787477e-06\n","Epoch[4/10](10580/14999) || training loss 0.2259 || training accuracy 90.62% || lr 4.24986286708141e-06\n","Epoch[4/10](10600/14999) || training loss 0.1396 || training accuracy 92.50% || lr 4.248792561375343e-06\n","Epoch[4/10](10620/14999) || training loss 0.1196 || training accuracy 96.25% || lr 4.247722255669275e-06\n","Epoch[4/10](10640/14999) || training loss 0.1036 || training accuracy 95.00% || lr 4.246651949963208e-06\n","Epoch[4/10](10660/14999) || training loss 0.1206 || training accuracy 96.88% || lr 4.24558164425714e-06\n","Epoch[4/10](10680/14999) || training loss 0.1281 || training accuracy 96.25% || lr 4.244511338551074e-06\n","Epoch[4/10](10700/14999) || training loss 0.1316 || training accuracy 93.75% || lr 4.2434410328450065e-06\n","Epoch[4/10](10720/14999) || training loss 0.08435 || training accuracy 96.88% || lr 4.242370727138939e-06\n","Epoch[4/10](10740/14999) || training loss 0.1543 || training accuracy 93.12% || lr 4.241300421432871e-06\n","Epoch[4/10](10760/14999) || training loss 0.1062 || training accuracy 96.88% || lr 4.240230115726804e-06\n","Epoch[4/10](10780/14999) || training loss 0.1639 || training accuracy 94.38% || lr 4.239159810020738e-06\n","Epoch[4/10](10800/14999) || training loss 0.1278 || training accuracy 95.62% || lr 4.2380895043146695e-06\n","Epoch[4/10](10820/14999) || training loss 0.1325 || training accuracy 95.62% || lr 4.237019198608602e-06\n","Epoch[4/10](10840/14999) || training loss 0.03773 || training accuracy 98.75% || lr 4.235948892902535e-06\n","Epoch[4/10](10860/14999) || training loss 0.1174 || training accuracy 96.88% || lr 4.234878587196467e-06\n","Epoch[4/10](10880/14999) || training loss 0.08616 || training accuracy 96.88% || lr 4.233808281490401e-06\n","Epoch[4/10](10900/14999) || training loss 0.0989 || training accuracy 97.50% || lr 4.232737975784333e-06\n","Epoch[4/10](10920/14999) || training loss 0.07167 || training accuracy 98.12% || lr 4.231667670078266e-06\n","Epoch[4/10](10940/14999) || training loss 0.1444 || training accuracy 93.75% || lr 4.230597364372198e-06\n","Epoch[4/10](10960/14999) || training loss 0.1698 || training accuracy 94.38% || lr 4.229527058666132e-06\n","Epoch[4/10](10980/14999) || training loss 0.1145 || training accuracy 96.25% || lr 4.2284567529600645e-06\n","Epoch[4/10](11000/14999) || training loss 0.1104 || training accuracy 96.88% || lr 4.227386447253996e-06\n","Epoch[4/10](11020/14999) || training loss 0.1026 || training accuracy 97.50% || lr 4.226316141547929e-06\n","Epoch[4/10](11040/14999) || training loss 0.09333 || training accuracy 96.88% || lr 4.225245835841862e-06\n","Epoch[4/10](11060/14999) || training loss 0.06864 || training accuracy 98.12% || lr 4.224175530135795e-06\n","Epoch[4/10](11080/14999) || training loss 0.1161 || training accuracy 96.25% || lr 4.2231052244297275e-06\n","Epoch[4/10](11100/14999) || training loss 0.09733 || training accuracy 97.50% || lr 4.22203491872366e-06\n","Epoch[4/10](11120/14999) || training loss 0.1382 || training accuracy 93.75% || lr 4.220964613017593e-06\n","Epoch[4/10](11140/14999) || training loss 0.187 || training accuracy 92.50% || lr 4.219894307311525e-06\n","Epoch[4/10](11160/14999) || training loss 0.07811 || training accuracy 96.88% || lr 4.218824001605459e-06\n","Epoch[4/10](11180/14999) || training loss 0.09327 || training accuracy 95.62% || lr 4.217753695899391e-06\n","Epoch[4/10](11200/14999) || training loss 0.09959 || training accuracy 95.62% || lr 4.216683390193323e-06\n","Epoch[4/10](11220/14999) || training loss 0.141 || training accuracy 95.00% || lr 4.215613084487256e-06\n","Epoch[4/10](11240/14999) || training loss 0.1616 || training accuracy 92.50% || lr 4.21454277878119e-06\n","Epoch[4/10](11260/14999) || training loss 0.08859 || training accuracy 96.25% || lr 4.2134724730751225e-06\n","Epoch[4/10](11280/14999) || training loss 0.1398 || training accuracy 94.38% || lr 4.212402167369054e-06\n","Epoch[4/10](11300/14999) || training loss 0.08267 || training accuracy 97.50% || lr 4.211331861662987e-06\n","Epoch[4/10](11320/14999) || training loss 0.1781 || training accuracy 92.50% || lr 4.21026155595692e-06\n","Epoch[4/10](11340/14999) || training loss 0.1499 || training accuracy 95.00% || lr 4.209191250250853e-06\n","Epoch[4/10](11360/14999) || training loss 0.1471 || training accuracy 94.38% || lr 4.2081209445447855e-06\n","Epoch[4/10](11380/14999) || training loss 0.1517 || training accuracy 93.75% || lr 4.207050638838718e-06\n","Epoch[4/10](11400/14999) || training loss 0.09337 || training accuracy 96.88% || lr 4.20598033313265e-06\n","Epoch[4/10](11420/14999) || training loss 0.09806 || training accuracy 95.00% || lr 4.204910027426584e-06\n","Epoch[4/10](11440/14999) || training loss 0.0954 || training accuracy 98.12% || lr 4.203839721720517e-06\n","Epoch[4/10](11460/14999) || training loss 0.07033 || training accuracy 97.50% || lr 4.202769416014449e-06\n","Epoch[4/10](11480/14999) || training loss 0.09558 || training accuracy 96.25% || lr 4.201699110308381e-06\n","Epoch[4/10](11500/14999) || training loss 0.1085 || training accuracy 95.00% || lr 4.200628804602314e-06\n","Epoch[4/10](11520/14999) || training loss 0.09167 || training accuracy 96.25% || lr 4.199558498896248e-06\n","Epoch[4/10](11540/14999) || training loss 0.1555 || training accuracy 95.00% || lr 4.19848819319018e-06\n","Epoch[4/10](11560/14999) || training loss 0.08367 || training accuracy 96.88% || lr 4.197417887484112e-06\n","Epoch[4/10](11580/14999) || training loss 0.1682 || training accuracy 94.38% || lr 4.196347581778045e-06\n","Epoch[4/10](11600/14999) || training loss 0.06608 || training accuracy 96.88% || lr 4.195277276071977e-06\n","Epoch[4/10](11620/14999) || training loss 0.07654 || training accuracy 97.50% || lr 4.194206970365911e-06\n","Epoch[4/10](11640/14999) || training loss 0.05809 || training accuracy 98.12% || lr 4.1931366646598435e-06\n","Epoch[4/10](11660/14999) || training loss 0.1524 || training accuracy 93.75% || lr 4.192066358953776e-06\n","Epoch[4/10](11680/14999) || training loss 0.1122 || training accuracy 95.62% || lr 4.190996053247708e-06\n","Epoch[4/10](11700/14999) || training loss 0.09216 || training accuracy 97.50% || lr 4.189925747541642e-06\n","Epoch[4/10](11720/14999) || training loss 0.2478 || training accuracy 93.12% || lr 4.1888554418355746e-06\n","Epoch[4/10](11740/14999) || training loss 0.1705 || training accuracy 93.75% || lr 4.1877851361295065e-06\n","Epoch[4/10](11760/14999) || training loss 0.142 || training accuracy 93.75% || lr 4.186714830423439e-06\n","Epoch[4/10](11780/14999) || training loss 0.1057 || training accuracy 96.88% || lr 4.185644524717372e-06\n","Epoch[4/10](11800/14999) || training loss 0.1045 || training accuracy 96.88% || lr 4.184574219011305e-06\n","Epoch[4/10](11820/14999) || training loss 0.1413 || training accuracy 93.75% || lr 4.183503913305238e-06\n","Epoch[4/10](11840/14999) || training loss 0.1293 || training accuracy 94.38% || lr 4.18243360759917e-06\n","Epoch[4/10](11860/14999) || training loss 0.1706 || training accuracy 93.12% || lr 4.181363301893103e-06\n","Epoch[4/10](11880/14999) || training loss 0.1297 || training accuracy 93.12% || lr 4.180292996187035e-06\n","Epoch[4/10](11900/14999) || training loss 0.1571 || training accuracy 93.75% || lr 4.179222690480969e-06\n","Epoch[4/10](11920/14999) || training loss 0.1223 || training accuracy 95.00% || lr 4.1781523847749015e-06\n","Epoch[4/10](11940/14999) || training loss 0.1117 || training accuracy 95.00% || lr 4.177082079068833e-06\n","Epoch[4/10](11960/14999) || training loss 0.1074 || training accuracy 95.62% || lr 4.176011773362766e-06\n","Epoch[4/10](11980/14999) || training loss 0.08999 || training accuracy 96.25% || lr 4.1749414676567e-06\n","Epoch[4/10](12000/14999) || training loss 0.148 || training accuracy 93.75% || lr 4.1738711619506326e-06\n","Epoch[4/10](12020/14999) || training loss 0.1262 || training accuracy 96.25% || lr 4.1728008562445645e-06\n","Epoch[4/10](12040/14999) || training loss 0.09434 || training accuracy 95.62% || lr 4.171730550538497e-06\n","Epoch[4/10](12060/14999) || training loss 0.1566 || training accuracy 95.00% || lr 4.17066024483243e-06\n","Epoch[4/10](12080/14999) || training loss 0.1872 || training accuracy 93.12% || lr 4.169589939126363e-06\n","Epoch[4/10](12100/14999) || training loss 0.1376 || training accuracy 95.00% || lr 4.168519633420296e-06\n","Epoch[4/10](12120/14999) || training loss 0.1243 || training accuracy 95.00% || lr 4.167449327714228e-06\n","Epoch[4/10](12140/14999) || training loss 0.1073 || training accuracy 96.25% || lr 4.16637902200816e-06\n","Epoch[4/10](12160/14999) || training loss 0.1481 || training accuracy 95.62% || lr 4.165308716302093e-06\n","Epoch[4/10](12180/14999) || training loss 0.09588 || training accuracy 95.00% || lr 4.164238410596027e-06\n","Epoch[4/10](12200/14999) || training loss 0.1358 || training accuracy 94.38% || lr 4.1631681048899594e-06\n","Epoch[4/10](12220/14999) || training loss 0.1469 || training accuracy 92.50% || lr 4.162097799183891e-06\n","Epoch[4/10](12240/14999) || training loss 0.1183 || training accuracy 97.50% || lr 4.161027493477824e-06\n","Epoch[4/10](12260/14999) || training loss 0.1186 || training accuracy 95.62% || lr 4.159957187771758e-06\n","Epoch[4/10](12280/14999) || training loss 0.1266 || training accuracy 96.25% || lr 4.15888688206569e-06\n","Epoch[4/10](12300/14999) || training loss 0.1232 || training accuracy 97.50% || lr 4.1578165763596225e-06\n","Epoch[4/10](12320/14999) || training loss 0.1555 || training accuracy 93.12% || lr 4.156746270653555e-06\n","Epoch[4/10](12340/14999) || training loss 0.1172 || training accuracy 95.00% || lr 4.155675964947487e-06\n","Epoch[4/10](12360/14999) || training loss 0.2845 || training accuracy 90.62% || lr 4.154605659241421e-06\n","Epoch[4/10](12380/14999) || training loss 0.08377 || training accuracy 98.75% || lr 4.1535353535353536e-06\n","Epoch[4/10](12400/14999) || training loss 0.08364 || training accuracy 96.25% || lr 4.152465047829286e-06\n","Epoch[4/10](12420/14999) || training loss 0.09663 || training accuracy 95.62% || lr 4.151394742123218e-06\n","Epoch[4/10](12440/14999) || training loss 0.1138 || training accuracy 96.25% || lr 4.150324436417152e-06\n","Epoch[4/10](12460/14999) || training loss 0.1173 || training accuracy 96.25% || lr 4.149254130711085e-06\n","Epoch[4/10](12480/14999) || training loss 0.09152 || training accuracy 95.62% || lr 4.148183825005017e-06\n","Epoch[4/10](12500/14999) || training loss 0.1238 || training accuracy 94.38% || lr 4.147113519298949e-06\n","Epoch[4/10](12520/14999) || training loss 0.1649 || training accuracy 95.62% || lr 4.146043213592882e-06\n","Epoch[4/10](12540/14999) || training loss 0.1584 || training accuracy 95.00% || lr 4.144972907886816e-06\n","Epoch[4/10](12560/14999) || training loss 0.1007 || training accuracy 95.62% || lr 4.143902602180748e-06\n","Epoch[4/10](12580/14999) || training loss 0.1506 || training accuracy 94.38% || lr 4.1428322964746805e-06\n","Epoch[4/10](12600/14999) || training loss 0.08237 || training accuracy 96.25% || lr 4.141761990768613e-06\n","Epoch[4/10](12620/14999) || training loss 0.1059 || training accuracy 96.25% || lr 4.140691685062545e-06\n","Epoch[4/10](12640/14999) || training loss 0.1736 || training accuracy 94.38% || lr 4.139621379356479e-06\n","Epoch[4/10](12660/14999) || training loss 0.1065 || training accuracy 95.00% || lr 4.1385510736504115e-06\n","Epoch[4/10](12680/14999) || training loss 0.1466 || training accuracy 94.38% || lr 4.1374807679443435e-06\n","Epoch[4/10](12700/14999) || training loss 0.1678 || training accuracy 91.88% || lr 4.136410462238276e-06\n","Epoch[4/10](12720/14999) || training loss 0.1337 || training accuracy 95.62% || lr 4.13534015653221e-06\n","Epoch[4/10](12740/14999) || training loss 0.1009 || training accuracy 97.50% || lr 4.134269850826143e-06\n","Epoch[4/10](12760/14999) || training loss 0.1231 || training accuracy 95.00% || lr 4.1331995451200746e-06\n","Epoch[4/10](12780/14999) || training loss 0.1513 || training accuracy 96.25% || lr 4.132129239414007e-06\n","Epoch[4/10](12800/14999) || training loss 0.1224 || training accuracy 94.38% || lr 4.13105893370794e-06\n","Epoch[4/10](12820/14999) || training loss 0.1441 || training accuracy 93.75% || lr 4.129988628001873e-06\n","Epoch[4/10](12840/14999) || training loss 0.1014 || training accuracy 95.62% || lr 4.128918322295806e-06\n","Epoch[4/10](12860/14999) || training loss 0.129 || training accuracy 95.62% || lr 4.1278480165897384e-06\n","Epoch[4/10](12880/14999) || training loss 0.1499 || training accuracy 95.00% || lr 4.12677771088367e-06\n","Epoch[4/10](12900/14999) || training loss 0.1286 || training accuracy 93.75% || lr 4.125707405177603e-06\n","Epoch[4/10](12920/14999) || training loss 0.1018 || training accuracy 95.00% || lr 4.124637099471537e-06\n","Epoch[4/10](12940/14999) || training loss 0.2067 || training accuracy 92.50% || lr 4.1235667937654695e-06\n","Epoch[4/10](12960/14999) || training loss 0.1653 || training accuracy 92.50% || lr 4.1224964880594015e-06\n","Epoch[4/10](12980/14999) || training loss 0.1121 || training accuracy 97.50% || lr 4.121426182353334e-06\n","Epoch[4/10](13000/14999) || training loss 0.1825 || training accuracy 91.88% || lr 4.120355876647268e-06\n","Epoch[4/10](13020/14999) || training loss 0.1385 || training accuracy 96.25% || lr 4.1192855709412e-06\n","Epoch[4/10](13040/14999) || training loss 0.09232 || training accuracy 96.25% || lr 4.1182152652351326e-06\n","Epoch[4/10](13060/14999) || training loss 0.09211 || training accuracy 96.88% || lr 4.117144959529065e-06\n","Epoch[4/10](13080/14999) || training loss 0.1088 || training accuracy 96.88% || lr 4.116074653822997e-06\n","Epoch[4/10](13100/14999) || training loss 0.1705 || training accuracy 93.75% || lr 4.115004348116931e-06\n","Epoch[4/10](13120/14999) || training loss 0.1147 || training accuracy 96.25% || lr 4.113934042410864e-06\n","Epoch[4/10](13140/14999) || training loss 0.1419 || training accuracy 96.25% || lr 4.112863736704796e-06\n","Epoch[4/10](13160/14999) || training loss 0.1239 || training accuracy 96.88% || lr 4.111793430998728e-06\n","Epoch[4/10](13180/14999) || training loss 0.1323 || training accuracy 95.00% || lr 4.110723125292661e-06\n","Epoch[4/10](13200/14999) || training loss 0.1224 || training accuracy 96.25% || lr 4.109652819586595e-06\n","Epoch[4/10](13220/14999) || training loss 0.09033 || training accuracy 95.62% || lr 4.108582513880527e-06\n","Epoch[4/10](13240/14999) || training loss 0.1103 || training accuracy 96.25% || lr 4.1075122081744594e-06\n","Epoch[4/10](13260/14999) || training loss 0.04752 || training accuracy 98.75% || lr 4.106441902468392e-06\n","Epoch[4/10](13280/14999) || training loss 0.1156 || training accuracy 95.62% || lr 4.105371596762326e-06\n","Epoch[4/10](13300/14999) || training loss 0.1204 || training accuracy 95.62% || lr 4.104301291056258e-06\n","Epoch[4/10](13320/14999) || training loss 0.1277 || training accuracy 94.38% || lr 4.1032309853501905e-06\n","Epoch[4/10](13340/14999) || training loss 0.1466 || training accuracy 94.38% || lr 4.102160679644123e-06\n","Epoch[4/10](13360/14999) || training loss 0.1274 || training accuracy 95.00% || lr 4.101090373938055e-06\n","Epoch[4/10](13380/14999) || training loss 0.1054 || training accuracy 95.62% || lr 4.100020068231989e-06\n","Epoch[4/10](13400/14999) || training loss 0.09696 || training accuracy 95.62% || lr 4.098949762525922e-06\n","Epoch[4/10](13420/14999) || training loss 0.1372 || training accuracy 95.00% || lr 4.0978794568198536e-06\n","Epoch[4/10](13440/14999) || training loss 0.1381 || training accuracy 93.75% || lr 4.096809151113786e-06\n","Epoch[4/10](13460/14999) || training loss 0.1114 || training accuracy 95.62% || lr 4.095738845407719e-06\n","Epoch[4/10](13480/14999) || training loss 0.08999 || training accuracy 98.12% || lr 4.094668539701653e-06\n","Epoch[4/10](13500/14999) || training loss 0.07261 || training accuracy 98.12% || lr 4.093598233995585e-06\n","Epoch[4/10](13520/14999) || training loss 0.1158 || training accuracy 96.88% || lr 4.0925279282895174e-06\n","Epoch[4/10](13540/14999) || training loss 0.08073 || training accuracy 97.50% || lr 4.09145762258345e-06\n","Epoch[4/10](13560/14999) || training loss 0.06391 || training accuracy 97.50% || lr 4.090387316877383e-06\n","Epoch[4/10](13580/14999) || training loss 0.1397 || training accuracy 96.25% || lr 4.089317011171316e-06\n","Epoch[4/10](13600/14999) || training loss 0.1123 || training accuracy 96.25% || lr 4.0882467054652485e-06\n","Epoch[4/10](13620/14999) || training loss 0.1097 || training accuracy 96.25% || lr 4.0871763997591804e-06\n","Epoch[4/10](13640/14999) || training loss 0.1294 || training accuracy 95.00% || lr 4.086106094053113e-06\n","Epoch[4/10](13660/14999) || training loss 0.1126 || training accuracy 96.88% || lr 4.085035788347047e-06\n","Epoch[4/10](13680/14999) || training loss 0.1294 || training accuracy 95.00% || lr 4.08396548264098e-06\n","Epoch[4/10](13700/14999) || training loss 0.07607 || training accuracy 97.50% || lr 4.0828951769349115e-06\n","Epoch[4/10](13720/14999) || training loss 0.03772 || training accuracy 100.00% || lr 4.081824871228844e-06\n","Epoch[4/10](13740/14999) || training loss 0.1511 || training accuracy 93.75% || lr 4.080754565522778e-06\n","Epoch[4/10](13760/14999) || training loss 0.1189 || training accuracy 96.25% || lr 4.07968425981671e-06\n","Epoch[4/10](13780/14999) || training loss 0.1513 || training accuracy 94.38% || lr 4.078613954110643e-06\n","Epoch[4/10](13800/14999) || training loss 0.1201 || training accuracy 96.25% || lr 4.077543648404575e-06\n","Epoch[4/10](13820/14999) || training loss 0.1191 || training accuracy 94.38% || lr 4.076473342698508e-06\n","Epoch[4/10](13840/14999) || training loss 0.08928 || training accuracy 96.88% || lr 4.075403036992441e-06\n","Epoch[4/10](13860/14999) || training loss 0.05619 || training accuracy 97.50% || lr 4.074332731286374e-06\n","Epoch[4/10](13880/14999) || training loss 0.07003 || training accuracy 97.50% || lr 4.0732624255803065e-06\n","Epoch[4/10](13900/14999) || training loss 0.08133 || training accuracy 96.88% || lr 4.0721921198742384e-06\n","Epoch[4/10](13920/14999) || training loss 0.1102 || training accuracy 95.62% || lr 4.071121814168171e-06\n","Epoch[4/10](13940/14999) || training loss 0.1199 || training accuracy 95.00% || lr 4.070051508462105e-06\n","Epoch[4/10](13960/14999) || training loss 0.09444 || training accuracy 96.25% || lr 4.068981202756037e-06\n","Epoch[4/10](13980/14999) || training loss 0.07427 || training accuracy 99.38% || lr 4.0679108970499695e-06\n","Epoch[4/10](14000/14999) || training loss 0.1192 || training accuracy 95.00% || lr 4.066840591343902e-06\n","Epoch[4/10](14020/14999) || training loss 0.06701 || training accuracy 98.75% || lr 4.065770285637836e-06\n","Epoch[4/10](14040/14999) || training loss 0.03008 || training accuracy 99.38% || lr 4.064699979931768e-06\n","Epoch[4/10](14060/14999) || training loss 0.1284 || training accuracy 96.25% || lr 4.063629674225701e-06\n","Epoch[4/10](14080/14999) || training loss 0.07293 || training accuracy 96.88% || lr 4.062559368519633e-06\n","Epoch[4/10](14100/14999) || training loss 0.09875 || training accuracy 97.50% || lr 4.061489062813565e-06\n","Epoch[4/10](14120/14999) || training loss 0.09665 || training accuracy 96.88% || lr 4.060418757107499e-06\n","Epoch[4/10](14140/14999) || training loss 0.1331 || training accuracy 96.25% || lr 4.059348451401432e-06\n","Epoch[4/10](14160/14999) || training loss 0.149 || training accuracy 93.12% || lr 4.058278145695364e-06\n","Epoch[4/10](14180/14999) || training loss 0.1342 || training accuracy 95.00% || lr 4.057207839989296e-06\n","Epoch[4/10](14200/14999) || training loss 0.1903 || training accuracy 95.00% || lr 4.056137534283229e-06\n","Epoch[4/10](14220/14999) || training loss 0.1002 || training accuracy 95.62% || lr 4.055067228577163e-06\n","Epoch[4/10](14240/14999) || training loss 0.2026 || training accuracy 92.50% || lr 4.053996922871095e-06\n","Epoch[4/10](14260/14999) || training loss 0.1062 || training accuracy 94.38% || lr 4.0529266171650275e-06\n","Epoch[4/10](14280/14999) || training loss 0.1262 || training accuracy 95.00% || lr 4.05185631145896e-06\n","Epoch[4/10](14300/14999) || training loss 0.06823 || training accuracy 98.75% || lr 4.050786005752893e-06\n","Epoch[4/10](14320/14999) || training loss 0.09542 || training accuracy 96.88% || lr 4.049715700046826e-06\n","Epoch[4/10](14340/14999) || training loss 0.1267 || training accuracy 96.25% || lr 4.048645394340759e-06\n","Epoch[4/10](14360/14999) || training loss 0.1581 || training accuracy 93.12% || lr 4.0475750886346905e-06\n","Epoch[4/10](14380/14999) || training loss 0.1218 || training accuracy 96.25% || lr 4.046504782928623e-06\n","Epoch[4/10](14400/14999) || training loss 0.08878 || training accuracy 96.88% || lr 4.045434477222557e-06\n","Epoch[4/10](14420/14999) || training loss 0.07427 || training accuracy 96.25% || lr 4.04436417151649e-06\n","Epoch[4/10](14440/14999) || training loss 0.2196 || training accuracy 89.38% || lr 4.043293865810422e-06\n","Epoch[4/10](14460/14999) || training loss 0.1002 || training accuracy 96.25% || lr 4.042223560104354e-06\n","Epoch[4/10](14480/14999) || training loss 0.1585 || training accuracy 93.12% || lr 4.041153254398287e-06\n","Epoch[4/10](14500/14999) || training loss 0.1058 || training accuracy 95.00% || lr 4.04008294869222e-06\n","Epoch[4/10](14520/14999) || training loss 0.1572 || training accuracy 95.62% || lr 4.039012642986153e-06\n","Epoch[4/10](14540/14999) || training loss 0.1232 || training accuracy 94.38% || lr 4.0379423372800855e-06\n","Epoch[4/10](14560/14999) || training loss 0.1761 || training accuracy 94.38% || lr 4.036872031574018e-06\n","Epoch[4/10](14580/14999) || training loss 0.2314 || training accuracy 89.38% || lr 4.035801725867951e-06\n","Epoch[4/10](14600/14999) || training loss 0.1595 || training accuracy 93.75% || lr 4.034731420161884e-06\n","Epoch[4/10](14620/14999) || training loss 0.1463 || training accuracy 94.38% || lr 4.033661114455817e-06\n","Epoch[4/10](14640/14999) || training loss 0.168 || training accuracy 92.50% || lr 4.0325908087497485e-06\n","Epoch[4/10](14660/14999) || training loss 0.09394 || training accuracy 96.25% || lr 4.031520503043681e-06\n","Epoch[4/10](14680/14999) || training loss 0.104 || training accuracy 95.62% || lr 4.030450197337615e-06\n","Epoch[4/10](14700/14999) || training loss 0.1551 || training accuracy 94.38% || lr 4.029379891631547e-06\n","Epoch[4/10](14720/14999) || training loss 0.1343 || training accuracy 92.50% || lr 4.02830958592548e-06\n","Epoch[4/10](14740/14999) || training loss 0.1306 || training accuracy 95.62% || lr 4.027239280219412e-06\n","Epoch[4/10](14760/14999) || training loss 0.1146 || training accuracy 95.00% || lr 4.026168974513346e-06\n","Epoch[4/10](14780/14999) || training loss 0.159 || training accuracy 92.50% || lr 4.025098668807278e-06\n","Epoch[4/10](14800/14999) || training loss 0.1505 || training accuracy 93.75% || lr 4.024028363101211e-06\n","Epoch[4/10](14820/14999) || training loss 0.1689 || training accuracy 95.00% || lr 4.0229580573951435e-06\n","Epoch[4/10](14840/14999) || training loss 0.07548 || training accuracy 97.50% || lr 4.021887751689075e-06\n","Epoch[4/10](14860/14999) || training loss 0.0557 || training accuracy 98.75% || lr 4.020817445983009e-06\n","Epoch[4/10](14880/14999) || training loss 0.1825 || training accuracy 95.00% || lr 4.019747140276942e-06\n","Epoch[4/10](14900/14999) || training loss 0.1076 || training accuracy 96.25% || lr 4.018676834570874e-06\n","Epoch[4/10](14920/14999) || training loss 0.1031 || training accuracy 97.50% || lr 4.0176065288648065e-06\n","Epoch[4/10](14940/14999) || training loss 0.1435 || training accuracy 94.38% || lr 4.016536223158739e-06\n","Epoch[4/10](14960/14999) || training loss 0.07805 || training accuracy 97.50% || lr 4.015465917452673e-06\n","Epoch[4/10](14980/14999) || training loss 0.1187 || training accuracy 95.62% || lr 4.014395611746605e-06\n","Calculating validation results...\n","100% 235/235 [02:22<00:00,  1.65it/s]\n","New best model for val acc : 91.09%! saving the best model..\n","[Val] acc : 91.09%, loss: 0.2807, F1 : 0.9109 || best acc : 91.09%, best loss: 0.2442\n","Time elapsed:  334.00 min\n","\n","Epoch[5/10](20/14999) || training loss 0.1206 || training accuracy 95.62% || lr 4.012308515619773e-06\n","Epoch[5/10](40/14999) || training loss 0.0497 || training accuracy 97.50% || lr 4.011238209913706e-06\n","Epoch[5/10](60/14999) || training loss 0.09864 || training accuracy 97.50% || lr 4.010167904207639e-06\n","Epoch[5/10](80/14999) || training loss 0.123 || training accuracy 96.88% || lr 4.0090975985015725e-06\n","Epoch[5/10](100/14999) || training loss 0.1396 || training accuracy 95.00% || lr 4.0080272927955044e-06\n","Epoch[5/10](120/14999) || training loss 0.06655 || training accuracy 96.88% || lr 4.006956987089437e-06\n","Epoch[5/10](140/14999) || training loss 0.1381 || training accuracy 93.75% || lr 4.00588668138337e-06\n","Epoch[5/10](160/14999) || training loss 0.08826 || training accuracy 96.88% || lr 4.004816375677303e-06\n","Epoch[5/10](180/14999) || training loss 0.109 || training accuracy 95.00% || lr 4.0037460699712355e-06\n","Epoch[5/10](200/14999) || training loss 0.04505 || training accuracy 98.12% || lr 4.002675764265168e-06\n","Epoch[5/10](220/14999) || training loss 0.05559 || training accuracy 98.75% || lr 4.0016054585591e-06\n","Epoch[5/10](240/14999) || training loss 0.1055 || training accuracy 96.25% || lr 4.000535152853033e-06\n","Epoch[5/10](260/14999) || training loss 0.05111 || training accuracy 98.12% || lr 3.999464847146967e-06\n","Epoch[5/10](280/14999) || training loss 0.05616 || training accuracy 97.50% || lr 3.9983945414408986e-06\n","Epoch[5/10](300/14999) || training loss 0.09974 || training accuracy 95.62% || lr 3.997324235734832e-06\n","Epoch[5/10](320/14999) || training loss 0.07031 || training accuracy 98.12% || lr 3.996253930028764e-06\n","Epoch[5/10](340/14999) || training loss 0.05193 || training accuracy 97.50% || lr 3.995183624322697e-06\n","Epoch[5/10](360/14999) || training loss 0.05055 || training accuracy 98.75% || lr 3.99411331861663e-06\n","Epoch[5/10](380/14999) || training loss 0.08463 || training accuracy 98.12% || lr 3.9930430129105624e-06\n","Epoch[5/10](400/14999) || training loss 0.08444 || training accuracy 97.50% || lr 3.991972707204495e-06\n","Epoch[5/10](420/14999) || training loss 0.132 || training accuracy 95.62% || lr 3.990902401498428e-06\n","Epoch[5/10](440/14999) || training loss 0.1127 || training accuracy 96.25% || lr 3.989832095792361e-06\n","Epoch[5/10](460/14999) || training loss 0.1849 || training accuracy 93.12% || lr 3.9887617900862935e-06\n","Epoch[5/10](480/14999) || training loss 0.08401 || training accuracy 98.12% || lr 3.9876914843802254e-06\n","Epoch[5/10](500/14999) || training loss 0.1572 || training accuracy 95.00% || lr 3.986621178674159e-06\n","Epoch[5/10](520/14999) || training loss 0.1004 || training accuracy 96.88% || lr 3.985550872968091e-06\n","Epoch[5/10](540/14999) || training loss 0.08001 || training accuracy 98.12% || lr 3.984480567262024e-06\n","Epoch[5/10](560/14999) || training loss 0.09748 || training accuracy 95.62% || lr 3.9834102615559565e-06\n","Epoch[5/10](580/14999) || training loss 0.1559 || training accuracy 95.00% || lr 3.982339955849889e-06\n","Epoch[5/10](600/14999) || training loss 0.2071 || training accuracy 91.88% || lr 3.981269650143822e-06\n","Epoch[5/10](620/14999) || training loss 0.06278 || training accuracy 98.12% || lr 3.980199344437755e-06\n","Epoch[5/10](640/14999) || training loss 0.07099 || training accuracy 96.88% || lr 3.979129038731688e-06\n","Epoch[5/10](660/14999) || training loss 0.05671 || training accuracy 98.12% || lr 3.97805873302562e-06\n","Epoch[5/10](680/14999) || training loss 0.03197 || training accuracy 99.38% || lr 3.976988427319553e-06\n","Epoch[5/10](700/14999) || training loss 0.09106 || training accuracy 96.25% || lr 3.975918121613486e-06\n","Epoch[5/10](720/14999) || training loss 0.09461 || training accuracy 96.25% || lr 3.974847815907419e-06\n","Epoch[5/10](740/14999) || training loss 0.1141 || training accuracy 95.62% || lr 3.973777510201351e-06\n","Epoch[5/10](760/14999) || training loss 0.04231 || training accuracy 98.75% || lr 3.9727072044952834e-06\n","Epoch[5/10](780/14999) || training loss 0.08221 || training accuracy 97.50% || lr 3.971636898789216e-06\n","Epoch[5/10](800/14999) || training loss 0.1192 || training accuracy 96.25% || lr 3.970566593083149e-06\n","Epoch[5/10](820/14999) || training loss 0.08126 || training accuracy 96.25% || lr 3.969496287377082e-06\n","Epoch[5/10](840/14999) || training loss 0.07974 || training accuracy 97.50% || lr 3.9684259816710145e-06\n","Epoch[5/10](860/14999) || training loss 0.05365 || training accuracy 99.38% || lr 3.967355675964947e-06\n","Epoch[5/10](880/14999) || training loss 0.08205 || training accuracy 96.88% || lr 3.96628537025888e-06\n","Epoch[5/10](900/14999) || training loss 0.03697 || training accuracy 99.38% || lr 3.965215064552813e-06\n","Epoch[5/10](920/14999) || training loss 0.1017 || training accuracy 96.88% || lr 3.964144758846746e-06\n","Epoch[5/10](940/14999) || training loss 0.1054 || training accuracy 95.62% || lr 3.963074453140678e-06\n","Epoch[5/10](960/14999) || training loss 0.08249 || training accuracy 96.88% || lr 3.962004147434611e-06\n","Epoch[5/10](980/14999) || training loss 0.06005 || training accuracy 96.88% || lr 3.960933841728543e-06\n","Epoch[5/10](1000/14999) || training loss 0.06183 || training accuracy 98.12% || lr 3.959863536022477e-06\n","Epoch[5/10](1020/14999) || training loss 0.1132 || training accuracy 95.62% || lr 3.958793230316409e-06\n","Epoch[5/10](1040/14999) || training loss 0.1391 || training accuracy 95.00% || lr 3.957722924610342e-06\n","Epoch[5/10](1060/14999) || training loss 0.06027 || training accuracy 97.50% || lr 3.956652618904274e-06\n","Epoch[5/10](1080/14999) || training loss 0.1126 || training accuracy 96.25% || lr 3.955582313198207e-06\n","Epoch[5/10](1100/14999) || training loss 0.1443 || training accuracy 95.00% || lr 3.95451200749214e-06\n","Epoch[5/10](1120/14999) || training loss 0.04698 || training accuracy 98.75% || lr 3.9534417017860725e-06\n","Epoch[5/10](1140/14999) || training loss 0.1226 || training accuracy 95.00% || lr 3.952371396080005e-06\n","Epoch[5/10](1160/14999) || training loss 0.1365 || training accuracy 96.88% || lr 3.951301090373938e-06\n","Epoch[5/10](1180/14999) || training loss 0.06296 || training accuracy 96.88% || lr 3.950230784667871e-06\n","Epoch[5/10](1200/14999) || training loss 0.1081 || training accuracy 95.00% || lr 3.949160478961804e-06\n","Epoch[5/10](1220/14999) || training loss 0.06127 || training accuracy 98.75% || lr 3.9480901732557355e-06\n","Epoch[5/10](1240/14999) || training loss 0.0537 || training accuracy 98.75% || lr 3.947019867549669e-06\n","Epoch[5/10](1260/14999) || training loss 0.07584 || training accuracy 96.88% || lr 3.945949561843601e-06\n","Epoch[5/10](1280/14999) || training loss 0.1961 || training accuracy 95.62% || lr 3.944879256137534e-06\n","Epoch[5/10](1300/14999) || training loss 0.06278 || training accuracy 98.12% || lr 3.943808950431467e-06\n","Epoch[5/10](1320/14999) || training loss 0.06309 || training accuracy 97.50% || lr 3.942738644725399e-06\n","Epoch[5/10](1340/14999) || training loss 0.0827 || training accuracy 98.12% || lr 3.941668339019332e-06\n","Epoch[5/10](1360/14999) || training loss 0.05876 || training accuracy 97.50% || lr 3.940598033313265e-06\n","Epoch[5/10](1380/14999) || training loss 0.06657 || training accuracy 97.50% || lr 3.939527727607198e-06\n","Epoch[5/10](1400/14999) || training loss 0.06749 || training accuracy 97.50% || lr 3.9384574219011305e-06\n","Epoch[5/10](1420/14999) || training loss 0.07697 || training accuracy 97.50% || lr 3.937387116195063e-06\n","Epoch[5/10](1440/14999) || training loss 0.1599 || training accuracy 95.00% || lr 3.936316810488996e-06\n","Epoch[5/10](1460/14999) || training loss 0.07738 || training accuracy 96.25% || lr 3.935246504782929e-06\n","Epoch[5/10](1480/14999) || training loss 0.1265 || training accuracy 95.62% || lr 3.934176199076861e-06\n","Epoch[5/10](1500/14999) || training loss 0.1439 || training accuracy 93.75% || lr 3.9331058933707935e-06\n","Epoch[5/10](1520/14999) || training loss 0.123 || training accuracy 95.62% || lr 3.932035587664726e-06\n","Epoch[5/10](1540/14999) || training loss 0.1078 || training accuracy 96.25% || lr 3.930965281958659e-06\n","Epoch[5/10](1560/14999) || training loss 0.1358 || training accuracy 95.00% || lr 3.929894976252592e-06\n","Epoch[5/10](1580/14999) || training loss 0.07487 || training accuracy 98.12% || lr 3.928824670546525e-06\n","Epoch[5/10](1600/14999) || training loss 0.1182 || training accuracy 95.62% || lr 3.927754364840457e-06\n","Epoch[5/10](1620/14999) || training loss 0.07231 || training accuracy 98.12% || lr 3.92668405913439e-06\n","Epoch[5/10](1640/14999) || training loss 0.09148 || training accuracy 96.88% || lr 3.925613753428323e-06\n","Epoch[5/10](1660/14999) || training loss 0.07029 || training accuracy 96.88% || lr 3.924543447722256e-06\n","Epoch[5/10](1680/14999) || training loss 0.05539 || training accuracy 98.75% || lr 3.9234731420161885e-06\n","Epoch[5/10](1700/14999) || training loss 0.06921 || training accuracy 97.50% || lr 3.922402836310121e-06\n","Epoch[5/10](1720/14999) || training loss 0.1088 || training accuracy 96.25% || lr 3.921332530604053e-06\n","Epoch[5/10](1740/14999) || training loss 0.1435 || training accuracy 96.25% || lr 3.920262224897987e-06\n","Epoch[5/10](1760/14999) || training loss 0.0758 || training accuracy 96.88% || lr 3.919191919191919e-06\n","Epoch[5/10](1780/14999) || training loss 0.07335 || training accuracy 96.88% || lr 3.9181216134858515e-06\n","Epoch[5/10](1800/14999) || training loss 0.1492 || training accuracy 93.12% || lr 3.917051307779784e-06\n","Epoch[5/10](1820/14999) || training loss 0.1239 || training accuracy 95.62% || lr 3.915981002073717e-06\n","Epoch[5/10](1840/14999) || training loss 0.1085 || training accuracy 96.88% || lr 3.91491069636765e-06\n","Epoch[5/10](1860/14999) || training loss 0.08262 || training accuracy 97.50% || lr 3.913840390661583e-06\n","Epoch[5/10](1880/14999) || training loss 0.06615 || training accuracy 96.88% || lr 3.912770084955515e-06\n","Epoch[5/10](1900/14999) || training loss 0.06601 || training accuracy 98.12% || lr 3.911699779249448e-06\n","Epoch[5/10](1920/14999) || training loss 0.04977 || training accuracy 98.12% || lr 3.91062947354338e-06\n","Epoch[5/10](1940/14999) || training loss 0.08861 || training accuracy 95.62% || lr 3.909559167837314e-06\n","Epoch[5/10](1960/14999) || training loss 0.05796 || training accuracy 98.12% || lr 3.908488862131246e-06\n","Epoch[5/10](1980/14999) || training loss 0.09992 || training accuracy 97.50% || lr 3.907418556425179e-06\n","Epoch[5/10](2000/14999) || training loss 0.1805 || training accuracy 93.75% || lr 3.906348250719111e-06\n","Epoch[5/10](2020/14999) || training loss 0.1431 || training accuracy 94.38% || lr 3.905277945013044e-06\n","Epoch[5/10](2040/14999) || training loss 0.03897 || training accuracy 100.00% || lr 3.904207639306977e-06\n","Epoch[5/10](2060/14999) || training loss 0.08734 || training accuracy 95.00% || lr 3.9031373336009095e-06\n","Epoch[5/10](2080/14999) || training loss 0.06872 || training accuracy 97.50% || lr 3.902067027894842e-06\n","Epoch[5/10](2100/14999) || training loss 0.09675 || training accuracy 95.62% || lr 3.900996722188775e-06\n","Epoch[5/10](2120/14999) || training loss 0.03908 || training accuracy 98.75% || lr 3.899926416482708e-06\n","Epoch[5/10](2140/14999) || training loss 0.07078 || training accuracy 98.12% || lr 3.8988561107766406e-06\n","Epoch[5/10](2160/14999) || training loss 0.1135 || training accuracy 95.62% || lr 3.897785805070573e-06\n","Epoch[5/10](2180/14999) || training loss 0.146 || training accuracy 95.00% || lr 3.896715499364506e-06\n","Epoch[5/10](2200/14999) || training loss 0.09621 || training accuracy 95.62% || lr 3.895645193658438e-06\n","Epoch[5/10](2220/14999) || training loss 0.09203 || training accuracy 96.25% || lr 3.894574887952372e-06\n","Epoch[5/10](2240/14999) || training loss 0.06908 || training accuracy 96.88% || lr 3.893504582246304e-06\n","Epoch[5/10](2260/14999) || training loss 0.0594 || training accuracy 98.75% || lr 3.892434276540236e-06\n","Epoch[5/10](2280/14999) || training loss 0.09472 || training accuracy 98.12% || lr 3.891363970834169e-06\n","Epoch[5/10](2300/14999) || training loss 0.122 || training accuracy 97.50% || lr 3.890293665128102e-06\n","Epoch[5/10](2320/14999) || training loss 0.07755 || training accuracy 96.25% || lr 3.889223359422035e-06\n","Epoch[5/10](2340/14999) || training loss 0.08574 || training accuracy 96.88% || lr 3.8881530537159675e-06\n","Epoch[5/10](2360/14999) || training loss 0.05545 || training accuracy 98.12% || lr 3.8870827480099e-06\n","Epoch[5/10](2380/14999) || training loss 0.08004 || training accuracy 97.50% || lr 3.886012442303833e-06\n","Epoch[5/10](2400/14999) || training loss 0.09801 || training accuracy 96.88% || lr 3.884942136597766e-06\n","Epoch[5/10](2420/14999) || training loss 0.0899 || training accuracy 96.88% || lr 3.8838718308916986e-06\n","Epoch[5/10](2440/14999) || training loss 0.09831 || training accuracy 96.88% || lr 3.882801525185631e-06\n","Epoch[5/10](2460/14999) || training loss 0.1304 || training accuracy 95.62% || lr 3.881731219479563e-06\n","Epoch[5/10](2480/14999) || training loss 0.1272 || training accuracy 95.62% || lr 3.880660913773497e-06\n","Epoch[5/10](2500/14999) || training loss 0.1521 || training accuracy 93.75% || lr 3.879590608067429e-06\n","Epoch[5/10](2520/14999) || training loss 0.07427 || training accuracy 96.25% || lr 3.878520302361362e-06\n","Epoch[5/10](2540/14999) || training loss 0.08087 || training accuracy 96.25% || lr 3.877449996655294e-06\n","Epoch[5/10](2560/14999) || training loss 0.08764 || training accuracy 96.88% || lr 3.876379690949227e-06\n","Epoch[5/10](2580/14999) || training loss 0.1592 || training accuracy 93.75% || lr 3.87530938524316e-06\n","Epoch[5/10](2600/14999) || training loss 0.04702 || training accuracy 99.38% || lr 3.874239079537093e-06\n","Epoch[5/10](2620/14999) || training loss 0.05892 || training accuracy 98.12% || lr 3.8731687738310254e-06\n","Epoch[5/10](2640/14999) || training loss 0.08606 || training accuracy 96.25% || lr 3.872098468124958e-06\n","Epoch[5/10](2660/14999) || training loss 0.0852 || training accuracy 96.88% || lr 3.87102816241889e-06\n","Epoch[5/10](2680/14999) || training loss 0.08405 || training accuracy 95.00% || lr 3.869957856712824e-06\n","Epoch[5/10](2700/14999) || training loss 0.08272 || training accuracy 98.75% || lr 3.868887551006756e-06\n","Epoch[5/10](2720/14999) || training loss 0.1008 || training accuracy 96.88% || lr 3.867817245300689e-06\n","Epoch[5/10](2740/14999) || training loss 0.101 || training accuracy 97.50% || lr 3.866746939594621e-06\n","Epoch[5/10](2760/14999) || training loss 0.08137 || training accuracy 97.50% || lr 3.865676633888554e-06\n","Epoch[5/10](2780/14999) || training loss 0.06382 || training accuracy 98.12% || lr 3.864606328182487e-06\n","Epoch[5/10](2800/14999) || training loss 0.1582 || training accuracy 93.75% || lr 3.8635360224764196e-06\n","Epoch[5/10](2820/14999) || training loss 0.09365 || training accuracy 95.62% || lr 3.862465716770352e-06\n","Epoch[5/10](2840/14999) || training loss 0.1361 || training accuracy 96.25% || lr 3.861395411064285e-06\n","Epoch[5/10](2860/14999) || training loss 0.07655 || training accuracy 96.25% || lr 3.860325105358218e-06\n","Epoch[5/10](2880/14999) || training loss 0.116 || training accuracy 94.38% || lr 3.859254799652151e-06\n","Epoch[5/10](2900/14999) || training loss 0.1447 || training accuracy 94.38% || lr 3.8581844939460834e-06\n","Epoch[5/10](2920/14999) || training loss 0.06517 || training accuracy 97.50% || lr 3.857114188240016e-06\n","Epoch[5/10](2940/14999) || training loss 0.1073 || training accuracy 95.62% || lr 3.856043882533948e-06\n","Epoch[5/10](2960/14999) || training loss 0.05614 || training accuracy 97.50% || lr 3.854973576827882e-06\n","Epoch[5/10](2980/14999) || training loss 0.07687 || training accuracy 96.88% || lr 3.853903271121814e-06\n","Epoch[5/10](3000/14999) || training loss 0.1452 || training accuracy 93.75% || lr 3.8528329654157465e-06\n","Epoch[5/10](3020/14999) || training loss 0.1043 || training accuracy 96.88% || lr 3.851762659709679e-06\n","Epoch[5/10](3040/14999) || training loss 0.1172 || training accuracy 93.75% || lr 3.850692354003612e-06\n","Epoch[5/10](3060/14999) || training loss 0.07331 || training accuracy 97.50% || lr 3.849622048297545e-06\n","Epoch[5/10](3080/14999) || training loss 0.0827 || training accuracy 96.25% || lr 3.8485517425914775e-06\n","Epoch[5/10](3100/14999) || training loss 0.1171 || training accuracy 96.88% || lr 3.84748143688541e-06\n","Epoch[5/10](3120/14999) || training loss 0.1186 || training accuracy 96.88% || lr 3.846411131179343e-06\n","Epoch[5/10](3140/14999) || training loss 0.08223 || training accuracy 98.12% || lr 3.845340825473276e-06\n","Epoch[5/10](3160/14999) || training loss 0.124 || training accuracy 94.38% || lr 3.844270519767209e-06\n","Epoch[5/10](3180/14999) || training loss 0.1184 || training accuracy 95.00% || lr 3.843200214061141e-06\n","Epoch[5/10](3200/14999) || training loss 0.07784 || training accuracy 96.88% || lr 3.842129908355073e-06\n","Epoch[5/10](3220/14999) || training loss 0.1773 || training accuracy 93.75% || lr 3.841059602649006e-06\n","Epoch[5/10](3240/14999) || training loss 0.08703 || training accuracy 95.62% || lr 3.839989296942939e-06\n","Epoch[5/10](3260/14999) || training loss 0.09374 || training accuracy 96.25% || lr 3.838918991236872e-06\n","Epoch[5/10](3280/14999) || training loss 0.1397 || training accuracy 95.00% || lr 3.8378486855308044e-06\n","Epoch[5/10](3300/14999) || training loss 0.09825 || training accuracy 95.62% || lr 3.836778379824737e-06\n","Epoch[5/10](3320/14999) || training loss 0.05964 || training accuracy 98.12% || lr 3.83570807411867e-06\n","Epoch[5/10](3340/14999) || training loss 0.1594 || training accuracy 93.75% || lr 3.834637768412603e-06\n","Epoch[5/10](3360/14999) || training loss 0.08342 || training accuracy 98.12% || lr 3.8335674627065355e-06\n","Epoch[5/10](3380/14999) || training loss 0.09986 || training accuracy 96.88% || lr 3.832497157000468e-06\n","Epoch[5/10](3400/14999) || training loss 0.08755 || training accuracy 96.25% || lr 3.8314268512944e-06\n","Epoch[5/10](3420/14999) || training loss 0.06631 || training accuracy 97.50% || lr 3.830356545588334e-06\n","Epoch[5/10](3440/14999) || training loss 0.09738 || training accuracy 96.25% || lr 3.829286239882266e-06\n","Epoch[5/10](3460/14999) || training loss 0.04928 || training accuracy 98.75% || lr 3.828215934176199e-06\n","Epoch[5/10](3480/14999) || training loss 0.08979 || training accuracy 96.88% || lr 3.827145628470131e-06\n","Epoch[5/10](3500/14999) || training loss 0.07226 || training accuracy 97.50% || lr 3.826075322764065e-06\n","Epoch[5/10](3520/14999) || training loss 0.1322 || training accuracy 95.62% || lr 3.825005017057997e-06\n","Epoch[5/10](3540/14999) || training loss 0.1606 || training accuracy 94.38% || lr 3.82393471135193e-06\n","Epoch[5/10](3560/14999) || training loss 0.04168 || training accuracy 99.38% || lr 3.822864405645862e-06\n","Epoch[5/10](3580/14999) || training loss 0.09946 || training accuracy 95.00% || lr 3.821794099939795e-06\n","Epoch[5/10](3600/14999) || training loss 0.06472 || training accuracy 98.12% || lr 3.820723794233728e-06\n","Epoch[5/10](3620/14999) || training loss 0.06971 || training accuracy 97.50% || lr 3.819653488527661e-06\n","Epoch[5/10](3640/14999) || training loss 0.08748 || training accuracy 97.50% || lr 3.818583182821593e-06\n","Epoch[5/10](3660/14999) || training loss 0.05562 || training accuracy 98.12% || lr 3.817512877115526e-06\n","Epoch[5/10](3680/14999) || training loss 0.08739 || training accuracy 96.88% || lr 3.816442571409458e-06\n","Epoch[5/10](3700/14999) || training loss 0.1388 || training accuracy 96.25% || lr 3.815372265703392e-06\n","Epoch[5/10](3720/14999) || training loss 0.07018 || training accuracy 97.50% || lr 3.814301959997324e-06\n","Epoch[5/10](3740/14999) || training loss 0.03995 || training accuracy 98.75% || lr 3.8132316542912565e-06\n","Epoch[5/10](3760/14999) || training loss 0.09751 || training accuracy 96.88% || lr 3.8121613485851897e-06\n","Epoch[5/10](3780/14999) || training loss 0.07866 || training accuracy 96.25% || lr 3.811091042879122e-06\n","Epoch[5/10](3800/14999) || training loss 0.08314 || training accuracy 96.25% || lr 3.810020737173055e-06\n","Epoch[5/10](3820/14999) || training loss 0.08816 || training accuracy 97.50% || lr 3.8089504314669876e-06\n","Epoch[5/10](3840/14999) || training loss 0.1215 || training accuracy 94.38% || lr 3.80788012576092e-06\n","Epoch[5/10](3860/14999) || training loss 0.1289 || training accuracy 95.00% || lr 3.806809820054853e-06\n","Epoch[5/10](3880/14999) || training loss 0.1222 || training accuracy 96.88% || lr 3.8057395143487855e-06\n","Epoch[5/10](3900/14999) || training loss 0.1486 || training accuracy 94.38% || lr 3.8046692086427187e-06\n","Epoch[5/10](3920/14999) || training loss 0.1056 || training accuracy 96.25% || lr 3.803598902936651e-06\n","Epoch[5/10](3940/14999) || training loss 0.1119 || training accuracy 95.62% || lr 3.8025285972305834e-06\n","Epoch[5/10](3960/14999) || training loss 0.04698 || training accuracy 98.12% || lr 3.8014582915245166e-06\n","Epoch[5/10](3980/14999) || training loss 0.1102 || training accuracy 95.00% || lr 3.800387985818449e-06\n","Epoch[5/10](4000/14999) || training loss 0.08312 || training accuracy 96.88% || lr 3.799317680112382e-06\n","Epoch[5/10](4020/14999) || training loss 0.0815 || training accuracy 96.25% || lr 3.7982473744063145e-06\n","Epoch[5/10](4040/14999) || training loss 0.1414 || training accuracy 93.75% || lr 3.7971770687002473e-06\n","Epoch[5/10](4060/14999) || training loss 0.02768 || training accuracy 100.00% || lr 3.79610676299418e-06\n","Epoch[5/10](4080/14999) || training loss 0.08892 || training accuracy 98.75% || lr 3.7950364572881124e-06\n","Epoch[5/10](4100/14999) || training loss 0.1133 || training accuracy 96.25% || lr 3.7939661515820456e-06\n","Epoch[5/10](4120/14999) || training loss 0.04458 || training accuracy 98.12% || lr 3.792895845875978e-06\n","Epoch[5/10](4140/14999) || training loss 0.1075 || training accuracy 96.88% || lr 3.791825540169911e-06\n","Epoch[5/10](4160/14999) || training loss 0.1517 || training accuracy 92.50% || lr 3.7907552344638435e-06\n","Epoch[5/10](4180/14999) || training loss 0.114 || training accuracy 96.25% || lr 3.7896849287577763e-06\n","Epoch[5/10](4200/14999) || training loss 0.08482 || training accuracy 96.88% || lr 3.788614623051709e-06\n","Epoch[5/10](4220/14999) || training loss 0.09089 || training accuracy 95.00% || lr 3.7875443173456414e-06\n","Epoch[5/10](4240/14999) || training loss 0.1215 || training accuracy 95.00% || lr 3.7864740116395746e-06\n","Epoch[5/10](4260/14999) || training loss 0.1153 || training accuracy 96.88% || lr 3.785403705933507e-06\n","Epoch[5/10](4280/14999) || training loss 0.05932 || training accuracy 97.50% || lr 3.7843334002274397e-06\n","Epoch[5/10](4300/14999) || training loss 0.04653 || training accuracy 98.12% || lr 3.7832630945213725e-06\n","Epoch[5/10](4320/14999) || training loss 0.1362 || training accuracy 94.38% || lr 3.7821927888153053e-06\n","Epoch[5/10](4340/14999) || training loss 0.1673 || training accuracy 95.00% || lr 3.781122483109238e-06\n","Epoch[5/10](4360/14999) || training loss 0.09534 || training accuracy 96.25% || lr 3.780052177403171e-06\n","Epoch[5/10](4380/14999) || training loss 0.1437 || training accuracy 92.50% || lr 3.778981871697103e-06\n","Epoch[5/10](4400/14999) || training loss 0.1206 || training accuracy 95.00% || lr 3.777911565991036e-06\n","Epoch[5/10](4420/14999) || training loss 0.1132 || training accuracy 95.00% || lr 3.7768412602849687e-06\n","Epoch[5/10](4440/14999) || training loss 0.1668 || training accuracy 94.38% || lr 3.7757709545789015e-06\n","Epoch[5/10](4460/14999) || training loss 0.1344 || training accuracy 94.38% || lr 3.7747006488728343e-06\n","Epoch[5/10](4480/14999) || training loss 0.1192 || training accuracy 93.75% || lr 3.7736303431667666e-06\n","Epoch[5/10](4500/14999) || training loss 0.05061 || training accuracy 98.75% || lr 3.7725600374607e-06\n","Epoch[5/10](4520/14999) || training loss 0.07322 || training accuracy 96.88% || lr 3.771489731754632e-06\n","Epoch[5/10](4540/14999) || training loss 0.1367 || training accuracy 95.62% || lr 3.770419426048565e-06\n","Epoch[5/10](4560/14999) || training loss 0.07506 || training accuracy 98.12% || lr 3.7693491203424977e-06\n","Epoch[5/10](4580/14999) || training loss 0.08362 || training accuracy 96.25% || lr 3.76827881463643e-06\n","Epoch[5/10](4600/14999) || training loss 0.183 || training accuracy 94.38% || lr 3.7672085089303633e-06\n","Epoch[5/10](4620/14999) || training loss 0.1038 || training accuracy 96.88% || lr 3.7661382032242956e-06\n","Epoch[5/10](4640/14999) || training loss 0.08094 || training accuracy 96.25% || lr 3.765067897518229e-06\n","Epoch[5/10](4660/14999) || training loss 0.1406 || training accuracy 94.38% || lr 3.763997591812161e-06\n","Epoch[5/10](4680/14999) || training loss 0.05187 || training accuracy 98.75% || lr 3.7629272861060935e-06\n","Epoch[5/10](4700/14999) || training loss 0.1162 || training accuracy 95.62% || lr 3.7618569804000267e-06\n","Epoch[5/10](4720/14999) || training loss 0.07681 || training accuracy 98.75% || lr 3.760786674693959e-06\n","Epoch[5/10](4740/14999) || training loss 0.05482 || training accuracy 98.75% || lr 3.7597163689878923e-06\n","Epoch[5/10](4760/14999) || training loss 0.09871 || training accuracy 96.88% || lr 3.7586460632818246e-06\n","Epoch[5/10](4780/14999) || training loss 0.1107 || training accuracy 96.25% || lr 3.757575757575758e-06\n","Epoch[5/10](4800/14999) || training loss 0.1237 || training accuracy 96.25% || lr 3.75650545186969e-06\n","Epoch[5/10](4820/14999) || training loss 0.03431 || training accuracy 99.38% || lr 3.7554351461636225e-06\n","Epoch[5/10](4840/14999) || training loss 0.07854 || training accuracy 97.50% || lr 3.7543648404575557e-06\n","Epoch[5/10](4860/14999) || training loss 0.1273 || training accuracy 94.38% || lr 3.753294534751488e-06\n","Epoch[5/10](4880/14999) || training loss 0.1623 || training accuracy 93.75% || lr 3.7522242290454212e-06\n","Epoch[5/10](4900/14999) || training loss 0.08977 || training accuracy 95.62% || lr 3.7511539233393536e-06\n","Epoch[5/10](4920/14999) || training loss 0.05786 || training accuracy 96.25% || lr 3.7500836176332864e-06\n","Epoch[5/10](4940/14999) || training loss 0.1444 || training accuracy 93.75% || lr 3.749013311927219e-06\n","Epoch[5/10](4960/14999) || training loss 0.07397 || training accuracy 98.75% || lr 3.7479430062211515e-06\n","Epoch[5/10](4980/14999) || training loss 0.1405 || training accuracy 93.75% || lr 3.7468727005150847e-06\n","Epoch[5/10](5000/14999) || training loss 0.08361 || training accuracy 97.50% || lr 3.745802394809017e-06\n","Epoch[5/10](5020/14999) || training loss 0.07711 || training accuracy 97.50% || lr 3.74473208910295e-06\n","Epoch[5/10](5040/14999) || training loss 0.07087 || training accuracy 97.50% || lr 3.7436617833968826e-06\n","Epoch[5/10](5060/14999) || training loss 0.08413 || training accuracy 95.62% || lr 3.7425914776908154e-06\n","Epoch[5/10](5080/14999) || training loss 0.0933 || training accuracy 96.88% || lr 3.741521171984748e-06\n","Epoch[5/10](5100/14999) || training loss 0.06383 || training accuracy 97.50% || lr 3.7404508662786805e-06\n","Epoch[5/10](5120/14999) || training loss 0.1191 || training accuracy 97.50% || lr 3.7393805605726133e-06\n","Epoch[5/10](5140/14999) || training loss 0.1117 || training accuracy 97.50% || lr 3.738310254866546e-06\n","Epoch[5/10](5160/14999) || training loss 0.06256 || training accuracy 97.50% || lr 3.737239949160479e-06\n","Epoch[5/10](5180/14999) || training loss 0.1276 || training accuracy 96.88% || lr 3.7361696434544116e-06\n","Epoch[5/10](5200/14999) || training loss 0.07057 || training accuracy 98.12% || lr 3.7350993377483444e-06\n","Epoch[5/10](5220/14999) || training loss 0.02468 || training accuracy 100.00% || lr 3.7340290320422767e-06\n","Epoch[5/10](5240/14999) || training loss 0.1036 || training accuracy 95.00% || lr 3.7329587263362095e-06\n","Epoch[5/10](5260/14999) || training loss 0.1377 || training accuracy 96.25% || lr 3.7318884206301423e-06\n","Epoch[5/10](5280/14999) || training loss 0.09368 || training accuracy 97.50% || lr 3.730818114924075e-06\n","Epoch[5/10](5300/14999) || training loss 0.08496 || training accuracy 98.75% || lr 3.729747809218008e-06\n","Epoch[5/10](5320/14999) || training loss 0.08544 || training accuracy 95.62% || lr 3.72867750351194e-06\n","Epoch[5/10](5340/14999) || training loss 0.06433 || training accuracy 98.12% || lr 3.7276071978058733e-06\n","Epoch[5/10](5360/14999) || training loss 0.1849 || training accuracy 95.00% || lr 3.7265368920998057e-06\n","Epoch[5/10](5380/14999) || training loss 0.06056 || training accuracy 96.88% || lr 3.7254665863937385e-06\n","Epoch[5/10](5400/14999) || training loss 0.08949 || training accuracy 96.25% || lr 3.7243962806876712e-06\n","Epoch[5/10](5420/14999) || training loss 0.1108 || training accuracy 95.62% || lr 3.723325974981604e-06\n","Epoch[5/10](5440/14999) || training loss 0.1095 || training accuracy 96.88% || lr 3.722255669275537e-06\n","Epoch[5/10](5460/14999) || training loss 0.0992 || training accuracy 97.50% || lr 3.721185363569469e-06\n","Epoch[5/10](5480/14999) || training loss 0.1204 || training accuracy 95.62% || lr 3.7201150578634023e-06\n","Epoch[5/10](5500/14999) || training loss 0.1218 || training accuracy 96.25% || lr 3.7190447521573347e-06\n","Epoch[5/10](5520/14999) || training loss 0.06654 || training accuracy 97.50% || lr 3.717974446451268e-06\n","Epoch[5/10](5540/14999) || training loss 0.04715 || training accuracy 98.75% || lr 3.7169041407452002e-06\n","Epoch[5/10](5560/14999) || training loss 0.05898 || training accuracy 98.12% || lr 3.7158338350391326e-06\n","Epoch[5/10](5580/14999) || training loss 0.1056 || training accuracy 96.88% || lr 3.7147635293330658e-06\n","Epoch[5/10](5600/14999) || training loss 0.055 || training accuracy 97.50% || lr 3.713693223626998e-06\n","Epoch[5/10](5620/14999) || training loss 0.0984 || training accuracy 96.88% || lr 3.7126229179209313e-06\n","Epoch[5/10](5640/14999) || training loss 0.05465 || training accuracy 97.50% || lr 3.7115526122148637e-06\n","Epoch[5/10](5660/14999) || training loss 0.1255 || training accuracy 96.25% || lr 3.710482306508796e-06\n","Epoch[5/10](5680/14999) || training loss 0.06523 || training accuracy 97.50% || lr 3.7094120008027292e-06\n","Epoch[5/10](5700/14999) || training loss 0.06058 || training accuracy 98.12% || lr 3.7083416950966616e-06\n","Epoch[5/10](5720/14999) || training loss 0.06624 || training accuracy 97.50% || lr 3.7072713893905948e-06\n","Epoch[5/10](5740/14999) || training loss 0.07152 || training accuracy 96.25% || lr 3.706201083684527e-06\n","Epoch[5/10](5760/14999) || training loss 0.09962 || training accuracy 95.62% || lr 3.70513077797846e-06\n","Epoch[5/10](5780/14999) || training loss 0.1159 || training accuracy 95.00% || lr 3.7040604722723927e-06\n","Epoch[5/10](5800/14999) || training loss 0.1679 || training accuracy 93.12% || lr 3.702990166566325e-06\n","Epoch[5/10](5820/14999) || training loss 0.1519 || training accuracy 96.25% || lr 3.7019198608602582e-06\n","Epoch[5/10](5840/14999) || training loss 0.106 || training accuracy 94.38% || lr 3.7008495551541906e-06\n","Epoch[5/10](5860/14999) || training loss 0.06016 || training accuracy 98.12% || lr 3.6997792494481233e-06\n","Epoch[5/10](5880/14999) || training loss 0.08 || training accuracy 95.62% || lr 3.698708943742056e-06\n","Epoch[5/10](5900/14999) || training loss 0.08927 || training accuracy 96.88% || lr 3.697638638035989e-06\n","Epoch[5/10](5920/14999) || training loss 0.07017 || training accuracy 97.50% || lr 3.6965683323299217e-06\n","Epoch[5/10](5940/14999) || training loss 0.05617 || training accuracy 98.12% || lr 3.6954980266238544e-06\n","Epoch[5/10](5960/14999) || training loss 0.07987 || training accuracy 98.12% || lr 3.694427720917787e-06\n","Epoch[5/10](5980/14999) || training loss 0.03078 || training accuracy 99.38% || lr 3.6933574152117196e-06\n","Epoch[5/10](6000/14999) || training loss 0.05832 || training accuracy 98.12% || lr 3.6922871095056523e-06\n","Epoch[5/10](6020/14999) || training loss 0.09516 || training accuracy 96.25% || lr 3.691216803799585e-06\n","Epoch[5/10](6040/14999) || training loss 0.1539 || training accuracy 95.00% || lr 3.690146498093518e-06\n","Epoch[5/10](6060/14999) || training loss 0.09006 || training accuracy 96.25% || lr 3.6890761923874507e-06\n","Epoch[5/10](6080/14999) || training loss 0.1294 || training accuracy 95.00% || lr 3.6880058866813834e-06\n","Epoch[5/10](6100/14999) || training loss 0.09948 || training accuracy 94.38% || lr 3.6869355809753158e-06\n","Epoch[5/10](6120/14999) || training loss 0.06715 || training accuracy 98.12% || lr 3.6858652752692486e-06\n","Epoch[5/10](6140/14999) || training loss 0.08208 || training accuracy 96.25% || lr 3.6847949695631813e-06\n","Epoch[5/10](6160/14999) || training loss 0.04941 || training accuracy 98.75% || lr 3.683724663857114e-06\n","Epoch[5/10](6180/14999) || training loss 0.1193 || training accuracy 96.88% || lr 3.682654358151047e-06\n","Epoch[5/10](6200/14999) || training loss 0.08838 || training accuracy 96.25% || lr 3.6815840524449792e-06\n","Epoch[5/10](6220/14999) || training loss 0.07489 || training accuracy 97.50% || lr 3.6805137467389124e-06\n","Epoch[5/10](6240/14999) || training loss 0.04272 || training accuracy 98.12% || lr 3.6794434410328448e-06\n","Epoch[5/10](6260/14999) || training loss 0.1216 || training accuracy 93.12% || lr 3.6783731353267775e-06\n","Epoch[5/10](6280/14999) || training loss 0.0623 || training accuracy 98.12% || lr 3.6773028296207103e-06\n","Epoch[5/10](6300/14999) || training loss 0.1531 || training accuracy 95.00% || lr 3.6762325239146427e-06\n","Epoch[5/10](6320/14999) || training loss 0.07448 || training accuracy 96.88% || lr 3.675162218208576e-06\n","Epoch[5/10](6340/14999) || training loss 0.143 || training accuracy 93.75% || lr 3.6740919125025082e-06\n","Epoch[5/10](6360/14999) || training loss 0.05947 || training accuracy 97.50% || lr 3.6730216067964414e-06\n","Epoch[5/10](6380/14999) || training loss 0.07445 || training accuracy 98.12% || lr 3.6719513010903738e-06\n","Epoch[5/10](6400/14999) || training loss 0.08807 || training accuracy 97.50% || lr 3.670880995384306e-06\n","Epoch[5/10](6420/14999) || training loss 0.06828 || training accuracy 98.12% || lr 3.6698106896782393e-06\n","Epoch[5/10](6440/14999) || training loss 0.06115 || training accuracy 98.75% || lr 3.6687403839721717e-06\n","Epoch[5/10](6460/14999) || training loss 0.1039 || training accuracy 97.50% || lr 3.667670078266105e-06\n","Epoch[5/10](6480/14999) || training loss 0.09069 || training accuracy 94.38% || lr 3.666599772560037e-06\n","Epoch[5/10](6500/14999) || training loss 0.1164 || training accuracy 95.00% || lr 3.66552946685397e-06\n","Epoch[5/10](6520/14999) || training loss 0.1845 || training accuracy 93.12% || lr 3.6644591611479028e-06\n","Epoch[5/10](6540/14999) || training loss 0.06631 || training accuracy 97.50% || lr 3.663388855441835e-06\n","Epoch[5/10](6560/14999) || training loss 0.04406 || training accuracy 98.75% || lr 3.6623185497357683e-06\n","Epoch[5/10](6580/14999) || training loss 0.07487 || training accuracy 96.88% || lr 3.6612482440297007e-06\n","Epoch[5/10](6600/14999) || training loss 0.06348 || training accuracy 98.12% || lr 3.6601779383236334e-06\n","Epoch[5/10](6620/14999) || training loss 0.0684 || training accuracy 96.88% || lr 3.659107632617566e-06\n","Epoch[5/10](6640/14999) || training loss 0.1243 || training accuracy 95.62% || lr 3.658037326911499e-06\n","Epoch[5/10](6660/14999) || training loss 0.07739 || training accuracy 96.25% || lr 3.6569670212054318e-06\n","Epoch[5/10](6680/14999) || training loss 0.1194 || training accuracy 93.75% || lr 3.655896715499364e-06\n","Epoch[5/10](6700/14999) || training loss 0.1034 || training accuracy 96.25% || lr 3.6548264097932973e-06\n","Epoch[5/10](6720/14999) || training loss 0.07729 || training accuracy 96.25% || lr 3.6537561040872296e-06\n","Epoch[5/10](6740/14999) || training loss 0.04741 || training accuracy 98.12% || lr 3.6526857983811624e-06\n","Epoch[5/10](6760/14999) || training loss 0.231 || training accuracy 93.75% || lr 3.651615492675095e-06\n","Epoch[5/10](6780/14999) || training loss 0.11 || training accuracy 95.62% || lr 3.650545186969028e-06\n","Epoch[5/10](6800/14999) || training loss 0.06465 || training accuracy 96.88% || lr 3.6494748812629607e-06\n","Epoch[5/10](6820/14999) || training loss 0.08979 || training accuracy 96.88% || lr 3.648404575556893e-06\n","Epoch[5/10](6840/14999) || training loss 0.1274 || training accuracy 95.00% || lr 3.647334269850826e-06\n","Epoch[5/10](6860/14999) || training loss  0.2 || training accuracy 94.38% || lr 3.6462639641447586e-06\n","Epoch[5/10](6880/14999) || training loss 0.1109 || training accuracy 96.88% || lr 3.6451936584386914e-06\n","Epoch[5/10](6900/14999) || training loss 0.1114 || training accuracy 95.00% || lr 3.644123352732624e-06\n","Epoch[5/10](6920/14999) || training loss 0.08364 || training accuracy 98.12% || lr 3.643053047026557e-06\n","Epoch[5/10](6940/14999) || training loss 0.1602 || training accuracy 92.50% || lr 3.6419827413204893e-06\n","Epoch[5/10](6960/14999) || training loss 0.07881 || training accuracy 96.25% || lr 3.640912435614422e-06\n","Epoch[5/10](6980/14999) || training loss 0.139 || training accuracy 92.50% || lr 3.639842129908355e-06\n","Epoch[5/10](7000/14999) || training loss 0.08865 || training accuracy 96.25% || lr 3.6387718242022876e-06\n","Epoch[5/10](7020/14999) || training loss 0.08427 || training accuracy 96.25% || lr 3.6377015184962204e-06\n","Epoch[5/10](7040/14999) || training loss 0.06136 || training accuracy 98.75% || lr 3.6366312127901528e-06\n","Epoch[5/10](7060/14999) || training loss 0.06674 || training accuracy 96.88% || lr 3.635560907084086e-06\n","Epoch[5/10](7080/14999) || training loss 0.05637 || training accuracy 96.88% || lr 3.6344906013780183e-06\n","Epoch[5/10](7100/14999) || training loss 0.06499 || training accuracy 97.50% || lr 3.6334202956719515e-06\n","Epoch[5/10](7120/14999) || training loss 0.05687 || training accuracy 98.75% || lr 3.632349989965884e-06\n","Epoch[5/10](7140/14999) || training loss 0.09438 || training accuracy 96.88% || lr 3.631279684259816e-06\n","Epoch[5/10](7160/14999) || training loss 0.1156 || training accuracy 96.88% || lr 3.6302093785537494e-06\n","Epoch[5/10](7180/14999) || training loss 0.1801 || training accuracy 94.38% || lr 3.6291390728476817e-06\n","Epoch[5/10](7200/14999) || training loss 0.07031 || training accuracy 98.75% || lr 3.628068767141615e-06\n","Epoch[5/10](7220/14999) || training loss 0.1005 || training accuracy 94.38% || lr 3.6269984614355473e-06\n","Epoch[5/10](7240/14999) || training loss 0.1204 || training accuracy 93.75% || lr 3.6259281557294796e-06\n","Epoch[5/10](7260/14999) || training loss 0.08732 || training accuracy 96.25% || lr 3.624857850023413e-06\n","Epoch[5/10](7280/14999) || training loss 0.05152 || training accuracy 96.88% || lr 3.623787544317345e-06\n","Epoch[5/10](7300/14999) || training loss 0.06078 || training accuracy 97.50% || lr 3.6227172386112784e-06\n","Epoch[5/10](7320/14999) || training loss 0.09038 || training accuracy 96.25% || lr 3.6216469329052107e-06\n","Epoch[5/10](7340/14999) || training loss 0.09461 || training accuracy 95.62% || lr 3.620576627199144e-06\n","Epoch[5/10](7360/14999) || training loss 0.1078 || training accuracy 95.62% || lr 3.6195063214930763e-06\n","Epoch[5/10](7380/14999) || training loss 0.08228 || training accuracy 97.50% || lr 3.618436015787009e-06\n","Epoch[5/10](7400/14999) || training loss 0.08103 || training accuracy 98.12% || lr 3.617365710080942e-06\n","Epoch[5/10](7420/14999) || training loss 0.09344 || training accuracy 96.25% || lr 3.616295404374874e-06\n","Epoch[5/10](7440/14999) || training loss 0.1635 || training accuracy 94.38% || lr 3.6152250986688074e-06\n","Epoch[5/10](7460/14999) || training loss 0.1421 || training accuracy 95.62% || lr 3.6141547929627397e-06\n","Epoch[5/10](7480/14999) || training loss 0.1065 || training accuracy 96.88% || lr 3.6130844872566725e-06\n","Epoch[5/10](7500/14999) || training loss 0.1397 || training accuracy 96.88% || lr 3.6120141815506053e-06\n","Epoch[5/10](7520/14999) || training loss 0.1258 || training accuracy 96.88% || lr 3.610943875844538e-06\n","Epoch[5/10](7540/14999) || training loss 0.07042 || training accuracy 97.50% || lr 3.609873570138471e-06\n","Epoch[5/10](7560/14999) || training loss 0.1357 || training accuracy 97.50% || lr 3.608803264432403e-06\n","Epoch[5/10](7580/14999) || training loss 0.06916 || training accuracy 96.88% || lr 3.607732958726336e-06\n","Epoch[5/10](7600/14999) || training loss 0.12 || training accuracy 95.00% || lr 3.6066626530202687e-06\n","Epoch[5/10](7620/14999) || training loss 0.08388 || training accuracy 96.25% || lr 3.6055923473142015e-06\n","Epoch[5/10](7640/14999) || training loss 0.07968 || training accuracy 96.25% || lr 3.6045220416081343e-06\n","Epoch[5/10](7660/14999) || training loss 0.0915 || training accuracy 96.88% || lr 3.603451735902067e-06\n","Epoch[5/10](7680/14999) || training loss 0.08295 || training accuracy 98.12% || lr 3.6023814301959994e-06\n","Epoch[5/10](7700/14999) || training loss 0.148 || training accuracy 96.25% || lr 3.601311124489932e-06\n","Epoch[5/10](7720/14999) || training loss 0.03613 || training accuracy 98.75% || lr 3.600240818783865e-06\n","Epoch[5/10](7740/14999) || training loss 0.08348 || training accuracy 97.50% || lr 3.5991705130777977e-06\n","Epoch[5/10](7760/14999) || training loss 0.1522 || training accuracy 96.88% || lr 3.5981002073717305e-06\n","Epoch[5/10](7780/14999) || training loss 0.06777 || training accuracy 98.12% || lr 3.597029901665663e-06\n","Epoch[5/10](7800/14999) || training loss 0.08682 || training accuracy 95.62% || lr 3.595959595959596e-06\n","Epoch[5/10](7820/14999) || training loss 0.1074 || training accuracy 96.88% || lr 3.5948892902535284e-06\n","Epoch[5/10](7840/14999) || training loss 0.08425 || training accuracy 97.50% || lr 3.593818984547461e-06\n","Epoch[5/10](7860/14999) || training loss 0.07958 || training accuracy 96.88% || lr 3.592748678841394e-06\n","Epoch[5/10](7880/14999) || training loss 0.06596 || training accuracy 97.50% || lr 3.5916783731353263e-06\n","Epoch[5/10](7900/14999) || training loss 0.1105 || training accuracy 95.00% || lr 3.5906080674292595e-06\n","Epoch[5/10](7920/14999) || training loss 0.1207 || training accuracy 97.50% || lr 3.589537761723192e-06\n","Epoch[5/10](7940/14999) || training loss 0.0748 || training accuracy 97.50% || lr 3.588467456017125e-06\n","Epoch[5/10](7960/14999) || training loss 0.0853 || training accuracy 96.25% || lr 3.5873971503110574e-06\n","Epoch[5/10](7980/14999) || training loss 0.07563 || training accuracy 98.12% || lr 3.58632684460499e-06\n","Epoch[5/10](8000/14999) || training loss 0.1095 || training accuracy 96.88% || lr 3.585256538898923e-06\n","Epoch[5/10](8020/14999) || training loss 0.07652 || training accuracy 96.25% || lr 3.5841862331928553e-06\n","Epoch[5/10](8040/14999) || training loss 0.06308 || training accuracy 98.12% || lr 3.5831159274867885e-06\n","Epoch[5/10](8060/14999) || training loss 0.0856 || training accuracy 96.88% || lr 3.582045621780721e-06\n","Epoch[5/10](8080/14999) || training loss 0.1313 || training accuracy 96.25% || lr 3.580975316074654e-06\n","Epoch[5/10](8100/14999) || training loss 0.1145 || training accuracy 96.88% || lr 3.5799050103685864e-06\n","Epoch[5/10](8120/14999) || training loss 0.2172 || training accuracy 91.25% || lr 3.5788347046625187e-06\n","Epoch[5/10](8140/14999) || training loss 0.04969 || training accuracy 98.75% || lr 3.577764398956452e-06\n","Epoch[5/10](8160/14999) || training loss 0.08865 || training accuracy 96.25% || lr 3.5766940932503843e-06\n","Epoch[5/10](8180/14999) || training loss 0.07917 || training accuracy 96.88% || lr 3.5756237875443175e-06\n","Epoch[5/10](8200/14999) || training loss 0.0681 || training accuracy 96.88% || lr 3.57455348183825e-06\n","Epoch[5/10](8220/14999) || training loss 0.1154 || training accuracy 97.50% || lr 3.5734831761321826e-06\n","Epoch[5/10](8240/14999) || training loss 0.08617 || training accuracy 96.88% || lr 3.5724128704261154e-06\n","Epoch[5/10](8260/14999) || training loss 0.1235 || training accuracy 95.00% || lr 3.5713425647200477e-06\n","Epoch[5/10](8280/14999) || training loss 0.1303 || training accuracy 95.62% || lr 3.570272259013981e-06\n","Epoch[5/10](8300/14999) || training loss 0.07178 || training accuracy 98.12% || lr 3.5692019533079133e-06\n","Epoch[5/10](8320/14999) || training loss 0.08061 || training accuracy 96.88% || lr 3.568131647601846e-06\n","Epoch[5/10](8340/14999) || training loss 0.1083 || training accuracy 96.25% || lr 3.567061341895779e-06\n","Epoch[5/10](8360/14999) || training loss 0.1448 || training accuracy 94.38% || lr 3.5659910361897116e-06\n","Epoch[5/10](8380/14999) || training loss 0.08595 || training accuracy 96.88% || lr 3.5649207304836444e-06\n","Epoch[5/10](8400/14999) || training loss 0.04447 || training accuracy 98.12% || lr 3.5638504247775767e-06\n","Epoch[5/10](8420/14999) || training loss 0.1087 || training accuracy 95.00% || lr 3.5627801190715095e-06\n","Epoch[5/10](8440/14999) || training loss 0.1347 || training accuracy 94.38% || lr 3.5617098133654423e-06\n","Epoch[5/10](8460/14999) || training loss 0.05774 || training accuracy 98.12% || lr 3.560639507659375e-06\n","Epoch[5/10](8480/14999) || training loss 0.1575 || training accuracy 95.00% || lr 3.559569201953308e-06\n","Epoch[5/10](8500/14999) || training loss 0.04947 || training accuracy 98.75% || lr 3.5584988962472406e-06\n","Epoch[5/10](8520/14999) || training loss 0.09869 || training accuracy 95.62% || lr 3.557428590541173e-06\n","Epoch[5/10](8540/14999) || training loss 0.113 || training accuracy 94.38% || lr 3.556358284835106e-06\n","Epoch[5/10](8560/14999) || training loss 0.1595 || training accuracy 95.62% || lr 3.5552879791290385e-06\n","Epoch[5/10](8580/14999) || training loss 0.143 || training accuracy 93.12% || lr 3.5542176734229712e-06\n","Epoch[5/10](8600/14999) || training loss 0.05199 || training accuracy 98.12% || lr 3.553147367716904e-06\n","Epoch[5/10](8620/14999) || training loss 0.1459 || training accuracy 95.00% || lr 3.552077062010837e-06\n","Epoch[5/10](8640/14999) || training loss 0.1275 || training accuracy 93.75% || lr 3.5510067563047696e-06\n","Epoch[5/10](8660/14999) || training loss 0.1409 || training accuracy 94.38% || lr 3.549936450598702e-06\n","Epoch[5/10](8680/14999) || training loss 0.102 || training accuracy 96.25% || lr 3.548866144892635e-06\n","Epoch[5/10](8700/14999) || training loss 0.08418 || training accuracy 96.88% || lr 3.5477958391865675e-06\n","Epoch[5/10](8720/14999) || training loss 0.0868 || training accuracy 96.88% || lr 3.5467255334805002e-06\n","Epoch[5/10](8740/14999) || training loss 0.102 || training accuracy 96.88% || lr 3.545655227774433e-06\n","Epoch[5/10](8760/14999) || training loss 0.06892 || training accuracy 97.50% || lr 3.5445849220683654e-06\n","Epoch[5/10](8780/14999) || training loss 0.07444 || training accuracy 99.38% || lr 3.5435146163622986e-06\n","Epoch[5/10](8800/14999) || training loss 0.1454 || training accuracy 93.75% || lr 3.542444310656231e-06\n","Epoch[5/10](8820/14999) || training loss 0.1349 || training accuracy 96.25% || lr 3.541374004950164e-06\n","Epoch[5/10](8840/14999) || training loss 0.07143 || training accuracy 96.25% || lr 3.5403036992440965e-06\n","Epoch[5/10](8860/14999) || training loss 0.1954 || training accuracy 93.75% || lr 3.539233393538029e-06\n","Epoch[5/10](8880/14999) || training loss 0.06102 || training accuracy 98.12% || lr 3.538163087831962e-06\n","Epoch[5/10](8900/14999) || training loss 0.1192 || training accuracy 96.25% || lr 3.5370927821258944e-06\n","Epoch[5/10](8920/14999) || training loss 0.1243 || training accuracy 93.12% || lr 3.5360224764198275e-06\n","Epoch[5/10](8940/14999) || training loss 0.04754 || training accuracy 98.12% || lr 3.53495217071376e-06\n","Epoch[5/10](8960/14999) || training loss 0.0973 || training accuracy 96.88% || lr 3.5338818650076927e-06\n","Epoch[5/10](8980/14999) || training loss 0.06283 || training accuracy 98.12% || lr 3.5328115593016254e-06\n","Epoch[5/10](9000/14999) || training loss 0.09062 || training accuracy 95.00% || lr 3.531741253595558e-06\n","Epoch[5/10](9020/14999) || training loss 0.0578 || training accuracy 98.75% || lr 3.530670947889491e-06\n","Epoch[5/10](9040/14999) || training loss 0.0802 || training accuracy 96.88% || lr 3.5296006421834233e-06\n","Epoch[5/10](9060/14999) || training loss 0.06903 || training accuracy 98.12% || lr 3.528530336477356e-06\n","Epoch[5/10](9080/14999) || training loss 0.05417 || training accuracy 97.50% || lr 3.527460030771289e-06\n","Epoch[5/10](9100/14999) || training loss 0.05907 || training accuracy 97.50% || lr 3.5263897250652217e-06\n","Epoch[5/10](9120/14999) || training loss 0.08688 || training accuracy 97.50% || lr 3.5253194193591544e-06\n","Epoch[5/10](9140/14999) || training loss 0.05379 || training accuracy 97.50% || lr 3.524249113653087e-06\n","Epoch[5/10](9160/14999) || training loss 0.04012 || training accuracy 98.75% || lr 3.5231788079470196e-06\n","Epoch[5/10](9180/14999) || training loss 0.08805 || training accuracy 96.88% || lr 3.5221085022409523e-06\n","Epoch[5/10](9200/14999) || training loss 0.1135 || training accuracy 96.88% || lr 3.521038196534885e-06\n","Epoch[5/10](9220/14999) || training loss 0.1297 || training accuracy 95.62% || lr 3.519967890828818e-06\n","Epoch[5/10](9240/14999) || training loss 0.07885 || training accuracy 96.88% || lr 3.5188975851227507e-06\n","Epoch[5/10](9260/14999) || training loss 0.07381 || training accuracy 97.50% || lr 3.5178272794166834e-06\n","Epoch[5/10](9280/14999) || training loss 0.08936 || training accuracy 96.88% || lr 3.5167569737106158e-06\n","Epoch[5/10](9300/14999) || training loss 0.123 || training accuracy 95.62% || lr 3.5156866680045486e-06\n","Epoch[5/10](9320/14999) || training loss 0.08713 || training accuracy 96.88% || lr 3.5146163622984813e-06\n","Epoch[5/10](9340/14999) || training loss 0.04205 || training accuracy 98.12% || lr 3.513546056592414e-06\n","Epoch[5/10](9360/14999) || training loss 0.1103 || training accuracy 96.88% || lr 3.512475750886347e-06\n","Epoch[5/10](9380/14999) || training loss 0.05653 || training accuracy 97.50% || lr 3.5114054451802796e-06\n","Epoch[5/10](9400/14999) || training loss 0.1386 || training accuracy 93.75% || lr 3.510335139474212e-06\n","Epoch[5/10](9420/14999) || training loss 0.1114 || training accuracy 96.25% || lr 3.5092648337681448e-06\n","Epoch[5/10](9440/14999) || training loss 0.05401 || training accuracy 97.50% || lr 3.5081945280620775e-06\n","Epoch[5/10](9460/14999) || training loss 0.1394 || training accuracy 95.62% || lr 3.5071242223560103e-06\n","Epoch[5/10](9480/14999) || training loss 0.08956 || training accuracy 96.88% || lr 3.506053916649943e-06\n","Epoch[5/10](9500/14999) || training loss 0.0596 || training accuracy 98.12% || lr 3.5049836109438754e-06\n","Epoch[5/10](9520/14999) || training loss 0.109 || training accuracy 97.50% || lr 3.5039133052378086e-06\n","Epoch[5/10](9540/14999) || training loss 0.07765 || training accuracy 97.50% || lr 3.502842999531741e-06\n","Epoch[5/10](9560/14999) || training loss 0.1138 || training accuracy 96.88% || lr 3.5017726938256738e-06\n","Epoch[5/10](9580/14999) || training loss 0.06609 || training accuracy 96.88% || lr 3.5007023881196065e-06\n","Epoch[5/10](9600/14999) || training loss 0.1368 || training accuracy 96.25% || lr 3.499632082413539e-06\n","Epoch[5/10](9620/14999) || training loss 0.1107 || training accuracy 95.62% || lr 3.498561776707472e-06\n","Epoch[5/10](9640/14999) || training loss 0.1029 || training accuracy 96.25% || lr 3.4974914710014044e-06\n","Epoch[5/10](9660/14999) || training loss 0.08183 || training accuracy 96.88% || lr 3.4964211652953376e-06\n","Epoch[5/10](9680/14999) || training loss 0.04452 || training accuracy 98.75% || lr 3.49535085958927e-06\n","Epoch[5/10](9700/14999) || training loss 0.08661 || training accuracy 96.25% || lr 3.4942805538832023e-06\n","Epoch[5/10](9720/14999) || training loss 0.1329 || training accuracy 95.62% || lr 3.4932102481771355e-06\n","Epoch[5/10](9740/14999) || training loss 0.1155 || training accuracy 96.25% || lr 3.492139942471068e-06\n","Epoch[5/10](9760/14999) || training loss 0.1155 || training accuracy 94.38% || lr 3.491069636765001e-06\n","Epoch[5/10](9780/14999) || training loss 0.1374 || training accuracy 95.62% || lr 3.4899993310589334e-06\n","Epoch[5/10](9800/14999) || training loss 0.1112 || training accuracy 95.00% || lr 3.488929025352866e-06\n","Epoch[5/10](9820/14999) || training loss 0.06092 || training accuracy 98.12% || lr 3.487858719646799e-06\n","Epoch[5/10](9840/14999) || training loss 0.1419 || training accuracy 95.00% || lr 3.4867884139407313e-06\n","Epoch[5/10](9860/14999) || training loss 0.1093 || training accuracy 96.25% || lr 3.4857181082346645e-06\n","Epoch[5/10](9880/14999) || training loss 0.08003 || training accuracy 96.88% || lr 3.484647802528597e-06\n","Epoch[5/10](9900/14999) || training loss 0.1253 || training accuracy 95.62% || lr 3.48357749682253e-06\n","Epoch[5/10](9920/14999) || training loss 0.09825 || training accuracy 96.25% || lr 3.4825071911164624e-06\n","Epoch[5/10](9940/14999) || training loss 0.08592 || training accuracy 96.25% || lr 3.481436885410395e-06\n","Epoch[5/10](9960/14999) || training loss 0.1669 || training accuracy 96.25% || lr 3.480366579704328e-06\n","Epoch[5/10](9980/14999) || training loss 0.07801 || training accuracy 96.88% || lr 3.4792962739982607e-06\n","Epoch[5/10](10000/14999) || training loss 0.1679 || training accuracy 91.88% || lr 3.4782259682921935e-06\n","Epoch[5/10](10020/14999) || training loss 0.03733 || training accuracy 98.75% || lr 3.477155662586126e-06\n","Epoch[5/10](10040/14999) || training loss 0.08862 || training accuracy 96.25% || lr 3.4760853568800586e-06\n","Epoch[5/10](10060/14999) || training loss 0.07032 || training accuracy 98.75% || lr 3.4750150511739914e-06\n","Epoch[5/10](10080/14999) || training loss 0.1308 || training accuracy 95.00% || lr 3.473944745467924e-06\n","Epoch[5/10](10100/14999) || training loss 0.1734 || training accuracy 94.38% || lr 3.472874439761857e-06\n","Epoch[5/10](10120/14999) || training loss 0.1298 || training accuracy 96.25% || lr 3.4718041340557897e-06\n","Epoch[5/10](10140/14999) || training loss 0.0823 || training accuracy 98.12% || lr 3.470733828349722e-06\n","Epoch[5/10](10160/14999) || training loss 0.1124 || training accuracy 96.88% || lr 3.469663522643655e-06\n","Epoch[5/10](10180/14999) || training loss 0.07341 || training accuracy 96.88% || lr 3.4685932169375876e-06\n","Epoch[5/10](10200/14999) || training loss 0.08182 || training accuracy 96.88% || lr 3.4675229112315204e-06\n","Epoch[5/10](10220/14999) || training loss 0.06992 || training accuracy 98.75% || lr 3.466452605525453e-06\n","Epoch[5/10](10240/14999) || training loss 0.0561 || training accuracy 98.12% || lr 3.4653822998193855e-06\n","Epoch[5/10](10260/14999) || training loss 0.1018 || training accuracy 95.62% || lr 3.4643119941133187e-06\n","Epoch[5/10](10280/14999) || training loss 0.07543 || training accuracy 97.50% || lr 3.463241688407251e-06\n","Epoch[5/10](10300/14999) || training loss 0.07134 || training accuracy 96.88% || lr 3.462171382701184e-06\n","Epoch[5/10](10320/14999) || training loss 0.07598 || training accuracy 96.88% || lr 3.4611010769951166e-06\n","Epoch[5/10](10340/14999) || training loss 0.1061 || training accuracy 95.62% || lr 3.460030771289049e-06\n","Epoch[5/10](10360/14999) || training loss 0.03378 || training accuracy 100.00% || lr 3.458960465582982e-06\n","Epoch[5/10](10380/14999) || training loss 0.05643 || training accuracy 96.88% || lr 3.4578901598769145e-06\n","Epoch[5/10](10400/14999) || training loss 0.2121 || training accuracy 94.38% || lr 3.4568198541708477e-06\n","Epoch[5/10](10420/14999) || training loss 0.1515 || training accuracy 95.62% || lr 3.45574954846478e-06\n","Epoch[5/10](10440/14999) || training loss 0.1251 || training accuracy 95.62% || lr 3.4546792427587124e-06\n","Epoch[5/10](10460/14999) || training loss 0.1511 || training accuracy 95.00% || lr 3.4536089370526456e-06\n","Epoch[5/10](10480/14999) || training loss 0.1771 || training accuracy 94.38% || lr 3.452538631346578e-06\n","Epoch[5/10](10500/14999) || training loss 0.0871 || training accuracy 96.88% || lr 3.451468325640511e-06\n","Epoch[5/10](10520/14999) || training loss 0.07917 || training accuracy 96.88% || lr 3.4503980199344435e-06\n","Epoch[5/10](10540/14999) || training loss 0.1182 || training accuracy 95.62% || lr 3.4493277142283767e-06\n","Epoch[5/10](10560/14999) || training loss 0.06052 || training accuracy 98.12% || lr 3.448257408522309e-06\n","Epoch[5/10](10580/14999) || training loss 0.1275 || training accuracy 95.00% || lr 3.4471871028162414e-06\n","Epoch[5/10](10600/14999) || training loss 0.03947 || training accuracy 98.75% || lr 3.4461167971101746e-06\n","Epoch[5/10](10620/14999) || training loss 0.1341 || training accuracy 94.38% || lr 3.445046491404107e-06\n","Epoch[5/10](10640/14999) || training loss 0.1235 || training accuracy 95.62% || lr 3.44397618569804e-06\n","Epoch[5/10](10660/14999) || training loss 0.09267 || training accuracy 96.88% || lr 3.4429058799919725e-06\n","Epoch[5/10](10680/14999) || training loss 0.08158 || training accuracy 96.88% || lr 3.4418355742859053e-06\n","Epoch[5/10](10700/14999) || training loss 0.06746 || training accuracy 96.25% || lr 3.440765268579838e-06\n","Epoch[5/10](10720/14999) || training loss 0.05858 || training accuracy 98.75% || lr 3.4396949628737704e-06\n","Epoch[5/10](10740/14999) || training loss 0.07833 || training accuracy 95.62% || lr 3.4386246571677036e-06\n","Epoch[5/10](10760/14999) || training loss 0.1411 || training accuracy 95.62% || lr 3.437554351461636e-06\n","Epoch[5/10](10780/14999) || training loss 0.1446 || training accuracy 94.38% || lr 3.4364840457555687e-06\n","Epoch[5/10](10800/14999) || training loss 0.1061 || training accuracy 96.88% || lr 3.4354137400495015e-06\n","Epoch[5/10](10820/14999) || training loss 0.1429 || training accuracy 95.62% || lr 3.4343434343434343e-06\n","Epoch[5/10](10840/14999) || training loss 0.1497 || training accuracy 94.38% || lr 3.433273128637367e-06\n","Epoch[5/10](10860/14999) || training loss 0.1121 || training accuracy 96.25% || lr 3.4322028229312994e-06\n","Epoch[5/10](10880/14999) || training loss 0.04336 || training accuracy 98.12% || lr 3.431132517225232e-06\n","Epoch[5/10](10900/14999) || training loss 0.1246 || training accuracy 95.62% || lr 3.430062211519165e-06\n","Epoch[5/10](10920/14999) || training loss 0.0446 || training accuracy 98.12% || lr 3.4289919058130977e-06\n","Epoch[5/10](10940/14999) || training loss 0.05061 || training accuracy 98.12% || lr 3.4279216001070305e-06\n","Epoch[5/10](10960/14999) || training loss 0.08804 || training accuracy 97.50% || lr 3.4268512944009633e-06\n","Epoch[5/10](10980/14999) || training loss 0.1484 || training accuracy 95.00% || lr 3.4257809886948956e-06\n","Epoch[5/10](11000/14999) || training loss 0.1025 || training accuracy 97.50% || lr 3.4247106829888284e-06\n","Epoch[5/10](11020/14999) || training loss 0.1243 || training accuracy 95.62% || lr 3.423640377282761e-06\n","Epoch[5/10](11040/14999) || training loss 0.1167 || training accuracy 96.25% || lr 3.422570071576694e-06\n","Epoch[5/10](11060/14999) || training loss 0.08279 || training accuracy 96.88% || lr 3.4214997658706267e-06\n","Epoch[5/10](11080/14999) || training loss 0.08575 || training accuracy 95.62% || lr 3.420429460164559e-06\n","Epoch[5/10](11100/14999) || training loss 0.05047 || training accuracy 97.50% || lr 3.4193591544584923e-06\n","Epoch[5/10](11120/14999) || training loss 0.1205 || training accuracy 93.12% || lr 3.4182888487524246e-06\n","Epoch[5/10](11140/14999) || training loss 0.05927 || training accuracy 96.88% || lr 3.417218543046358e-06\n","Epoch[5/10](11160/14999) || training loss 0.1385 || training accuracy 95.62% || lr 3.41614823734029e-06\n","Epoch[5/10](11180/14999) || training loss 0.08856 || training accuracy 96.88% || lr 3.415077931634223e-06\n","Epoch[5/10](11200/14999) || training loss 0.09431 || training accuracy 96.88% || lr 3.4140076259281557e-06\n","Epoch[5/10](11220/14999) || training loss 0.0935 || training accuracy 96.25% || lr 3.412937320222088e-06\n","Epoch[5/10](11240/14999) || training loss 0.08509 || training accuracy 98.12% || lr 3.4118670145160212e-06\n","Epoch[5/10](11260/14999) || training loss 0.07597 || training accuracy 96.88% || lr 3.4107967088099536e-06\n","Epoch[5/10](11280/14999) || training loss 0.1319 || training accuracy 96.25% || lr 3.409726403103887e-06\n","Epoch[5/10](11300/14999) || training loss 0.07488 || training accuracy 96.88% || lr 3.408656097397819e-06\n","Epoch[5/10](11320/14999) || training loss 0.1442 || training accuracy 94.38% || lr 3.4075857916917515e-06\n","Epoch[5/10](11340/14999) || training loss 0.1154 || training accuracy 97.50% || lr 3.4065154859856847e-06\n","Epoch[5/10](11360/14999) || training loss 0.07054 || training accuracy 97.50% || lr 3.405445180279617e-06\n","Epoch[5/10](11380/14999) || training loss 0.1057 || training accuracy 96.25% || lr 3.4043748745735502e-06\n","Epoch[5/10](11400/14999) || training loss 0.09884 || training accuracy 96.88% || lr 3.4033045688674826e-06\n","Epoch[5/10](11420/14999) || training loss 0.1056 || training accuracy 96.88% || lr 3.4022342631614154e-06\n","Epoch[5/10](11440/14999) || training loss 0.1353 || training accuracy 96.25% || lr 3.401163957455348e-06\n","Epoch[5/10](11460/14999) || training loss 0.09884 || training accuracy 96.88% || lr 3.4000936517492805e-06\n","Epoch[5/10](11480/14999) || training loss 0.1208 || training accuracy 95.62% || lr 3.3990233460432137e-06\n","Epoch[5/10](11500/14999) || training loss 0.167 || training accuracy 94.38% || lr 3.397953040337146e-06\n","Epoch[5/10](11520/14999) || training loss 0.08983 || training accuracy 96.88% || lr 3.396882734631079e-06\n","Epoch[5/10](11540/14999) || training loss 0.107 || training accuracy 95.62% || lr 3.3958124289250116e-06\n","Epoch[5/10](11560/14999) || training loss 0.06235 || training accuracy 98.75% || lr 3.3947421232189444e-06\n","Epoch[5/10](11580/14999) || training loss 0.1299 || training accuracy 96.88% || lr 3.393671817512877e-06\n","Epoch[5/10](11600/14999) || training loss 0.1159 || training accuracy 95.62% || lr 3.3926015118068095e-06\n","Epoch[5/10](11620/14999) || training loss 0.1019 || training accuracy 97.50% || lr 3.3915312061007423e-06\n","Epoch[5/10](11640/14999) || training loss 0.1596 || training accuracy 93.75% || lr 3.390460900394675e-06\n","Epoch[5/10](11660/14999) || training loss 0.06503 || training accuracy 97.50% || lr 3.389390594688608e-06\n","Epoch[5/10](11680/14999) || training loss 0.1007 || training accuracy 96.25% || lr 3.3883202889825406e-06\n","Epoch[5/10](11700/14999) || training loss 0.1753 || training accuracy 95.00% || lr 3.3872499832764733e-06\n","Epoch[5/10](11720/14999) || training loss 0.1366 || training accuracy 95.62% || lr 3.3861796775704057e-06\n","Epoch[5/10](11740/14999) || training loss 0.08828 || training accuracy 98.12% || lr 3.3851093718643385e-06\n","Epoch[5/10](11760/14999) || training loss 0.06617 || training accuracy 98.12% || lr 3.3840390661582712e-06\n","Epoch[5/10](11780/14999) || training loss 0.05567 || training accuracy 98.75% || lr 3.382968760452204e-06\n","Epoch[5/10](11800/14999) || training loss 0.1334 || training accuracy 95.62% || lr 3.381898454746137e-06\n","Epoch[5/10](11820/14999) || training loss 0.04584 || training accuracy 98.12% || lr 3.3808281490400696e-06\n","Epoch[5/10](11840/14999) || training loss 0.07014 || training accuracy 96.88% || lr 3.3797578433340023e-06\n","Epoch[5/10](11860/14999) || training loss 0.0879 || training accuracy 96.88% || lr 3.3786875376279347e-06\n","Epoch[5/10](11880/14999) || training loss 0.1209 || training accuracy 95.62% || lr 3.3776172319218675e-06\n","Epoch[5/10](11900/14999) || training loss 0.04209 || training accuracy 98.75% || lr 3.3765469262158002e-06\n","Epoch[5/10](11920/14999) || training loss 0.11 || training accuracy 95.00% || lr 3.375476620509733e-06\n","Epoch[5/10](11940/14999) || training loss 0.1514 || training accuracy 94.38% || lr 3.3744063148036658e-06\n","Epoch[5/10](11960/14999) || training loss 0.1343 || training accuracy 93.75% || lr 3.373336009097598e-06\n","Epoch[5/10](11980/14999) || training loss 0.06154 || training accuracy 96.88% || lr 3.3722657033915313e-06\n","Epoch[5/10](12000/14999) || training loss 0.07267 || training accuracy 97.50% || lr 3.3711953976854637e-06\n","Epoch[5/10](12020/14999) || training loss 0.1458 || training accuracy 95.62% || lr 3.3701250919793965e-06\n","Epoch[5/10](12040/14999) || training loss 0.1085 || training accuracy 96.25% || lr 3.3690547862733292e-06\n","Epoch[5/10](12060/14999) || training loss 0.04164 || training accuracy 98.75% || lr 3.3679844805672616e-06\n","Epoch[5/10](12080/14999) || training loss 0.1274 || training accuracy 95.00% || lr 3.3669141748611948e-06\n","Epoch[5/10](12100/14999) || training loss 0.07452 || training accuracy 95.62% || lr 3.365843869155127e-06\n","Epoch[5/10](12120/14999) || training loss 0.183 || training accuracy 93.12% || lr 3.3647735634490603e-06\n","Epoch[5/10](12140/14999) || training loss 0.169 || training accuracy 93.75% || lr 3.3637032577429927e-06\n","Epoch[5/10](12160/14999) || training loss 0.1139 || training accuracy 95.62% || lr 3.362632952036925e-06\n","Epoch[5/10](12180/14999) || training loss 0.1047 || training accuracy 96.25% || lr 3.3615626463308582e-06\n","Epoch[5/10](12200/14999) || training loss 0.06202 || training accuracy 97.50% || lr 3.3604923406247906e-06\n","Epoch[5/10](12220/14999) || training loss 0.07905 || training accuracy 97.50% || lr 3.3594220349187238e-06\n","Epoch[5/10](12240/14999) || training loss 0.07687 || training accuracy 98.75% || lr 3.358351729212656e-06\n","Epoch[5/10](12260/14999) || training loss 0.06907 || training accuracy 97.50% || lr 3.357281423506589e-06\n","Epoch[5/10](12280/14999) || training loss 0.1146 || training accuracy 96.88% || lr 3.3562111178005217e-06\n","Epoch[5/10](12300/14999) || training loss 0.09988 || training accuracy 95.62% || lr 3.355140812094454e-06\n","Epoch[5/10](12320/14999) || training loss 0.1228 || training accuracy 95.00% || lr 3.354070506388387e-06\n","Epoch[5/10](12340/14999) || training loss 0.1678 || training accuracy 95.00% || lr 3.3530002006823196e-06\n","Epoch[5/10](12360/14999) || training loss 0.07955 || training accuracy 97.50% || lr 3.3519298949762523e-06\n","Epoch[5/10](12380/14999) || training loss 0.1089 || training accuracy 96.25% || lr 3.350859589270185e-06\n","Epoch[5/10](12400/14999) || training loss 0.1247 || training accuracy 95.62% || lr 3.349789283564118e-06\n","Epoch[5/10](12420/14999) || training loss 0.1099 || training accuracy 97.50% || lr 3.3487189778580507e-06\n","Epoch[5/10](12440/14999) || training loss 0.0845 || training accuracy 96.88% || lr 3.347648672151983e-06\n","Epoch[5/10](12460/14999) || training loss 0.07061 || training accuracy 98.12% || lr 3.346578366445916e-06\n","Epoch[5/10](12480/14999) || training loss 0.1158 || training accuracy 95.00% || lr 3.3455080607398486e-06\n","Epoch[5/10](12500/14999) || training loss 0.07729 || training accuracy 97.50% || lr 3.3444377550337813e-06\n","Epoch[5/10](12520/14999) || training loss 0.06088 || training accuracy 98.12% || lr 3.343367449327714e-06\n","Epoch[5/10](12540/14999) || training loss 0.1333 || training accuracy 96.88% || lr 3.342297143621647e-06\n","Epoch[5/10](12560/14999) || training loss 0.06605 || training accuracy 96.88% || lr 3.3412268379155796e-06\n","Epoch[5/10](12580/14999) || training loss 0.06189 || training accuracy 98.12% || lr 3.3401565322095124e-06\n","Epoch[5/10](12600/14999) || training loss 0.06191 || training accuracy 97.50% || lr 3.3390862265034448e-06\n","Epoch[5/10](12620/14999) || training loss 0.05918 || training accuracy 98.12% || lr 3.3380159207973775e-06\n","Epoch[5/10](12640/14999) || training loss 0.1121 || training accuracy 98.12% || lr 3.3369456150913103e-06\n","Epoch[5/10](12660/14999) || training loss 0.1335 || training accuracy 96.25% || lr 3.335875309385243e-06\n","Epoch[5/10](12680/14999) || training loss 0.06675 || training accuracy 96.25% || lr 3.334805003679176e-06\n","Epoch[5/10](12700/14999) || training loss 0.05862 || training accuracy 96.88% || lr 3.3337346979731082e-06\n","Epoch[5/10](12720/14999) || training loss 0.1149 || training accuracy 97.50% || lr 3.3326643922670414e-06\n","Epoch[5/10](12740/14999) || training loss 0.09713 || training accuracy 95.62% || lr 3.3315940865609738e-06\n","Epoch[5/10](12760/14999) || training loss 0.05753 || training accuracy 97.50% || lr 3.3305237808549065e-06\n","Epoch[5/10](12780/14999) || training loss 0.1529 || training accuracy 92.50% || lr 3.3294534751488393e-06\n","Epoch[5/10](12800/14999) || training loss 0.2287 || training accuracy 92.50% || lr 3.3283831694427717e-06\n","Epoch[5/10](12820/14999) || training loss 0.1039 || training accuracy 96.88% || lr 3.327312863736705e-06\n","Epoch[5/10](12840/14999) || training loss 0.1203 || training accuracy 95.62% || lr 3.326242558030637e-06\n","Epoch[5/10](12860/14999) || training loss 0.1582 || training accuracy 96.25% || lr 3.3251722523245704e-06\n","Epoch[5/10](12880/14999) || training loss 0.1098 || training accuracy 96.25% || lr 3.3241019466185028e-06\n","Epoch[5/10](12900/14999) || training loss 0.1195 || training accuracy 93.75% || lr 3.323031640912435e-06\n","Epoch[5/10](12920/14999) || training loss 0.09347 || training accuracy 96.25% || lr 3.3219613352063683e-06\n","Epoch[5/10](12940/14999) || training loss 0.07672 || training accuracy 96.88% || lr 3.3208910295003007e-06\n","Epoch[5/10](12960/14999) || training loss 0.0958 || training accuracy 94.38% || lr 3.319820723794234e-06\n","Epoch[5/10](12980/14999) || training loss 0.1244 || training accuracy 96.25% || lr 3.318750418088166e-06\n","Epoch[5/10](13000/14999) || training loss 0.06141 || training accuracy 97.50% || lr 3.317680112382099e-06\n","Epoch[5/10](13020/14999) || training loss 0.142 || training accuracy 93.75% || lr 3.3166098066760317e-06\n","Epoch[5/10](13040/14999) || training loss 0.1998 || training accuracy 94.38% || lr 3.315539500969964e-06\n","Epoch[5/10](13060/14999) || training loss 0.09951 || training accuracy 96.25% || lr 3.3144691952638973e-06\n","Epoch[5/10](13080/14999) || training loss 0.05899 || training accuracy 98.12% || lr 3.3133988895578296e-06\n","Epoch[5/10](13100/14999) || training loss 0.1462 || training accuracy 95.00% || lr 3.312328583851763e-06\n","Epoch[5/10](13120/14999) || training loss 0.06424 || training accuracy 98.12% || lr 3.311258278145695e-06\n","Epoch[5/10](13140/14999) || training loss 0.08765 || training accuracy 96.25% || lr 3.310187972439628e-06\n","Epoch[5/10](13160/14999) || training loss 0.0758 || training accuracy 96.88% || lr 3.3091176667335607e-06\n","Epoch[5/10](13180/14999) || training loss 0.08861 || training accuracy 97.50% || lr 3.308047361027493e-06\n","Epoch[5/10](13200/14999) || training loss 0.1169 || training accuracy 95.62% || lr 3.3069770553214263e-06\n","Epoch[5/10](13220/14999) || training loss 0.05709 || training accuracy 98.75% || lr 3.3059067496153586e-06\n","Epoch[5/10](13240/14999) || training loss 0.07335 || training accuracy 96.88% || lr 3.3048364439092914e-06\n","Epoch[5/10](13260/14999) || training loss 0.06796 || training accuracy 96.88% || lr 3.303766138203224e-06\n","Epoch[5/10](13280/14999) || training loss 0.08017 || training accuracy 95.62% || lr 3.302695832497157e-06\n","Epoch[5/10](13300/14999) || training loss 0.1005 || training accuracy 95.62% || lr 3.3016255267910897e-06\n","Epoch[5/10](13320/14999) || training loss 0.1122 || training accuracy 93.12% || lr 3.300555221085022e-06\n","Epoch[5/10](13340/14999) || training loss 0.09853 || training accuracy 96.88% || lr 3.299484915378955e-06\n","Epoch[5/10](13360/14999) || training loss 0.07623 || training accuracy 96.88% || lr 3.2984146096728876e-06\n","Epoch[5/10](13380/14999) || training loss 0.09612 || training accuracy 96.25% || lr 3.2973443039668204e-06\n","Epoch[5/10](13400/14999) || training loss 0.06772 || training accuracy 96.25% || lr 3.296273998260753e-06\n","Epoch[5/10](13420/14999) || training loss 0.1732 || training accuracy 94.38% || lr 3.295203692554686e-06\n","Epoch[5/10](13440/14999) || training loss 0.07805 || training accuracy 96.88% || lr 3.2941333868486183e-06\n","Epoch[5/10](13460/14999) || training loss 0.08365 || training accuracy 96.88% || lr 3.293063081142551e-06\n","Epoch[5/10](13480/14999) || training loss 0.123 || training accuracy 95.00% || lr 3.291992775436484e-06\n","Epoch[5/10](13500/14999) || training loss 0.1497 || training accuracy 93.75% || lr 3.2909224697304166e-06\n","Epoch[5/10](13520/14999) || training loss 0.09142 || training accuracy 96.88% || lr 3.2898521640243494e-06\n","Epoch[5/10](13540/14999) || training loss 0.1372 || training accuracy 96.88% || lr 3.2887818583182817e-06\n","Epoch[5/10](13560/14999) || training loss 0.1223 || training accuracy 93.12% || lr 3.287711552612215e-06\n","Epoch[5/10](13580/14999) || training loss 0.09149 || training accuracy 96.88% || lr 3.2866412469061473e-06\n","Epoch[5/10](13600/14999) || training loss 0.1114 || training accuracy 96.88% || lr 3.28557094120008e-06\n","Epoch[5/10](13620/14999) || training loss 0.1217 || training accuracy 93.75% || lr 3.284500635494013e-06\n","Epoch[5/10](13640/14999) || training loss 0.1122 || training accuracy 97.50% || lr 3.283430329787945e-06\n","Epoch[5/10](13660/14999) || training loss 0.104 || training accuracy 94.38% || lr 3.2823600240818784e-06\n","Epoch[5/10](13680/14999) || training loss 0.2091 || training accuracy 91.88% || lr 3.2812897183758107e-06\n","Epoch[5/10](13700/14999) || training loss 0.1564 || training accuracy 94.38% || lr 3.280219412669744e-06\n","Epoch[5/10](13720/14999) || training loss 0.1008 || training accuracy 96.88% || lr 3.2791491069636763e-06\n","Epoch[5/10](13740/14999) || training loss 0.09269 || training accuracy 98.12% || lr 3.2780788012576095e-06\n","Epoch[5/10](13760/14999) || training loss 0.1397 || training accuracy 94.38% || lr 3.277008495551542e-06\n","Epoch[5/10](13780/14999) || training loss 0.0689 || training accuracy 96.88% || lr 3.275938189845474e-06\n","Epoch[5/10](13800/14999) || training loss 0.0987 || training accuracy 97.50% || lr 3.2748678841394074e-06\n","Epoch[5/10](13820/14999) || training loss 0.0771 || training accuracy 96.88% || lr 3.2737975784333397e-06\n","Epoch[5/10](13840/14999) || training loss 0.09278 || training accuracy 96.88% || lr 3.272727272727273e-06\n","Epoch[5/10](13860/14999) || training loss 0.02859 || training accuracy 98.75% || lr 3.2716569670212053e-06\n","Epoch[5/10](13880/14999) || training loss 0.1343 || training accuracy 95.62% || lr 3.2705866613151376e-06\n","Epoch[5/10](13900/14999) || training loss 0.08426 || training accuracy 97.50% || lr 3.269516355609071e-06\n","Epoch[5/10](13920/14999) || training loss 0.1143 || training accuracy 95.00% || lr 3.268446049903003e-06\n","Epoch[5/10](13940/14999) || training loss 0.03862 || training accuracy 98.12% || lr 3.2673757441969364e-06\n","Epoch[5/10](13960/14999) || training loss 0.07071 || training accuracy 96.88% || lr 3.2663054384908687e-06\n","Epoch[5/10](13980/14999) || training loss 0.0839 || training accuracy 98.12% || lr 3.2652351327848015e-06\n","Epoch[5/10](14000/14999) || training loss 0.08494 || training accuracy 96.88% || lr 3.2641648270787343e-06\n","Epoch[5/10](14020/14999) || training loss 0.03702 || training accuracy 100.00% || lr 3.263094521372667e-06\n","Epoch[5/10](14040/14999) || training loss 0.1006 || training accuracy 95.00% || lr 3.2620242156666e-06\n","Epoch[5/10](14060/14999) || training loss 0.1364 || training accuracy 95.00% || lr 3.260953909960532e-06\n","Epoch[5/10](14080/14999) || training loss 0.06651 || training accuracy 96.88% || lr 3.259883604254465e-06\n","Epoch[5/10](14100/14999) || training loss 0.1133 || training accuracy 95.62% || lr 3.2588132985483977e-06\n","Epoch[5/10](14120/14999) || training loss 0.1601 || training accuracy 95.00% || lr 3.2577429928423305e-06\n","Epoch[5/10](14140/14999) || training loss 0.09426 || training accuracy 97.50% || lr 3.2566726871362633e-06\n","Epoch[5/10](14160/14999) || training loss 0.06419 || training accuracy 98.12% || lr 3.255602381430196e-06\n","Epoch[5/10](14180/14999) || training loss 0.08117 || training accuracy 95.62% || lr 3.2545320757241284e-06\n","Epoch[5/10](14200/14999) || training loss 0.1041 || training accuracy 95.62% || lr 3.253461770018061e-06\n","Epoch[5/10](14220/14999) || training loss 0.1397 || training accuracy 95.00% || lr 3.252391464311994e-06\n","Epoch[5/10](14240/14999) || training loss 0.1339 || training accuracy 94.38% || lr 3.2513211586059267e-06\n","Epoch[5/10](14260/14999) || training loss 0.1326 || training accuracy 95.00% || lr 3.2502508528998595e-06\n","Epoch[5/10](14280/14999) || training loss 0.0715 || training accuracy 98.75% || lr 3.249180547193792e-06\n","Epoch[5/10](14300/14999) || training loss 0.08844 || training accuracy 95.62% || lr 3.248110241487725e-06\n","Epoch[5/10](14320/14999) || training loss 0.1398 || training accuracy 95.00% || lr 3.2470399357816574e-06\n","Epoch[5/10](14340/14999) || training loss 0.08549 || training accuracy 97.50% || lr 3.24596963007559e-06\n","Epoch[5/10](14360/14999) || training loss 0.05761 || training accuracy 98.12% || lr 3.244899324369523e-06\n","Epoch[5/10](14380/14999) || training loss 0.03833 || training accuracy 100.00% || lr 3.2438290186634557e-06\n","Epoch[5/10](14400/14999) || training loss 0.03012 || training accuracy 98.75% || lr 3.2427587129573885e-06\n","Epoch[5/10](14420/14999) || training loss 0.1283 || training accuracy 94.38% || lr 3.241688407251321e-06\n","Epoch[5/10](14440/14999) || training loss 0.05538 || training accuracy 96.25% || lr 3.240618101545254e-06\n","Epoch[5/10](14460/14999) || training loss 0.05273 || training accuracy 98.12% || lr 3.2395477958391864e-06\n","Epoch[5/10](14480/14999) || training loss 0.07843 || training accuracy 95.62% || lr 3.238477490133119e-06\n","Epoch[5/10](14500/14999) || training loss 0.07241 || training accuracy 97.50% || lr 3.237407184427052e-06\n","Epoch[5/10](14520/14999) || training loss 0.05052 || training accuracy 97.50% || lr 3.2363368787209843e-06\n","Epoch[5/10](14540/14999) || training loss 0.08855 || training accuracy 94.38% || lr 3.2352665730149175e-06\n","Epoch[5/10](14560/14999) || training loss 0.09135 || training accuracy 98.12% || lr 3.23419626730885e-06\n","Epoch[5/10](14580/14999) || training loss 0.1488 || training accuracy 95.00% || lr 3.233125961602783e-06\n","Epoch[5/10](14600/14999) || training loss 0.1613 || training accuracy 94.38% || lr 3.2320556558967154e-06\n","Epoch[5/10](14620/14999) || training loss 0.05787 || training accuracy 98.75% || lr 3.2309853501906477e-06\n","Epoch[5/10](14640/14999) || training loss 0.1016 || training accuracy 96.88% || lr 3.229915044484581e-06\n","Epoch[5/10](14660/14999) || training loss 0.05616 || training accuracy 97.50% || lr 3.2288447387785133e-06\n","Epoch[5/10](14680/14999) || training loss 0.1263 || training accuracy 95.62% || lr 3.2277744330724465e-06\n","Epoch[5/10](14700/14999) || training loss 0.1163 || training accuracy 96.25% || lr 3.226704127366379e-06\n","Epoch[5/10](14720/14999) || training loss 0.06606 || training accuracy 96.88% || lr 3.2256338216603116e-06\n","Epoch[5/10](14740/14999) || training loss 0.1002 || training accuracy 96.25% || lr 3.2245635159542444e-06\n","Epoch[5/10](14760/14999) || training loss 0.1323 || training accuracy 95.00% || lr 3.2234932102481767e-06\n","Epoch[5/10](14780/14999) || training loss 0.06092 || training accuracy 96.25% || lr 3.22242290454211e-06\n","Epoch[5/10](14800/14999) || training loss 0.1304 || training accuracy 95.00% || lr 3.2213525988360422e-06\n","Epoch[5/10](14820/14999) || training loss 0.06839 || training accuracy 96.88% || lr 3.220282293129975e-06\n","Epoch[5/10](14840/14999) || training loss 0.1395 || training accuracy 95.00% || lr 3.219211987423908e-06\n","Epoch[5/10](14860/14999) || training loss 0.05737 || training accuracy 98.75% || lr 3.2181416817178406e-06\n","Epoch[5/10](14880/14999) || training loss 0.09112 || training accuracy 95.62% || lr 3.2170713760117733e-06\n","Epoch[5/10](14900/14999) || training loss 0.1007 || training accuracy 96.25% || lr 3.2160010703057057e-06\n","Epoch[5/10](14920/14999) || training loss 0.09009 || training accuracy 96.25% || lr 3.2149307645996385e-06\n","Epoch[5/10](14940/14999) || training loss 0.08538 || training accuracy 96.25% || lr 3.2138604588935712e-06\n","Epoch[5/10](14960/14999) || training loss 0.07028 || training accuracy 97.50% || lr 3.212790153187504e-06\n","Epoch[5/10](14980/14999) || training loss 0.09203 || training accuracy 98.12% || lr 3.211719847481437e-06\n","Calculating validation results...\n","100% 235/235 [02:22<00:00,  1.65it/s]\n","New best model for val acc : 91.14%! saving the best model..\n","[Val] acc : 91.14%, loss: 0.3084, F1 : 0.9114 || best acc : 91.14%, best loss: 0.2442\n","Time elapsed:  401.05 min\n","\n","Epoch[6/10](20/14999) || training loss 0.04803 || training accuracy 99.38% || lr 3.2096327513546057e-06\n","Epoch[6/10](40/14999) || training loss 0.05893 || training accuracy 97.50% || lr 3.208562445648538e-06\n","Epoch[6/10](60/14999) || training loss 0.03242 || training accuracy 99.38% || lr 3.207492139942471e-06\n","Epoch[6/10](80/14999) || training loss 0.0694 || training accuracy 97.50% || lr 3.2064218342364036e-06\n","Epoch[6/10](100/14999) || training loss 0.04747 || training accuracy 97.50% || lr 3.2053515285303364e-06\n","Epoch[6/10](120/14999) || training loss 0.07155 || training accuracy 96.88% || lr 3.204281222824269e-06\n","Epoch[6/10](140/14999) || training loss 0.05433 || training accuracy 98.12% || lr 3.2032109171182015e-06\n","Epoch[6/10](160/14999) || training loss 0.05604 || training accuracy 98.12% || lr 3.2021406114121347e-06\n","Epoch[6/10](180/14999) || training loss 0.06663 || training accuracy 97.50% || lr 3.201070305706067e-06\n","Epoch[6/10](200/14999) || training loss 0.01717 || training accuracy 99.38% || lr 3.2e-06\n","Epoch[6/10](220/14999) || training loss 0.07495 || training accuracy 98.75% || lr 3.1989296942939326e-06\n","Epoch[6/10](240/14999) || training loss 0.04058 || training accuracy 98.75% || lr 3.197859388587865e-06\n","Epoch[6/10](260/14999) || training loss 0.07004 || training accuracy 98.12% || lr 3.196789082881798e-06\n","Epoch[6/10](280/14999) || training loss 0.02392 || training accuracy 99.38% || lr 3.1957187771757305e-06\n","Epoch[6/10](300/14999) || training loss 0.1054 || training accuracy 96.25% || lr 3.1946484714696637e-06\n","Epoch[6/10](320/14999) || training loss 0.08401 || training accuracy 97.50% || lr 3.193578165763596e-06\n","Epoch[6/10](340/14999) || training loss 0.1223 || training accuracy 95.62% || lr 3.1925078600575284e-06\n","Epoch[6/10](360/14999) || training loss 0.06011 || training accuracy 98.75% || lr 3.1914375543514616e-06\n","Epoch[6/10](380/14999) || training loss 0.0968 || training accuracy 96.25% || lr 3.190367248645394e-06\n","Epoch[6/10](400/14999) || training loss 0.08964 || training accuracy 95.62% || lr 3.189296942939327e-06\n","Epoch[6/10](420/14999) || training loss 0.1314 || training accuracy 97.50% || lr 3.1882266372332595e-06\n","Epoch[6/10](440/14999) || training loss 0.03856 || training accuracy 98.75% || lr 3.1871563315271923e-06\n","Epoch[6/10](460/14999) || training loss 0.07168 || training accuracy 96.88% || lr 3.186086025821125e-06\n","Epoch[6/10](480/14999) || training loss 0.0642 || training accuracy 98.12% || lr 3.1850157201150574e-06\n","Epoch[6/10](500/14999) || training loss 0.08877 || training accuracy 96.25% || lr 3.1839454144089906e-06\n","Epoch[6/10](520/14999) || training loss 0.06639 || training accuracy 98.12% || lr 3.182875108702923e-06\n","Epoch[6/10](540/14999) || training loss 0.04267 || training accuracy 99.38% || lr 3.181804802996856e-06\n","Epoch[6/10](560/14999) || training loss 0.06601 || training accuracy 96.88% || lr 3.1807344972907885e-06\n","Epoch[6/10](580/14999) || training loss 0.06166 || training accuracy 98.12% || lr 3.1796641915847213e-06\n","Epoch[6/10](600/14999) || training loss 0.06714 || training accuracy 97.50% || lr 3.178593885878654e-06\n","Epoch[6/10](620/14999) || training loss 0.02975 || training accuracy 98.75% || lr 3.1775235801725864e-06\n","Epoch[6/10](640/14999) || training loss 0.07148 || training accuracy 98.75% || lr 3.1764532744665196e-06\n","Epoch[6/10](660/14999) || training loss 0.09889 || training accuracy 97.50% || lr 3.175382968760452e-06\n","Epoch[6/10](680/14999) || training loss 0.04036 || training accuracy 98.12% || lr 3.1743126630543847e-06\n","Epoch[6/10](700/14999) || training loss 0.05589 || training accuracy 98.12% || lr 3.1732423573483175e-06\n","Epoch[6/10](720/14999) || training loss 0.04841 || training accuracy 98.12% || lr 3.1721720516422503e-06\n","Epoch[6/10](740/14999) || training loss 0.04722 || training accuracy 98.12% || lr 3.171101745936183e-06\n","Epoch[6/10](760/14999) || training loss 0.05012 || training accuracy 98.75% || lr 3.1700314402301154e-06\n","Epoch[6/10](780/14999) || training loss 0.0406 || training accuracy 98.75% || lr 3.168961134524048e-06\n","Epoch[6/10](800/14999) || training loss 0.0329 || training accuracy 98.75% || lr 3.167890828817981e-06\n","Epoch[6/10](820/14999) || training loss 0.0487 || training accuracy 97.50% || lr 3.1668205231119137e-06\n","Epoch[6/10](840/14999) || training loss 0.08599 || training accuracy 96.88% || lr 3.1657502174058465e-06\n","Epoch[6/10](860/14999) || training loss 0.05278 || training accuracy 97.50% || lr 3.1646799116997793e-06\n","Epoch[6/10](880/14999) || training loss 0.05101 || training accuracy 98.12% || lr 3.1636096059937116e-06\n","Epoch[6/10](900/14999) || training loss 0.07389 || training accuracy 98.12% || lr 3.1625393002876444e-06\n","Epoch[6/10](920/14999) || training loss 0.03455 || training accuracy 98.75% || lr 3.161468994581577e-06\n","Epoch[6/10](940/14999) || training loss 0.05891 || training accuracy 96.88% || lr 3.16039868887551e-06\n","Epoch[6/10](960/14999) || training loss 0.02729 || training accuracy 100.00% || lr 3.1593283831694427e-06\n","Epoch[6/10](980/14999) || training loss 0.06102 || training accuracy 96.25% || lr 3.158258077463375e-06\n","Epoch[6/10](1000/14999) || training loss 0.05272 || training accuracy 99.38% || lr 3.1571877717573083e-06\n","Epoch[6/10](1020/14999) || training loss 0.07162 || training accuracy 97.50% || lr 3.1561174660512406e-06\n","Epoch[6/10](1040/14999) || training loss 0.05137 || training accuracy 98.75% || lr 3.155047160345174e-06\n","Epoch[6/10](1060/14999) || training loss 0.0721 || training accuracy 96.25% || lr 3.153976854639106e-06\n","Epoch[6/10](1080/14999) || training loss 0.07058 || training accuracy 97.50% || lr 3.1529065489330385e-06\n","Epoch[6/10](1100/14999) || training loss 0.07519 || training accuracy 98.12% || lr 3.1518362432269717e-06\n","Epoch[6/10](1120/14999) || training loss 0.05008 || training accuracy 96.88% || lr 3.150765937520904e-06\n","Epoch[6/10](1140/14999) || training loss 0.0701 || training accuracy 98.12% || lr 3.1496956318148373e-06\n","Epoch[6/10](1160/14999) || training loss 0.05131 || training accuracy 97.50% || lr 3.1486253261087696e-06\n","Epoch[6/10](1180/14999) || training loss 0.06257 || training accuracy 98.12% || lr 3.147555020402703e-06\n","Epoch[6/10](1200/14999) || training loss 0.03946 || training accuracy 98.12% || lr 3.146484714696635e-06\n","Epoch[6/10](1220/14999) || training loss 0.1275 || training accuracy 96.88% || lr 3.1454144089905675e-06\n","Epoch[6/10](1240/14999) || training loss 0.05525 || training accuracy 98.12% || lr 3.1443441032845007e-06\n","Epoch[6/10](1260/14999) || training loss 0.04207 || training accuracy 99.38% || lr 3.143273797578433e-06\n","Epoch[6/10](1280/14999) || training loss 0.08083 || training accuracy 98.12% || lr 3.1422034918723662e-06\n","Epoch[6/10](1300/14999) || training loss 0.07526 || training accuracy 98.12% || lr 3.1411331861662986e-06\n","Epoch[6/10](1320/14999) || training loss 0.05778 || training accuracy 98.12% || lr 3.1400628804602314e-06\n","Epoch[6/10](1340/14999) || training loss 0.1105 || training accuracy 95.62% || lr 3.138992574754164e-06\n","Epoch[6/10](1360/14999) || training loss 0.0413 || training accuracy 98.75% || lr 3.1379222690480965e-06\n","Epoch[6/10](1380/14999) || training loss 0.05394 || training accuracy 97.50% || lr 3.1368519633420297e-06\n","Epoch[6/10](1400/14999) || training loss 0.1098 || training accuracy 96.25% || lr 3.135781657635962e-06\n","Epoch[6/10](1420/14999) || training loss 0.04695 || training accuracy 98.12% || lr 3.134711351929895e-06\n","Epoch[6/10](1440/14999) || training loss 0.06255 || training accuracy 96.88% || lr 3.1336410462238276e-06\n","Epoch[6/10](1460/14999) || training loss 0.07776 || training accuracy 97.50% || lr 3.1325707405177604e-06\n","Epoch[6/10](1480/14999) || training loss 0.1062 || training accuracy 95.62% || lr 3.131500434811693e-06\n","Epoch[6/10](1500/14999) || training loss 0.07561 || training accuracy 96.88% || lr 3.1304301291056255e-06\n","Epoch[6/10](1520/14999) || training loss 0.05971 || training accuracy 98.75% || lr 3.1293598233995583e-06\n","Epoch[6/10](1540/14999) || training loss 0.1495 || training accuracy 93.75% || lr 3.128289517693491e-06\n","Epoch[6/10](1560/14999) || training loss 0.08148 || training accuracy 97.50% || lr 3.127219211987424e-06\n","Epoch[6/10](1580/14999) || training loss 0.06566 || training accuracy 98.75% || lr 3.1261489062813566e-06\n","Epoch[6/10](1600/14999) || training loss 0.06401 || training accuracy 96.88% || lr 3.1250786005752894e-06\n","Epoch[6/10](1620/14999) || training loss 0.07979 || training accuracy 96.88% || lr 3.1240082948692217e-06\n","Epoch[6/10](1640/14999) || training loss 0.05543 || training accuracy 97.50% || lr 3.1229379891631545e-06\n","Epoch[6/10](1660/14999) || training loss 0.06298 || training accuracy 98.75% || lr 3.1218676834570872e-06\n","Epoch[6/10](1680/14999) || training loss 0.1331 || training accuracy 96.88% || lr 3.12079737775102e-06\n","Epoch[6/10](1700/14999) || training loss 0.1308 || training accuracy 96.25% || lr 3.119727072044953e-06\n","Epoch[6/10](1720/14999) || training loss 0.07275 || training accuracy 97.50% || lr 3.118656766338885e-06\n","Epoch[6/10](1740/14999) || training loss 0.06564 || training accuracy 98.75% || lr 3.1175864606328183e-06\n","Epoch[6/10](1760/14999) || training loss 0.089 || training accuracy 96.88% || lr 3.1165161549267507e-06\n","Epoch[6/10](1780/14999) || training loss 0.06317 || training accuracy 96.25% || lr 3.1154458492206835e-06\n","Epoch[6/10](1800/14999) || training loss 0.06901 || training accuracy 98.12% || lr 3.1143755435146162e-06\n","Epoch[6/10](1820/14999) || training loss 0.08862 || training accuracy 96.88% || lr 3.113305237808549e-06\n","Epoch[6/10](1840/14999) || training loss 0.09378 || training accuracy 96.88% || lr 3.112234932102482e-06\n","Epoch[6/10](1860/14999) || training loss 0.0601 || training accuracy 97.50% || lr 3.111164626396414e-06\n","Epoch[6/10](1880/14999) || training loss 0.03717 || training accuracy 98.75% || lr 3.1100943206903473e-06\n","Epoch[6/10](1900/14999) || training loss 0.05063 || training accuracy 98.12% || lr 3.1090240149842797e-06\n","Epoch[6/10](1920/14999) || training loss 0.07922 || training accuracy 95.62% || lr 3.1079537092782125e-06\n","Epoch[6/10](1940/14999) || training loss 0.04442 || training accuracy 98.75% || lr 3.1068834035721452e-06\n","Epoch[6/10](1960/14999) || training loss 0.09336 || training accuracy 96.88% || lr 3.1058130978660776e-06\n","Epoch[6/10](1980/14999) || training loss 0.09199 || training accuracy 97.50% || lr 3.1047427921600108e-06\n","Epoch[6/10](2000/14999) || training loss 0.1002 || training accuracy 96.88% || lr 3.103672486453943e-06\n","Epoch[6/10](2020/14999) || training loss 0.06782 || training accuracy 96.88% || lr 3.1026021807478763e-06\n","Epoch[6/10](2040/14999) || training loss 0.076 || training accuracy 98.12% || lr 3.1015318750418087e-06\n","Epoch[6/10](2060/14999) || training loss 0.09146 || training accuracy 96.88% || lr 3.100461569335741e-06\n","Epoch[6/10](2080/14999) || training loss 0.07954 || training accuracy 98.12% || lr 3.0993912636296742e-06\n","Epoch[6/10](2100/14999) || training loss 0.09609 || training accuracy 97.50% || lr 3.0983209579236066e-06\n","Epoch[6/10](2120/14999) || training loss 0.09432 || training accuracy 96.88% || lr 3.0972506522175398e-06\n","Epoch[6/10](2140/14999) || training loss 0.07322 || training accuracy 96.88% || lr 3.096180346511472e-06\n","Epoch[6/10](2160/14999) || training loss 0.08158 || training accuracy 96.88% || lr 3.095110040805405e-06\n","Epoch[6/10](2180/14999) || training loss 0.06232 || training accuracy 98.75% || lr 3.0940397350993377e-06\n","Epoch[6/10](2200/14999) || training loss 0.04353 || training accuracy 98.12% || lr 3.09296942939327e-06\n","Epoch[6/10](2220/14999) || training loss 0.01982 || training accuracy 99.38% || lr 3.0918991236872032e-06\n","Epoch[6/10](2240/14999) || training loss 0.06996 || training accuracy 96.88% || lr 3.0908288179811356e-06\n","Epoch[6/10](2260/14999) || training loss 0.0769 || training accuracy 96.88% || lr 3.0897585122750683e-06\n","Epoch[6/10](2280/14999) || training loss 0.02688 || training accuracy 98.12% || lr 3.088688206569001e-06\n","Epoch[6/10](2300/14999) || training loss 0.08075 || training accuracy 96.88% || lr 3.087617900862934e-06\n","Epoch[6/10](2320/14999) || training loss 0.07348 || training accuracy 96.88% || lr 3.0865475951568667e-06\n","Epoch[6/10](2340/14999) || training loss 0.0766 || training accuracy 96.88% || lr 3.085477289450799e-06\n","Epoch[6/10](2360/14999) || training loss 0.08473 || training accuracy 95.62% || lr 3.0844069837447318e-06\n","Epoch[6/10](2380/14999) || training loss 0.1091 || training accuracy 96.25% || lr 3.0833366780386646e-06\n","Epoch[6/10](2400/14999) || training loss 0.03802 || training accuracy 99.38% || lr 3.0822663723325973e-06\n","Epoch[6/10](2420/14999) || training loss 0.02633 || training accuracy 100.00% || lr 3.08119606662653e-06\n","Epoch[6/10](2440/14999) || training loss 0.03978 || training accuracy 99.38% || lr 3.080125760920463e-06\n","Epoch[6/10](2460/14999) || training loss 0.1189 || training accuracy 95.62% || lr 3.0790554552143957e-06\n","Epoch[6/10](2480/14999) || training loss 0.06048 || training accuracy 98.12% || lr 3.077985149508328e-06\n","Epoch[6/10](2500/14999) || training loss 0.05159 || training accuracy 97.50% || lr 3.0769148438022608e-06\n","Epoch[6/10](2520/14999) || training loss 0.0739 || training accuracy 97.50% || lr 3.0758445380961936e-06\n","Epoch[6/10](2540/14999) || training loss 0.06075 || training accuracy 97.50% || lr 3.0747742323901263e-06\n","Epoch[6/10](2560/14999) || training loss 0.04168 || training accuracy 98.12% || lr 3.073703926684059e-06\n","Epoch[6/10](2580/14999) || training loss 0.1197 || training accuracy 95.62% || lr 3.072633620977992e-06\n","Epoch[6/10](2600/14999) || training loss 0.04925 || training accuracy 99.38% || lr 3.0715633152719242e-06\n","Epoch[6/10](2620/14999) || training loss 0.09531 || training accuracy 98.12% || lr 3.0704930095658574e-06\n","Epoch[6/10](2640/14999) || training loss 0.08934 || training accuracy 96.88% || lr 3.0694227038597898e-06\n","Epoch[6/10](2660/14999) || training loss 0.06883 || training accuracy 98.75% || lr 3.0683523981537225e-06\n","Epoch[6/10](2680/14999) || training loss 0.1104 || training accuracy 96.25% || lr 3.0672820924476553e-06\n","Epoch[6/10](2700/14999) || training loss 0.05574 || training accuracy 98.75% || lr 3.0662117867415877e-06\n","Epoch[6/10](2720/14999) || training loss 0.05816 || training accuracy 97.50% || lr 3.065141481035521e-06\n","Epoch[6/10](2740/14999) || training loss 0.08754 || training accuracy 96.25% || lr 3.0640711753294532e-06\n","Epoch[6/10](2760/14999) || training loss 0.07545 || training accuracy 96.88% || lr 3.0630008696233864e-06\n","Epoch[6/10](2780/14999) || training loss 0.0643 || training accuracy 98.12% || lr 3.0619305639173188e-06\n","Epoch[6/10](2800/14999) || training loss 0.05987 || training accuracy 98.75% || lr 3.060860258211251e-06\n","Epoch[6/10](2820/14999) || training loss 0.03084 || training accuracy 98.12% || lr 3.0597899525051843e-06\n","Epoch[6/10](2840/14999) || training loss 0.04884 || training accuracy 98.12% || lr 3.0587196467991167e-06\n","Epoch[6/10](2860/14999) || training loss 0.05897 || training accuracy 97.50% || lr 3.05764934109305e-06\n","Epoch[6/10](2880/14999) || training loss 0.08719 || training accuracy 96.88% || lr 3.056579035386982e-06\n","Epoch[6/10](2900/14999) || training loss 0.05691 || training accuracy 97.50% || lr 3.055508729680915e-06\n","Epoch[6/10](2920/14999) || training loss 0.1072 || training accuracy 95.62% || lr 3.0544384239748478e-06\n","Epoch[6/10](2940/14999) || training loss 0.08422 || training accuracy 95.62% || lr 3.05336811826878e-06\n","Epoch[6/10](2960/14999) || training loss 0.02969 || training accuracy 98.75% || lr 3.0522978125627133e-06\n","Epoch[6/10](2980/14999) || training loss 0.05526 || training accuracy 98.12% || lr 3.0512275068566457e-06\n","Epoch[6/10](3000/14999) || training loss 0.05064 || training accuracy 98.12% || lr 3.0501572011505784e-06\n","Epoch[6/10](3020/14999) || training loss 0.05394 || training accuracy 98.75% || lr 3.049086895444511e-06\n","Epoch[6/10](3040/14999) || training loss 0.08091 || training accuracy 96.88% || lr 3.048016589738444e-06\n","Epoch[6/10](3060/14999) || training loss 0.03132 || training accuracy 98.75% || lr 3.0469462840323767e-06\n","Epoch[6/10](3080/14999) || training loss 0.02406 || training accuracy 99.38% || lr 3.045875978326309e-06\n","Epoch[6/10](3100/14999) || training loss 0.1269 || training accuracy 95.00% || lr 3.0448056726202423e-06\n","Epoch[6/10](3120/14999) || training loss 0.05626 || training accuracy 98.75% || lr 3.0437353669141746e-06\n","Epoch[6/10](3140/14999) || training loss 0.1685 || training accuracy 96.25% || lr 3.0426650612081074e-06\n","Epoch[6/10](3160/14999) || training loss 0.07517 || training accuracy 98.12% || lr 3.04159475550204e-06\n","Epoch[6/10](3180/14999) || training loss 0.07196 || training accuracy 97.50% || lr 3.040524449795973e-06\n","Epoch[6/10](3200/14999) || training loss 0.06534 || training accuracy 97.50% || lr 3.0394541440899057e-06\n","Epoch[6/10](3220/14999) || training loss 0.08996 || training accuracy 96.88% || lr 3.038383838383838e-06\n","Epoch[6/10](3240/14999) || training loss 0.1338 || training accuracy 97.50% || lr 3.037313532677771e-06\n","Epoch[6/10](3260/14999) || training loss 0.0958 || training accuracy 96.88% || lr 3.0362432269717036e-06\n","Epoch[6/10](3280/14999) || training loss 0.03277 || training accuracy 98.75% || lr 3.0351729212656364e-06\n","Epoch[6/10](3300/14999) || training loss 0.07769 || training accuracy 95.62% || lr 3.034102615559569e-06\n","Epoch[6/10](3320/14999) || training loss 0.05423 || training accuracy 98.12% || lr 3.033032309853502e-06\n","Epoch[6/10](3340/14999) || training loss 0.04075 || training accuracy 98.75% || lr 3.0319620041474343e-06\n","Epoch[6/10](3360/14999) || training loss 0.02618 || training accuracy 99.38% || lr 3.030891698441367e-06\n","Epoch[6/10](3380/14999) || training loss 0.09996 || training accuracy 96.88% || lr 3.0298213927353e-06\n","Epoch[6/10](3400/14999) || training loss 0.08874 || training accuracy 94.38% || lr 3.0287510870292326e-06\n","Epoch[6/10](3420/14999) || training loss 0.1554 || training accuracy 96.25% || lr 3.0276807813231654e-06\n","Epoch[6/10](3440/14999) || training loss 0.02374 || training accuracy 100.00% || lr 3.0266104756170978e-06\n","Epoch[6/10](3460/14999) || training loss 0.07314 || training accuracy 97.50% || lr 3.025540169911031e-06\n","Epoch[6/10](3480/14999) || training loss 0.04506 || training accuracy 98.12% || lr 3.0244698642049633e-06\n","Epoch[6/10](3500/14999) || training loss 0.114 || training accuracy 96.88% || lr 3.023399558498896e-06\n","Epoch[6/10](3520/14999) || training loss 0.07646 || training accuracy 96.88% || lr 3.022329252792829e-06\n","Epoch[6/10](3540/14999) || training loss 0.06236 || training accuracy 98.12% || lr 3.021258947086761e-06\n","Epoch[6/10](3560/14999) || training loss 0.04904 || training accuracy 98.75% || lr 3.0201886413806944e-06\n","Epoch[6/10](3580/14999) || training loss 0.03994 || training accuracy 99.38% || lr 3.0191183356746267e-06\n","Epoch[6/10](3600/14999) || training loss 0.09936 || training accuracy 97.50% || lr 3.01804802996856e-06\n","Epoch[6/10](3620/14999) || training loss 0.09284 || training accuracy 96.88% || lr 3.0169777242624923e-06\n","Epoch[6/10](3640/14999) || training loss 0.08402 || training accuracy 96.88% || lr 3.0159074185564246e-06\n","Epoch[6/10](3660/14999) || training loss 0.07231 || training accuracy 95.00% || lr 3.014837112850358e-06\n","Epoch[6/10](3680/14999) || training loss 0.0956 || training accuracy 96.88% || lr 3.01376680714429e-06\n","Epoch[6/10](3700/14999) || training loss 0.04769 || training accuracy 98.12% || lr 3.0126965014382234e-06\n","Epoch[6/10](3720/14999) || training loss 0.03879 || training accuracy 98.75% || lr 3.0116261957321557e-06\n","Epoch[6/10](3740/14999) || training loss 0.02664 || training accuracy 98.75% || lr 3.010555890026089e-06\n","Epoch[6/10](3760/14999) || training loss 0.07268 || training accuracy 98.12% || lr 3.0094855843200213e-06\n","Epoch[6/10](3780/14999) || training loss 0.095 || training accuracy 98.12% || lr 3.0084152786139536e-06\n","Epoch[6/10](3800/14999) || training loss 0.05171 || training accuracy 98.12% || lr 3.007344972907887e-06\n","Epoch[6/10](3820/14999) || training loss 0.06211 || training accuracy 96.88% || lr 3.006274667201819e-06\n","Epoch[6/10](3840/14999) || training loss 0.07265 || training accuracy 96.88% || lr 3.0052043614957524e-06\n","Epoch[6/10](3860/14999) || training loss 0.04895 || training accuracy 98.12% || lr 3.0041340557896847e-06\n","Epoch[6/10](3880/14999) || training loss 0.1007 || training accuracy 95.00% || lr 3.0030637500836175e-06\n","Epoch[6/10](3900/14999) || training loss 0.09978 || training accuracy 98.12% || lr 3.0019934443775503e-06\n","Epoch[6/10](3920/14999) || training loss 0.04953 || training accuracy 99.38% || lr 3.0009231386714826e-06\n","Epoch[6/10](3940/14999) || training loss 0.04367 || training accuracy 99.38% || lr 2.999852832965416e-06\n","Epoch[6/10](3960/14999) || training loss 0.115 || training accuracy 96.25% || lr 2.998782527259348e-06\n","Epoch[6/10](3980/14999) || training loss 0.1094 || training accuracy 95.62% || lr 2.997712221553281e-06\n","Epoch[6/10](4000/14999) || training loss 0.06024 || training accuracy 98.75% || lr 2.9966419158472137e-06\n","Epoch[6/10](4020/14999) || training loss 0.07582 || training accuracy 96.88% || lr 2.9955716101411465e-06\n","Epoch[6/10](4040/14999) || training loss 0.04109 || training accuracy 98.75% || lr 2.9945013044350793e-06\n","Epoch[6/10](4060/14999) || training loss 0.08372 || training accuracy 96.25% || lr 2.993430998729012e-06\n","Epoch[6/10](4080/14999) || training loss 0.02327 || training accuracy 99.38% || lr 2.9923606930229444e-06\n","Epoch[6/10](4100/14999) || training loss 0.09279 || training accuracy 96.25% || lr 2.991290387316877e-06\n","Epoch[6/10](4120/14999) || training loss 0.08869 || training accuracy 95.62% || lr 2.99022008161081e-06\n","Epoch[6/10](4140/14999) || training loss 0.09176 || training accuracy 96.25% || lr 2.9891497759047427e-06\n","Epoch[6/10](4160/14999) || training loss 0.1164 || training accuracy 96.88% || lr 2.9880794701986755e-06\n","Epoch[6/10](4180/14999) || training loss 0.129 || training accuracy 96.25% || lr 2.987009164492608e-06\n","Epoch[6/10](4200/14999) || training loss 0.05299 || training accuracy 98.75% || lr 2.985938858786541e-06\n","Epoch[6/10](4220/14999) || training loss 0.04403 || training accuracy 97.50% || lr 2.9848685530804734e-06\n","Epoch[6/10](4240/14999) || training loss 0.06257 || training accuracy 98.12% || lr 2.983798247374406e-06\n","Epoch[6/10](4260/14999) || training loss 0.05824 || training accuracy 98.75% || lr 2.982727941668339e-06\n","Epoch[6/10](4280/14999) || training loss 0.05037 || training accuracy 97.50% || lr 2.9816576359622713e-06\n","Epoch[6/10](4300/14999) || training loss 0.1278 || training accuracy 95.62% || lr 2.9805873302562045e-06\n","Epoch[6/10](4320/14999) || training loss 0.07791 || training accuracy 96.88% || lr 2.979517024550137e-06\n","Epoch[6/10](4340/14999) || training loss 0.05725 || training accuracy 98.12% || lr 2.97844671884407e-06\n","Epoch[6/10](4360/14999) || training loss 0.04984 || training accuracy 98.12% || lr 2.9773764131380024e-06\n","Epoch[6/10](4380/14999) || training loss 0.06149 || training accuracy 97.50% || lr 2.976306107431935e-06\n","Epoch[6/10](4400/14999) || training loss 0.05685 || training accuracy 98.12% || lr 2.975235801725868e-06\n","Epoch[6/10](4420/14999) || training loss 0.1023 || training accuracy 96.25% || lr 2.9741654960198003e-06\n","Epoch[6/10](4440/14999) || training loss 0.07372 || training accuracy 97.50% || lr 2.9730951903137335e-06\n","Epoch[6/10](4460/14999) || training loss 0.07949 || training accuracy 97.50% || lr 2.972024884607666e-06\n","Epoch[6/10](4480/14999) || training loss 0.0834 || training accuracy 95.62% || lr 2.970954578901599e-06\n","Epoch[6/10](4500/14999) || training loss 0.0583 || training accuracy 96.88% || lr 2.9698842731955314e-06\n","Epoch[6/10](4520/14999) || training loss 0.02949 || training accuracy 98.75% || lr 2.9688139674894637e-06\n","Epoch[6/10](4540/14999) || training loss 0.1002 || training accuracy 96.25% || lr 2.967743661783397e-06\n","Epoch[6/10](4560/14999) || training loss 0.05426 || training accuracy 96.88% || lr 2.9666733560773293e-06\n","Epoch[6/10](4580/14999) || training loss 0.08479 || training accuracy 97.50% || lr 2.9656030503712625e-06\n","Epoch[6/10](4600/14999) || training loss 0.0514 || training accuracy 98.12% || lr 2.964532744665195e-06\n","Epoch[6/10](4620/14999) || training loss 0.07361 || training accuracy 97.50% || lr 2.9634624389591276e-06\n","Epoch[6/10](4640/14999) || training loss 0.0553 || training accuracy 97.50% || lr 2.9623921332530604e-06\n","Epoch[6/10](4660/14999) || training loss 0.04409 || training accuracy 99.38% || lr 2.9613218275469927e-06\n","Epoch[6/10](4680/14999) || training loss 0.05304 || training accuracy 97.50% || lr 2.960251521840926e-06\n","Epoch[6/10](4700/14999) || training loss 0.05972 || training accuracy 98.75% || lr 2.9591812161348583e-06\n","Epoch[6/10](4720/14999) || training loss 0.0577 || training accuracy 98.12% || lr 2.958110910428791e-06\n","Epoch[6/10](4740/14999) || training loss 0.05773 || training accuracy 97.50% || lr 2.957040604722724e-06\n","Epoch[6/10](4760/14999) || training loss 0.07402 || training accuracy 96.25% || lr 2.9559702990166566e-06\n","Epoch[6/10](4780/14999) || training loss 0.07541 || training accuracy 98.75% || lr 2.9548999933105894e-06\n","Epoch[6/10](4800/14999) || training loss 0.0849 || training accuracy 97.50% || lr 2.9538296876045217e-06\n","Epoch[6/10](4820/14999) || training loss 0.02578 || training accuracy 100.00% || lr 2.9527593818984545e-06\n","Epoch[6/10](4840/14999) || training loss 0.04852 || training accuracy 98.12% || lr 2.9516890761923872e-06\n","Epoch[6/10](4860/14999) || training loss 0.05648 || training accuracy 96.88% || lr 2.95061877048632e-06\n","Epoch[6/10](4880/14999) || training loss 0.06276 || training accuracy 96.25% || lr 2.949548464780253e-06\n","Epoch[6/10](4900/14999) || training loss 0.09798 || training accuracy 96.25% || lr 2.9484781590741856e-06\n","Epoch[6/10](4920/14999) || training loss 0.1575 || training accuracy 95.00% || lr 2.947407853368118e-06\n","Epoch[6/10](4940/14999) || training loss 0.07949 || training accuracy 96.88% || lr 2.9463375476620507e-06\n","Epoch[6/10](4960/14999) || training loss 0.1111 || training accuracy 96.88% || lr 2.9452672419559835e-06\n","Epoch[6/10](4980/14999) || training loss 0.1607 || training accuracy 95.00% || lr 2.9441969362499162e-06\n","Epoch[6/10](5000/14999) || training loss 0.0384 || training accuracy 98.75% || lr 2.943126630543849e-06\n","Epoch[6/10](5020/14999) || training loss 0.09448 || training accuracy 98.12% || lr 2.9420563248377818e-06\n","Epoch[6/10](5040/14999) || training loss 0.05619 || training accuracy 98.75% || lr 2.9409860191317146e-06\n","Epoch[6/10](5060/14999) || training loss 0.06825 || training accuracy 97.50% || lr 2.939915713425647e-06\n","Epoch[6/10](5080/14999) || training loss 0.05252 || training accuracy 98.75% || lr 2.9388454077195797e-06\n","Epoch[6/10](5100/14999) || training loss 0.06822 || training accuracy 98.75% || lr 2.9377751020135125e-06\n","Epoch[6/10](5120/14999) || training loss 0.07927 || training accuracy 98.12% || lr 2.9367047963074452e-06\n","Epoch[6/10](5140/14999) || training loss 0.06392 || training accuracy 98.75% || lr 2.935634490601378e-06\n","Epoch[6/10](5160/14999) || training loss 0.02919 || training accuracy 99.38% || lr 2.9345641848953104e-06\n","Epoch[6/10](5180/14999) || training loss 0.0576 || training accuracy 97.50% || lr 2.9334938791892436e-06\n","Epoch[6/10](5200/14999) || training loss 0.05229 || training accuracy 98.75% || lr 2.932423573483176e-06\n","Epoch[6/10](5220/14999) || training loss 0.09283 || training accuracy 97.50% || lr 2.931353267777109e-06\n","Epoch[6/10](5240/14999) || training loss 0.0442 || training accuracy 98.12% || lr 2.9302829620710415e-06\n","Epoch[6/10](5260/14999) || training loss 0.0497 || training accuracy 98.75% || lr 2.929212656364974e-06\n","Epoch[6/10](5280/14999) || training loss 0.09214 || training accuracy 96.25% || lr 2.928142350658907e-06\n","Epoch[6/10](5300/14999) || training loss 0.04117 || training accuracy 98.12% || lr 2.9270720449528393e-06\n","Epoch[6/10](5320/14999) || training loss 0.054 || training accuracy 98.12% || lr 2.9260017392467725e-06\n","Epoch[6/10](5340/14999) || training loss 0.02933 || training accuracy 98.12% || lr 2.924931433540705e-06\n","Epoch[6/10](5360/14999) || training loss 0.064 || training accuracy 96.88% || lr 2.9238611278346372e-06\n","Epoch[6/10](5380/14999) || training loss 0.02497 || training accuracy 99.38% || lr 2.9227908221285704e-06\n","Epoch[6/10](5400/14999) || training loss 0.1426 || training accuracy 95.62% || lr 2.921720516422503e-06\n","Epoch[6/10](5420/14999) || training loss 0.1792 || training accuracy 96.25% || lr 2.920650210716436e-06\n","Epoch[6/10](5440/14999) || training loss 0.0885 || training accuracy 96.88% || lr 2.9195799050103683e-06\n","Epoch[6/10](5460/14999) || training loss 0.02027 || training accuracy 99.38% || lr 2.918509599304301e-06\n","Epoch[6/10](5480/14999) || training loss 0.02929 || training accuracy 99.38% || lr 2.917439293598234e-06\n","Epoch[6/10](5500/14999) || training loss 0.1146 || training accuracy 96.25% || lr 2.9163689878921667e-06\n","Epoch[6/10](5520/14999) || training loss 0.1038 || training accuracy 96.88% || lr 2.9152986821860994e-06\n","Epoch[6/10](5540/14999) || training loss 0.08735 || training accuracy 96.88% || lr 2.9142283764800318e-06\n","Epoch[6/10](5560/14999) || training loss 0.05448 || training accuracy 98.75% || lr 2.9131580707739646e-06\n","Epoch[6/10](5580/14999) || training loss 0.03357 || training accuracy 100.00% || lr 2.9120877650678973e-06\n","Epoch[6/10](5600/14999) || training loss 0.04027 || training accuracy 98.12% || lr 2.91101745936183e-06\n","Epoch[6/10](5620/14999) || training loss 0.06992 || training accuracy 98.12% || lr 2.909947153655763e-06\n","Epoch[6/10](5640/14999) || training loss 0.06782 || training accuracy 97.50% || lr 2.9088768479496957e-06\n","Epoch[6/10](5660/14999) || training loss 0.08099 || training accuracy 97.50% || lr 2.9078065422436284e-06\n","Epoch[6/10](5680/14999) || training loss 0.1484 || training accuracy 94.38% || lr 2.9067362365375608e-06\n","Epoch[6/10](5700/14999) || training loss 0.08465 || training accuracy 97.50% || lr 2.9056659308314936e-06\n","Epoch[6/10](5720/14999) || training loss 0.1204 || training accuracy 95.00% || lr 2.9045956251254263e-06\n","Epoch[6/10](5740/14999) || training loss 0.06763 || training accuracy 97.50% || lr 2.903525319419359e-06\n","Epoch[6/10](5760/14999) || training loss 0.1181 || training accuracy 97.50% || lr 2.902455013713292e-06\n","Epoch[6/10](5780/14999) || training loss 0.06568 || training accuracy 97.50% || lr 2.9013847080072246e-06\n","Epoch[6/10](5800/14999) || training loss 0.1324 || training accuracy 95.62% || lr 2.900314402301157e-06\n","Epoch[6/10](5820/14999) || training loss 0.04021 || training accuracy 98.75% || lr 2.8992440965950898e-06\n","Epoch[6/10](5840/14999) || training loss 0.0802 || training accuracy 96.88% || lr 2.8981737908890225e-06\n","Epoch[6/10](5860/14999) || training loss 0.09241 || training accuracy 96.88% || lr 2.8971034851829553e-06\n","Epoch[6/10](5880/14999) || training loss 0.1162 || training accuracy 95.62% || lr 2.896033179476888e-06\n","Epoch[6/10](5900/14999) || training loss 0.04615 || training accuracy 98.12% || lr 2.8949628737708204e-06\n","Epoch[6/10](5920/14999) || training loss 0.0874 || training accuracy 95.62% || lr 2.8938925680647536e-06\n","Epoch[6/10](5940/14999) || training loss 0.1334 || training accuracy 93.75% || lr 2.892822262358686e-06\n","Epoch[6/10](5960/14999) || training loss 0.06094 || training accuracy 96.88% || lr 2.8917519566526188e-06\n","Epoch[6/10](5980/14999) || training loss 0.08619 || training accuracy 96.88% || lr 2.8906816509465515e-06\n","Epoch[6/10](6000/14999) || training loss 0.05971 || training accuracy 97.50% || lr 2.889611345240484e-06\n","Epoch[6/10](6020/14999) || training loss 0.06111 || training accuracy 97.50% || lr 2.888541039534417e-06\n","Epoch[6/10](6040/14999) || training loss 0.08848 || training accuracy 97.50% || lr 2.8874707338283494e-06\n","Epoch[6/10](6060/14999) || training loss 0.05218 || training accuracy 98.75% || lr 2.8864004281222826e-06\n","Epoch[6/10](6080/14999) || training loss 0.1034 || training accuracy 96.25% || lr 2.885330122416215e-06\n","Epoch[6/10](6100/14999) || training loss 0.07209 || training accuracy 96.25% || lr 2.8842598167101473e-06\n","Epoch[6/10](6120/14999) || training loss 0.03018 || training accuracy 98.75% || lr 2.8831895110040805e-06\n","Epoch[6/10](6140/14999) || training loss 0.1006 || training accuracy 95.62% || lr 2.882119205298013e-06\n","Epoch[6/10](6160/14999) || training loss 0.04198 || training accuracy 98.12% || lr 2.881048899591946e-06\n","Epoch[6/10](6180/14999) || training loss 0.05456 || training accuracy 98.75% || lr 2.8799785938858784e-06\n","Epoch[6/10](6200/14999) || training loss 0.09515 || training accuracy 96.25% || lr 2.878908288179811e-06\n","Epoch[6/10](6220/14999) || training loss 0.1701 || training accuracy 95.62% || lr 2.877837982473744e-06\n","Epoch[6/10](6240/14999) || training loss 0.07733 || training accuracy 96.88% || lr 2.8767676767676763e-06\n","Epoch[6/10](6260/14999) || training loss 0.06086 || training accuracy 97.50% || lr 2.8756973710616095e-06\n","Epoch[6/10](6280/14999) || training loss 0.04897 || training accuracy 97.50% || lr 2.874627065355542e-06\n","Epoch[6/10](6300/14999) || training loss 0.09222 || training accuracy 95.00% || lr 2.873556759649475e-06\n","Epoch[6/10](6320/14999) || training loss 0.1133 || training accuracy 96.88% || lr 2.8724864539434074e-06\n","Epoch[6/10](6340/14999) || training loss 0.09184 || training accuracy 95.62% || lr 2.87141614823734e-06\n","Epoch[6/10](6360/14999) || training loss 0.05545 || training accuracy 98.75% || lr 2.870345842531273e-06\n","Epoch[6/10](6380/14999) || training loss 0.04028 || training accuracy 98.12% || lr 2.8692755368252053e-06\n","Epoch[6/10](6400/14999) || training loss 0.1259 || training accuracy 96.88% || lr 2.8682052311191385e-06\n","Epoch[6/10](6420/14999) || training loss 0.06513 || training accuracy 98.12% || lr 2.867134925413071e-06\n","Epoch[6/10](6440/14999) || training loss 0.0668 || training accuracy 97.50% || lr 2.8660646197070036e-06\n","Epoch[6/10](6460/14999) || training loss 0.04156 || training accuracy 98.75% || lr 2.8649943140009364e-06\n","Epoch[6/10](6480/14999) || training loss 0.08302 || training accuracy 97.50% || lr 2.863924008294869e-06\n","Epoch[6/10](6500/14999) || training loss 0.06572 || training accuracy 96.88% || lr 2.862853702588802e-06\n","Epoch[6/10](6520/14999) || training loss 0.1002 || training accuracy 96.88% || lr 2.8617833968827343e-06\n","Epoch[6/10](6540/14999) || training loss 0.0498 || training accuracy 97.50% || lr 2.860713091176667e-06\n","Epoch[6/10](6560/14999) || training loss 0.04394 || training accuracy 98.12% || lr 2.8596427854706e-06\n","Epoch[6/10](6580/14999) || training loss 0.067 || training accuracy 99.38% || lr 2.8585724797645326e-06\n","Epoch[6/10](6600/14999) || training loss 0.07946 || training accuracy 98.12% || lr 2.8575021740584654e-06\n","Epoch[6/10](6620/14999) || training loss 0.06415 || training accuracy 96.25% || lr 2.856431868352398e-06\n","Epoch[6/10](6640/14999) || training loss 0.05037 || training accuracy 98.75% || lr 2.8553615626463305e-06\n","Epoch[6/10](6660/14999) || training loss 0.07867 || training accuracy 97.50% || lr 2.8542912569402637e-06\n","Epoch[6/10](6680/14999) || training loss 0.08756 || training accuracy 96.25% || lr 2.853220951234196e-06\n","Epoch[6/10](6700/14999) || training loss 0.02597 || training accuracy 100.00% || lr 2.852150645528129e-06\n","Epoch[6/10](6720/14999) || training loss 0.1015 || training accuracy 98.12% || lr 2.8510803398220616e-06\n","Epoch[6/10](6740/14999) || training loss 0.08062 || training accuracy 97.50% || lr 2.850010034115994e-06\n","Epoch[6/10](6760/14999) || training loss 0.04375 || training accuracy 98.75% || lr 2.848939728409927e-06\n","Epoch[6/10](6780/14999) || training loss 0.03398 || training accuracy 98.12% || lr 2.8478694227038595e-06\n","Epoch[6/10](6800/14999) || training loss 0.09437 || training accuracy 95.62% || lr 2.8467991169977927e-06\n","Epoch[6/10](6820/14999) || training loss 0.02406 || training accuracy 99.38% || lr 2.845728811291725e-06\n","Epoch[6/10](6840/14999) || training loss 0.1312 || training accuracy 95.62% || lr 2.8446585055856574e-06\n","Epoch[6/10](6860/14999) || training loss 0.09647 || training accuracy 96.88% || lr 2.8435881998795906e-06\n","Epoch[6/10](6880/14999) || training loss 0.05561 || training accuracy 96.88% || lr 2.842517894173523e-06\n","Epoch[6/10](6900/14999) || training loss 0.0778 || training accuracy 97.50% || lr 2.841447588467456e-06\n","Epoch[6/10](6920/14999) || training loss 0.08304 || training accuracy 95.62% || lr 2.8403772827613885e-06\n","Epoch[6/10](6940/14999) || training loss 0.07935 || training accuracy 96.25% || lr 2.8393069770553217e-06\n","Epoch[6/10](6960/14999) || training loss 0.0917 || training accuracy 98.12% || lr 2.838236671349254e-06\n","Epoch[6/10](6980/14999) || training loss 0.05813 || training accuracy 97.50% || lr 2.8371663656431864e-06\n","Epoch[6/10](7000/14999) || training loss 0.03006 || training accuracy 98.75% || lr 2.8360960599371196e-06\n","Epoch[6/10](7020/14999) || training loss 0.132 || training accuracy 94.38% || lr 2.835025754231052e-06\n","Epoch[6/10](7040/14999) || training loss 0.1222 || training accuracy 94.38% || lr 2.833955448524985e-06\n","Epoch[6/10](7060/14999) || training loss 0.09912 || training accuracy 96.88% || lr 2.8328851428189175e-06\n","Epoch[6/10](7080/14999) || training loss 0.05138 || training accuracy 98.12% || lr 2.8318148371128503e-06\n","Epoch[6/10](7100/14999) || training loss 0.03777 || training accuracy 98.75% || lr 2.830744531406783e-06\n","Epoch[6/10](7120/14999) || training loss 0.08778 || training accuracy 96.25% || lr 2.8296742257007154e-06\n","Epoch[6/10](7140/14999) || training loss 0.06841 || training accuracy 96.88% || lr 2.8286039199946486e-06\n","Epoch[6/10](7160/14999) || training loss 0.09021 || training accuracy 97.50% || lr 2.827533614288581e-06\n","Epoch[6/10](7180/14999) || training loss 0.07048 || training accuracy 98.12% || lr 2.8264633085825137e-06\n","Epoch[6/10](7200/14999) || training loss 0.04598 || training accuracy 98.75% || lr 2.8253930028764465e-06\n","Epoch[6/10](7220/14999) || training loss 0.08286 || training accuracy 96.25% || lr 2.8243226971703793e-06\n","Epoch[6/10](7240/14999) || training loss 0.08245 || training accuracy 96.88% || lr 2.823252391464312e-06\n","Epoch[6/10](7260/14999) || training loss 0.05968 || training accuracy 97.50% || lr 2.8221820857582444e-06\n","Epoch[6/10](7280/14999) || training loss 0.1019 || training accuracy 96.88% || lr 2.821111780052177e-06\n","Epoch[6/10](7300/14999) || training loss 0.105 || training accuracy 96.25% || lr 2.82004147434611e-06\n","Epoch[6/10](7320/14999) || training loss 0.03502 || training accuracy 99.38% || lr 2.8189711686400427e-06\n","Epoch[6/10](7340/14999) || training loss 0.06275 || training accuracy 98.75% || lr 2.8179008629339755e-06\n","Epoch[6/10](7360/14999) || training loss 0.1498 || training accuracy 96.25% || lr 2.8168305572279083e-06\n","Epoch[6/10](7380/14999) || training loss 0.05346 || training accuracy 97.50% || lr 2.8157602515218406e-06\n","Epoch[6/10](7400/14999) || training loss 0.07009 || training accuracy 97.50% || lr 2.8146899458157734e-06\n","Epoch[6/10](7420/14999) || training loss 0.03982 || training accuracy 98.75% || lr 2.813619640109706e-06\n","Epoch[6/10](7440/14999) || training loss 0.05189 || training accuracy 97.50% || lr 2.812549334403639e-06\n","Epoch[6/10](7460/14999) || training loss 0.06001 || training accuracy 98.12% || lr 2.8114790286975717e-06\n","Epoch[6/10](7480/14999) || training loss 0.07247 || training accuracy 97.50% || lr 2.810408722991504e-06\n","Epoch[6/10](7500/14999) || training loss 0.07233 || training accuracy 96.88% || lr 2.8093384172854372e-06\n","Epoch[6/10](7520/14999) || training loss 0.1146 || training accuracy 98.12% || lr 2.8082681115793696e-06\n","Epoch[6/10](7540/14999) || training loss 0.09399 || training accuracy 98.12% || lr 2.8071978058733024e-06\n","Epoch[6/10](7560/14999) || training loss 0.07486 || training accuracy 97.50% || lr 2.806127500167235e-06\n","Epoch[6/10](7580/14999) || training loss 0.0431 || training accuracy 98.75% || lr 2.805057194461168e-06\n","Epoch[6/10](7600/14999) || training loss 0.02585 || training accuracy 99.38% || lr 2.8039868887551007e-06\n","Epoch[6/10](7620/14999) || training loss 0.03363 || training accuracy 98.75% || lr 2.802916583049033e-06\n","Epoch[6/10](7640/14999) || training loss 0.09878 || training accuracy 96.25% || lr 2.8018462773429662e-06\n","Epoch[6/10](7660/14999) || training loss 0.1051 || training accuracy 98.12% || lr 2.8007759716368986e-06\n","Epoch[6/10](7680/14999) || training loss 0.06274 || training accuracy 98.75% || lr 2.7997056659308314e-06\n","Epoch[6/10](7700/14999) || training loss 0.05704 || training accuracy 96.25% || lr 2.798635360224764e-06\n","Epoch[6/10](7720/14999) || training loss 0.05839 || training accuracy 99.38% || lr 2.7975650545186965e-06\n","Epoch[6/10](7740/14999) || training loss 0.03893 || training accuracy 98.12% || lr 2.7964947488126297e-06\n","Epoch[6/10](7760/14999) || training loss 0.08733 || training accuracy 96.88% || lr 2.795424443106562e-06\n","Epoch[6/10](7780/14999) || training loss 0.1125 || training accuracy 94.38% || lr 2.7943541374004952e-06\n","Epoch[6/10](7800/14999) || training loss 0.08817 || training accuracy 97.50% || lr 2.7932838316944276e-06\n","Epoch[6/10](7820/14999) || training loss 0.04859 || training accuracy 97.50% || lr 2.79221352598836e-06\n","Epoch[6/10](7840/14999) || training loss 0.05858 || training accuracy 96.88% || lr 2.791143220282293e-06\n","Epoch[6/10](7860/14999) || training loss 0.0796 || training accuracy 98.12% || lr 2.7900729145762255e-06\n","Epoch[6/10](7880/14999) || training loss 0.08479 || training accuracy 96.25% || lr 2.7890026088701587e-06\n","Epoch[6/10](7900/14999) || training loss 0.1221 || training accuracy 96.25% || lr 2.787932303164091e-06\n","Epoch[6/10](7920/14999) || training loss 0.04227 || training accuracy 98.75% || lr 2.786861997458024e-06\n","Epoch[6/10](7940/14999) || training loss 0.105 || training accuracy 95.62% || lr 2.7857916917519566e-06\n","Epoch[6/10](7960/14999) || training loss 0.04911 || training accuracy 98.12% || lr 2.784721386045889e-06\n","Epoch[6/10](7980/14999) || training loss 0.07712 || training accuracy 96.25% || lr 2.783651080339822e-06\n","Epoch[6/10](8000/14999) || training loss 0.04398 || training accuracy 98.12% || lr 2.7825807746337545e-06\n","Epoch[6/10](8020/14999) || training loss 0.07009 || training accuracy 98.75% || lr 2.7815104689276872e-06\n","Epoch[6/10](8040/14999) || training loss 0.08917 || training accuracy 97.50% || lr 2.78044016322162e-06\n","Epoch[6/10](8060/14999) || training loss 0.08265 || training accuracy 97.50% || lr 2.779369857515553e-06\n","Epoch[6/10](8080/14999) || training loss 0.06146 || training accuracy 96.88% || lr 2.7782995518094856e-06\n","Epoch[6/10](8100/14999) || training loss 0.03054 || training accuracy 99.38% || lr 2.7772292461034183e-06\n","Epoch[6/10](8120/14999) || training loss 0.06838 || training accuracy 96.25% || lr 2.7761589403973507e-06\n","Epoch[6/10](8140/14999) || training loss 0.09855 || training accuracy 95.62% || lr 2.7750886346912835e-06\n","Epoch[6/10](8160/14999) || training loss 0.04492 || training accuracy 98.12% || lr 2.7740183289852162e-06\n","Epoch[6/10](8180/14999) || training loss 0.07644 || training accuracy 96.25% || lr 2.772948023279149e-06\n","Epoch[6/10](8200/14999) || training loss 0.04555 || training accuracy 98.12% || lr 2.7718777175730818e-06\n","Epoch[6/10](8220/14999) || training loss 0.08107 || training accuracy 97.50% || lr 2.7708074118670146e-06\n","Epoch[6/10](8240/14999) || training loss 0.06674 || training accuracy 98.75% || lr 2.7697371061609473e-06\n","Epoch[6/10](8260/14999) || training loss 0.07715 || training accuracy 98.12% || lr 2.7686668004548797e-06\n","Epoch[6/10](8280/14999) || training loss 0.1046 || training accuracy 97.50% || lr 2.7675964947488125e-06\n","Epoch[6/10](8300/14999) || training loss 0.05346 || training accuracy 97.50% || lr 2.7665261890427452e-06\n","Epoch[6/10](8320/14999) || training loss 0.1098 || training accuracy 98.12% || lr 2.765455883336678e-06\n","Epoch[6/10](8340/14999) || training loss 0.1244 || training accuracy 95.00% || lr 2.7643855776306108e-06\n","Epoch[6/10](8360/14999) || training loss 0.08887 || training accuracy 97.50% || lr 2.763315271924543e-06\n","Epoch[6/10](8380/14999) || training loss 0.04845 || training accuracy 98.75% || lr 2.7622449662184763e-06\n","Epoch[6/10](8400/14999) || training loss 0.06653 || training accuracy 97.50% || lr 2.7611746605124087e-06\n","Epoch[6/10](8420/14999) || training loss 0.0774 || training accuracy 98.12% || lr 2.7601043548063414e-06\n","Epoch[6/10](8440/14999) || training loss 0.07098 || training accuracy 97.50% || lr 2.7590340491002742e-06\n","Epoch[6/10](8460/14999) || training loss 0.05967 || training accuracy 98.12% || lr 2.7579637433942066e-06\n","Epoch[6/10](8480/14999) || training loss 0.07559 || training accuracy 97.50% || lr 2.7568934376881398e-06\n","Epoch[6/10](8500/14999) || training loss 0.1108 || training accuracy 95.00% || lr 2.755823131982072e-06\n","Epoch[6/10](8520/14999) || training loss 0.07781 || training accuracy 98.12% || lr 2.7547528262760053e-06\n","Epoch[6/10](8540/14999) || training loss 0.05605 || training accuracy 98.75% || lr 2.7536825205699377e-06\n","Epoch[6/10](8560/14999) || training loss 0.0563 || training accuracy 96.25% || lr 2.75261221486387e-06\n","Epoch[6/10](8580/14999) || training loss 0.05952 || training accuracy 98.12% || lr 2.7515419091578032e-06\n","Epoch[6/10](8600/14999) || training loss 0.03798 || training accuracy 98.12% || lr 2.7504716034517356e-06\n","Epoch[6/10](8620/14999) || training loss 0.07304 || training accuracy 98.12% || lr 2.7494012977456688e-06\n","Epoch[6/10](8640/14999) || training loss 0.1323 || training accuracy 96.88% || lr 2.748330992039601e-06\n","Epoch[6/10](8660/14999) || training loss 0.08563 || training accuracy 95.62% || lr 2.747260686333534e-06\n","Epoch[6/10](8680/14999) || training loss 0.06181 || training accuracy 97.50% || lr 2.7461903806274667e-06\n","Epoch[6/10](8700/14999) || training loss 0.07225 || training accuracy 96.88% || lr 2.745120074921399e-06\n","Epoch[6/10](8720/14999) || training loss 0.01645 || training accuracy 100.00% || lr 2.744049769215332e-06\n","Epoch[6/10](8740/14999) || training loss 0.114 || training accuracy 97.50% || lr 2.7429794635092646e-06\n","Epoch[6/10](8760/14999) || training loss 0.04953 || training accuracy 97.50% || lr 2.7419091578031973e-06\n","Epoch[6/10](8780/14999) || training loss 0.04045 || training accuracy 98.75% || lr 2.74083885209713e-06\n","Epoch[6/10](8800/14999) || training loss 0.07723 || training accuracy 96.88% || lr 2.739768546391063e-06\n","Epoch[6/10](8820/14999) || training loss 0.05288 || training accuracy 99.38% || lr 2.7386982406849957e-06\n","Epoch[6/10](8840/14999) || training loss 0.1224 || training accuracy 96.88% || lr 2.737627934978928e-06\n","Epoch[6/10](8860/14999) || training loss 0.07041 || training accuracy 96.88% || lr 2.736557629272861e-06\n","Epoch[6/10](8880/14999) || training loss 0.1148 || training accuracy 95.62% || lr 2.7354873235667935e-06\n","Epoch[6/10](8900/14999) || training loss 0.0779 || training accuracy 96.25% || lr 2.7344170178607263e-06\n","Epoch[6/10](8920/14999) || training loss 0.02184 || training accuracy 99.38% || lr 2.733346712154659e-06\n","Epoch[6/10](8940/14999) || training loss 0.04753 || training accuracy 99.38% || lr 2.732276406448592e-06\n","Epoch[6/10](8960/14999) || training loss 0.09054 || training accuracy 96.25% || lr 2.7312061007425246e-06\n","Epoch[6/10](8980/14999) || training loss 0.05011 || training accuracy 98.12% || lr 2.730135795036457e-06\n","Epoch[6/10](9000/14999) || training loss 0.09766 || training accuracy 97.50% || lr 2.7290654893303898e-06\n","Epoch[6/10](9020/14999) || training loss 0.05419 || training accuracy 99.38% || lr 2.7279951836243225e-06\n","Epoch[6/10](9040/14999) || training loss 0.05481 || training accuracy 97.50% || lr 2.7269248779182553e-06\n","Epoch[6/10](9060/14999) || training loss 0.04365 || training accuracy 98.75% || lr 2.725854572212188e-06\n","Epoch[6/10](9080/14999) || training loss 0.0724 || training accuracy 98.12% || lr 2.724784266506121e-06\n","Epoch[6/10](9100/14999) || training loss 0.09978 || training accuracy 96.25% || lr 2.723713960800053e-06\n","Epoch[6/10](9120/14999) || training loss 0.08048 || training accuracy 97.50% || lr 2.722643655093986e-06\n","Epoch[6/10](9140/14999) || training loss 0.07268 || training accuracy 96.25% || lr 2.7215733493879188e-06\n","Epoch[6/10](9160/14999) || training loss 0.1095 || training accuracy 95.62% || lr 2.7205030436818515e-06\n","Epoch[6/10](9180/14999) || training loss 0.03546 || training accuracy 98.75% || lr 2.7194327379757843e-06\n","Epoch[6/10](9200/14999) || training loss 0.07147 || training accuracy 97.50% || lr 2.7183624322697167e-06\n","Epoch[6/10](9220/14999) || training loss 0.03808 || training accuracy 98.75% || lr 2.71729212656365e-06\n","Epoch[6/10](9240/14999) || training loss 0.134 || training accuracy 97.50% || lr 2.716221820857582e-06\n","Epoch[6/10](9260/14999) || training loss 0.05168 || training accuracy 98.12% || lr 2.7151515151515154e-06\n","Epoch[6/10](9280/14999) || training loss 0.09682 || training accuracy 96.88% || lr 2.7140812094454478e-06\n","Epoch[6/10](9300/14999) || training loss 0.07349 || training accuracy 96.88% || lr 2.71301090373938e-06\n","Epoch[6/10](9320/14999) || training loss 0.07009 || training accuracy 97.50% || lr 2.7119405980333133e-06\n","Epoch[6/10](9340/14999) || training loss 0.09558 || training accuracy 98.12% || lr 2.7108702923272456e-06\n","Epoch[6/10](9360/14999) || training loss 0.08143 || training accuracy 98.12% || lr 2.709799986621179e-06\n","Epoch[6/10](9380/14999) || training loss 0.08329 || training accuracy 96.88% || lr 2.708729680915111e-06\n","Epoch[6/10](9400/14999) || training loss 0.08623 || training accuracy 96.25% || lr 2.7076593752090435e-06\n","Epoch[6/10](9420/14999) || training loss 0.08237 || training accuracy 96.88% || lr 2.7065890695029767e-06\n","Epoch[6/10](9440/14999) || training loss 0.08641 || training accuracy 96.25% || lr 2.705518763796909e-06\n","Epoch[6/10](9460/14999) || training loss 0.05387 || training accuracy 98.12% || lr 2.7044484580908423e-06\n","Epoch[6/10](9480/14999) || training loss 0.0483 || training accuracy 98.12% || lr 2.7033781523847746e-06\n","Epoch[6/10](9500/14999) || training loss 0.09581 || training accuracy 95.62% || lr 2.702307846678708e-06\n","Epoch[6/10](9520/14999) || training loss 0.1306 || training accuracy 95.00% || lr 2.70123754097264e-06\n","Epoch[6/10](9540/14999) || training loss 0.05261 || training accuracy 97.50% || lr 2.700167235266573e-06\n","Epoch[6/10](9560/14999) || training loss 0.02304 || training accuracy 100.00% || lr 2.6990969295605057e-06\n","Epoch[6/10](9580/14999) || training loss 0.05572 || training accuracy 98.12% || lr 2.698026623854438e-06\n","Epoch[6/10](9600/14999) || training loss 0.04009 || training accuracy 98.75% || lr 2.6969563181483713e-06\n","Epoch[6/10](9620/14999) || training loss 0.09509 || training accuracy 97.50% || lr 2.6958860124423036e-06\n","Epoch[6/10](9640/14999) || training loss 0.04469 || training accuracy 98.12% || lr 2.6948157067362364e-06\n","Epoch[6/10](9660/14999) || training loss 0.1173 || training accuracy 94.38% || lr 2.693745401030169e-06\n","Epoch[6/10](9680/14999) || training loss 0.07534 || training accuracy 97.50% || lr 2.692675095324102e-06\n","Epoch[6/10](9700/14999) || training loss 0.09303 || training accuracy 96.88% || lr 2.6916047896180347e-06\n","Epoch[6/10](9720/14999) || training loss 0.1491 || training accuracy 95.00% || lr 2.690534483911967e-06\n","Epoch[6/10](9740/14999) || training loss 0.1334 || training accuracy 95.00% || lr 2.6894641782059e-06\n","Epoch[6/10](9760/14999) || training loss 0.06847 || training accuracy 96.88% || lr 2.6883938724998326e-06\n","Epoch[6/10](9780/14999) || training loss 0.08137 || training accuracy 96.25% || lr 2.6873235667937654e-06\n","Epoch[6/10](9800/14999) || training loss 0.1129 || training accuracy 96.88% || lr 2.686253261087698e-06\n","Epoch[6/10](9820/14999) || training loss 0.1433 || training accuracy 95.62% || lr 2.685182955381631e-06\n","Epoch[6/10](9840/14999) || training loss 0.147 || training accuracy 95.62% || lr 2.6841126496755633e-06\n","Epoch[6/10](9860/14999) || training loss 0.09141 || training accuracy 97.50% || lr 2.683042343969496e-06\n","Epoch[6/10](9880/14999) || training loss 0.09236 || training accuracy 96.25% || lr 2.681972038263429e-06\n","Epoch[6/10](9900/14999) || training loss 0.07364 || training accuracy 96.88% || lr 2.6809017325573616e-06\n","Epoch[6/10](9920/14999) || training loss 0.05957 || training accuracy 98.12% || lr 2.6798314268512944e-06\n","Epoch[6/10](9940/14999) || training loss 0.04911 || training accuracy 98.12% || lr 2.6787611211452267e-06\n","Epoch[6/10](9960/14999) || training loss 0.02394 || training accuracy 100.00% || lr 2.67769081543916e-06\n","Epoch[6/10](9980/14999) || training loss 0.1036 || training accuracy 95.62% || lr 2.6766205097330923e-06\n","Epoch[6/10](10000/14999) || training loss 0.08773 || training accuracy 96.88% || lr 2.675550204027025e-06\n","Epoch[6/10](10020/14999) || training loss 0.04543 || training accuracy 98.12% || lr 2.674479898320958e-06\n","Epoch[6/10](10040/14999) || training loss 0.05015 || training accuracy 98.12% || lr 2.67340959261489e-06\n","Epoch[6/10](10060/14999) || training loss 0.0859 || training accuracy 97.50% || lr 2.6723392869088234e-06\n","Epoch[6/10](10080/14999) || training loss 0.03975 || training accuracy 98.75% || lr 2.6712689812027557e-06\n","Epoch[6/10](10100/14999) || training loss 0.06172 || training accuracy 96.88% || lr 2.670198675496689e-06\n","Epoch[6/10](10120/14999) || training loss 0.07175 || training accuracy 96.25% || lr 2.6691283697906213e-06\n","Epoch[6/10](10140/14999) || training loss 0.09583 || training accuracy 96.88% || lr 2.668058064084554e-06\n","Epoch[6/10](10160/14999) || training loss 0.06316 || training accuracy 97.50% || lr 2.666987758378487e-06\n","Epoch[6/10](10180/14999) || training loss 0.03527 || training accuracy 99.38% || lr 2.665917452672419e-06\n","Epoch[6/10](10200/14999) || training loss 0.06311 || training accuracy 98.12% || lr 2.6648471469663524e-06\n","Epoch[6/10](10220/14999) || training loss 0.03519 || training accuracy 99.38% || lr 2.6637768412602847e-06\n","Epoch[6/10](10240/14999) || training loss 0.0943 || training accuracy 97.50% || lr 2.662706535554218e-06\n","Epoch[6/10](10260/14999) || training loss 0.07112 || training accuracy 98.12% || lr 2.6616362298481503e-06\n","Epoch[6/10](10280/14999) || training loss 0.0441 || training accuracy 98.75% || lr 2.6605659241420826e-06\n","Epoch[6/10](10300/14999) || training loss 0.04107 || training accuracy 98.12% || lr 2.659495618436016e-06\n","Epoch[6/10](10320/14999) || training loss 0.07119 || training accuracy 97.50% || lr 2.658425312729948e-06\n","Epoch[6/10](10340/14999) || training loss 0.03831 || training accuracy 98.75% || lr 2.6573550070238814e-06\n","Epoch[6/10](10360/14999) || training loss 0.05061 || training accuracy 98.75% || lr 2.6562847013178137e-06\n","Epoch[6/10](10380/14999) || training loss 0.08021 || training accuracy 97.50% || lr 2.6552143956117465e-06\n","Epoch[6/10](10400/14999) || training loss 0.0482 || training accuracy 98.12% || lr 2.6541440899056793e-06\n","Epoch[6/10](10420/14999) || training loss 0.1195 || training accuracy 96.88% || lr 2.6530737841996116e-06\n","Epoch[6/10](10440/14999) || training loss 0.08895 || training accuracy 95.00% || lr 2.652003478493545e-06\n","Epoch[6/10](10460/14999) || training loss 0.02644 || training accuracy 98.75% || lr 2.650933172787477e-06\n","Epoch[6/10](10480/14999) || training loss 0.07484 || training accuracy 96.25% || lr 2.64986286708141e-06\n","Epoch[6/10](10500/14999) || training loss 0.03821 || training accuracy 98.75% || lr 2.6487925613753427e-06\n","Epoch[6/10](10520/14999) || training loss 0.07911 || training accuracy 97.50% || lr 2.6477222556692755e-06\n","Epoch[6/10](10540/14999) || training loss 0.01762 || training accuracy 99.38% || lr 2.6466519499632083e-06\n","Epoch[6/10](10560/14999) || training loss 0.08254 || training accuracy 97.50% || lr 2.6455816442571406e-06\n","Epoch[6/10](10580/14999) || training loss 0.09206 || training accuracy 98.75% || lr 2.6445113385510734e-06\n","Epoch[6/10](10600/14999) || training loss 0.07406 || training accuracy 96.25% || lr 2.643441032845006e-06\n","Epoch[6/10](10620/14999) || training loss 0.04382 || training accuracy 98.12% || lr 2.642370727138939e-06\n","Epoch[6/10](10640/14999) || training loss 0.06764 || training accuracy 98.12% || lr 2.6413004214328717e-06\n","Epoch[6/10](10660/14999) || training loss 0.05382 || training accuracy 98.12% || lr 2.6402301157268045e-06\n","Epoch[6/10](10680/14999) || training loss 0.1179 || training accuracy 96.25% || lr 2.639159810020737e-06\n","Epoch[6/10](10700/14999) || training loss 0.04735 || training accuracy 97.50% || lr 2.63808950431467e-06\n","Epoch[6/10](10720/14999) || training loss 0.06244 || training accuracy 97.50% || lr 2.6370191986086024e-06\n","Epoch[6/10](10740/14999) || training loss 0.05618 || training accuracy 98.75% || lr 2.635948892902535e-06\n","Epoch[6/10](10760/14999) || training loss 0.06208 || training accuracy 96.88% || lr 2.634878587196468e-06\n","Epoch[6/10](10780/14999) || training loss 0.04685 || training accuracy 97.50% || lr 2.6338082814904007e-06\n","Epoch[6/10](10800/14999) || training loss 0.1784 || training accuracy 95.62% || lr 2.6327379757843335e-06\n","Epoch[6/10](10820/14999) || training loss 0.1064 || training accuracy 98.12% || lr 2.631667670078266e-06\n","Epoch[6/10](10840/14999) || training loss 0.09528 || training accuracy 96.25% || lr 2.630597364372199e-06\n","Epoch[6/10](10860/14999) || training loss 0.04466 || training accuracy 98.75% || lr 2.6295270586661314e-06\n","Epoch[6/10](10880/14999) || training loss 0.0752 || training accuracy 96.88% || lr 2.628456752960064e-06\n","Epoch[6/10](10900/14999) || training loss 0.1402 || training accuracy 95.00% || lr 2.627386447253997e-06\n","Epoch[6/10](10920/14999) || training loss 0.09854 || training accuracy 96.88% || lr 2.6263161415479293e-06\n","Epoch[6/10](10940/14999) || training loss 0.1101 || training accuracy 95.00% || lr 2.6252458358418625e-06\n","Epoch[6/10](10960/14999) || training loss 0.05172 || training accuracy 98.12% || lr 2.624175530135795e-06\n","Epoch[6/10](10980/14999) || training loss 0.05392 || training accuracy 97.50% || lr 2.623105224429728e-06\n","Epoch[6/10](11000/14999) || training loss 0.1177 || training accuracy 96.25% || lr 2.6220349187236604e-06\n","Epoch[6/10](11020/14999) || training loss 0.1242 || training accuracy 95.62% || lr 2.6209646130175927e-06\n","Epoch[6/10](11040/14999) || training loss 0.08103 || training accuracy 98.12% || lr 2.619894307311526e-06\n","Epoch[6/10](11060/14999) || training loss 0.07543 || training accuracy 97.50% || lr 2.6188240016054583e-06\n","Epoch[6/10](11080/14999) || training loss 0.04593 || training accuracy 98.75% || lr 2.6177536958993915e-06\n","Epoch[6/10](11100/14999) || training loss 0.0512 || training accuracy 98.12% || lr 2.616683390193324e-06\n","Epoch[6/10](11120/14999) || training loss 0.05197 || training accuracy 98.75% || lr 2.6156130844872566e-06\n","Epoch[6/10](11140/14999) || training loss 0.0958 || training accuracy 95.62% || lr 2.6145427787811893e-06\n","Epoch[6/10](11160/14999) || training loss 0.08859 || training accuracy 97.50% || lr 2.6134724730751217e-06\n","Epoch[6/10](11180/14999) || training loss 0.065 || training accuracy 98.12% || lr 2.612402167369055e-06\n","Epoch[6/10](11200/14999) || training loss 0.05116 || training accuracy 97.50% || lr 2.6113318616629872e-06\n","Epoch[6/10](11220/14999) || training loss 0.02392 || training accuracy 99.38% || lr 2.61026155595692e-06\n","Epoch[6/10](11240/14999) || training loss 0.03471 || training accuracy 98.75% || lr 2.609191250250853e-06\n","Epoch[6/10](11260/14999) || training loss 0.0381 || training accuracy 98.12% || lr 2.6081209445447856e-06\n","Epoch[6/10](11280/14999) || training loss 0.1123 || training accuracy 95.62% || lr 2.6070506388387183e-06\n","Epoch[6/10](11300/14999) || training loss 0.04925 || training accuracy 98.75% || lr 2.6059803331326507e-06\n","Epoch[6/10](11320/14999) || training loss 0.06887 || training accuracy 96.88% || lr 2.6049100274265835e-06\n","Epoch[6/10](11340/14999) || training loss 0.07716 || training accuracy 96.25% || lr 2.6038397217205162e-06\n","Epoch[6/10](11360/14999) || training loss 0.0658 || training accuracy 96.88% || lr 2.602769416014449e-06\n","Epoch[6/10](11380/14999) || training loss 0.05224 || training accuracy 98.75% || lr 2.6016991103083818e-06\n","Epoch[6/10](11400/14999) || training loss 0.05623 || training accuracy 97.50% || lr 2.6006288046023146e-06\n","Epoch[6/10](11420/14999) || training loss 0.01797 || training accuracy 100.00% || lr 2.5995584988962473e-06\n","Epoch[6/10](11440/14999) || training loss 0.04732 || training accuracy 98.12% || lr 2.5984881931901797e-06\n","Epoch[6/10](11460/14999) || training loss 0.08347 || training accuracy 95.00% || lr 2.5974178874841125e-06\n","Epoch[6/10](11480/14999) || training loss 0.04379 || training accuracy 98.12% || lr 2.5963475817780452e-06\n","Epoch[6/10](11500/14999) || training loss 0.1688 || training accuracy 94.38% || lr 2.595277276071978e-06\n","Epoch[6/10](11520/14999) || training loss 0.03881 || training accuracy 98.75% || lr 2.5942069703659108e-06\n","Epoch[6/10](11540/14999) || training loss 0.0752 || training accuracy 95.62% || lr 2.5931366646598435e-06\n","Epoch[6/10](11560/14999) || training loss 0.02436 || training accuracy 99.38% || lr 2.592066358953776e-06\n","Epoch[6/10](11580/14999) || training loss 0.007446 || training accuracy 100.00% || lr 2.5909960532477087e-06\n","Epoch[6/10](11600/14999) || training loss 0.03285 || training accuracy 99.38% || lr 2.5899257475416414e-06\n","Epoch[6/10](11620/14999) || training loss 0.07036 || training accuracy 96.88% || lr 2.5888554418355742e-06\n","Epoch[6/10](11640/14999) || training loss 0.05264 || training accuracy 98.12% || lr 2.587785136129507e-06\n","Epoch[6/10](11660/14999) || training loss 0.05768 || training accuracy 97.50% || lr 2.5867148304234393e-06\n","Epoch[6/10](11680/14999) || training loss 0.07582 || training accuracy 98.12% || lr 2.5856445247173725e-06\n","Epoch[6/10](11700/14999) || training loss 0.04365 || training accuracy 98.75% || lr 2.584574219011305e-06\n","Epoch[6/10](11720/14999) || training loss 0.09788 || training accuracy 96.88% || lr 2.5835039133052377e-06\n","Epoch[6/10](11740/14999) || training loss 0.1161 || training accuracy 95.00% || lr 2.5824336075991704e-06\n","Epoch[6/10](11760/14999) || training loss 0.1306 || training accuracy 94.38% || lr 2.581363301893103e-06\n","Epoch[6/10](11780/14999) || training loss 0.04743 || training accuracy 98.75% || lr 2.580292996187036e-06\n","Epoch[6/10](11800/14999) || training loss 0.09746 || training accuracy 96.25% || lr 2.5792226904809683e-06\n","Epoch[6/10](11820/14999) || training loss 0.07414 || training accuracy 98.75% || lr 2.5781523847749015e-06\n","Epoch[6/10](11840/14999) || training loss 0.1259 || training accuracy 96.25% || lr 2.577082079068834e-06\n","Epoch[6/10](11860/14999) || training loss 0.03301 || training accuracy 98.75% || lr 2.5760117733627662e-06\n","Epoch[6/10](11880/14999) || training loss 0.07975 || training accuracy 95.62% || lr 2.5749414676566994e-06\n","Epoch[6/10](11900/14999) || training loss 0.03123 || training accuracy 99.38% || lr 2.5738711619506318e-06\n","Epoch[6/10](11920/14999) || training loss 0.06354 || training accuracy 98.12% || lr 2.572800856244565e-06\n","Epoch[6/10](11940/14999) || training loss 0.172 || training accuracy 93.75% || lr 2.5717305505384973e-06\n","Epoch[6/10](11960/14999) || training loss 0.06361 || training accuracy 97.50% || lr 2.57066024483243e-06\n","Epoch[6/10](11980/14999) || training loss 0.05811 || training accuracy 96.88% || lr 2.569589939126363e-06\n","Epoch[6/10](12000/14999) || training loss 0.1084 || training accuracy 96.25% || lr 2.5685196334202952e-06\n","Epoch[6/10](12020/14999) || training loss 0.03021 || training accuracy 99.38% || lr 2.5674493277142284e-06\n","Epoch[6/10](12040/14999) || training loss 0.1124 || training accuracy 96.25% || lr 2.5663790220081608e-06\n","Epoch[6/10](12060/14999) || training loss 0.1174 || training accuracy 96.25% || lr 2.565308716302094e-06\n","Epoch[6/10](12080/14999) || training loss 0.08888 || training accuracy 98.12% || lr 2.5642384105960263e-06\n","Epoch[6/10](12100/14999) || training loss 0.07912 || training accuracy 98.12% || lr 2.563168104889959e-06\n","Epoch[6/10](12120/14999) || training loss 0.08536 || training accuracy 96.88% || lr 2.562097799183892e-06\n","Epoch[6/10](12140/14999) || training loss 0.0827 || training accuracy 97.50% || lr 2.5610274934778246e-06\n","Epoch[6/10](12160/14999) || training loss 0.08216 || training accuracy 96.25% || lr 2.5599571877717574e-06\n","Epoch[6/10](12180/14999) || training loss 0.0741 || training accuracy 96.88% || lr 2.5588868820656898e-06\n","Epoch[6/10](12200/14999) || training loss 0.129 || training accuracy 96.88% || lr 2.5578165763596225e-06\n","Epoch[6/10](12220/14999) || training loss 0.04467 || training accuracy 98.75% || lr 2.5567462706535553e-06\n","Epoch[6/10](12240/14999) || training loss 0.1037 || training accuracy 96.25% || lr 2.555675964947488e-06\n","Epoch[6/10](12260/14999) || training loss 0.08682 || training accuracy 97.50% || lr 2.554605659241421e-06\n","Epoch[6/10](12280/14999) || training loss 0.08881 || training accuracy 96.88% || lr 2.5535353535353536e-06\n","Epoch[6/10](12300/14999) || training loss 0.06189 || training accuracy 98.75% || lr 2.552465047829286e-06\n","Epoch[6/10](12320/14999) || training loss 0.06588 || training accuracy 98.12% || lr 2.5513947421232188e-06\n","Epoch[6/10](12340/14999) || training loss 0.02229 || training accuracy 99.38% || lr 2.5503244364171515e-06\n","Epoch[6/10](12360/14999) || training loss 0.0934 || training accuracy 95.62% || lr 2.5492541307110843e-06\n","Epoch[6/10](12380/14999) || training loss 0.04222 || training accuracy 98.75% || lr 2.548183825005017e-06\n","Epoch[6/10](12400/14999) || training loss 0.07608 || training accuracy 96.88% || lr 2.5471135192989494e-06\n","Epoch[6/10](12420/14999) || training loss 0.06219 || training accuracy 98.12% || lr 2.5460432135928826e-06\n","Epoch[6/10](12440/14999) || training loss 0.06599 || training accuracy 98.12% || lr 2.544972907886815e-06\n","Epoch[6/10](12460/14999) || training loss 0.04836 || training accuracy 98.12% || lr 2.5439026021807477e-06\n","Epoch[6/10](12480/14999) || training loss 0.1208 || training accuracy 96.88% || lr 2.5428322964746805e-06\n","Epoch[6/10](12500/14999) || training loss 0.03568 || training accuracy 98.75% || lr 2.541761990768613e-06\n","Epoch[6/10](12520/14999) || training loss 0.07393 || training accuracy 96.25% || lr 2.540691685062546e-06\n","Epoch[6/10](12540/14999) || training loss 0.03179 || training accuracy 99.38% || lr 2.5396213793564784e-06\n","Epoch[6/10](12560/14999) || training loss 0.078 || training accuracy 98.12% || lr 2.5385510736504116e-06\n","Epoch[6/10](12580/14999) || training loss 0.1191 || training accuracy 96.88% || lr 2.537480767944344e-06\n","Epoch[6/10](12600/14999) || training loss 0.07659 || training accuracy 98.12% || lr 2.5364104622382763e-06\n","Epoch[6/10](12620/14999) || training loss 0.0439 || training accuracy 98.75% || lr 2.5353401565322095e-06\n","Epoch[6/10](12640/14999) || training loss 0.1523 || training accuracy 95.00% || lr 2.534269850826142e-06\n","Epoch[6/10](12660/14999) || training loss 0.1024 || training accuracy 96.25% || lr 2.533199545120075e-06\n","Epoch[6/10](12680/14999) || training loss 0.07552 || training accuracy 98.12% || lr 2.5321292394140074e-06\n","Epoch[6/10](12700/14999) || training loss 0.06219 || training accuracy 97.50% || lr 2.5310589337079406e-06\n","Epoch[6/10](12720/14999) || training loss 0.06853 || training accuracy 96.88% || lr 2.529988628001873e-06\n","Epoch[6/10](12740/14999) || training loss 0.03903 || training accuracy 100.00% || lr 2.5289183222958053e-06\n","Epoch[6/10](12760/14999) || training loss 0.02301 || training accuracy 99.38% || lr 2.5278480165897385e-06\n","Epoch[6/10](12780/14999) || training loss 0.03677 || training accuracy 98.75% || lr 2.526777710883671e-06\n","Epoch[6/10](12800/14999) || training loss 0.07059 || training accuracy 96.88% || lr 2.525707405177604e-06\n","Epoch[6/10](12820/14999) || training loss 0.0479 || training accuracy 98.12% || lr 2.5246370994715364e-06\n","Epoch[6/10](12840/14999) || training loss 0.05118 || training accuracy 97.50% || lr 2.523566793765469e-06\n","Epoch[6/10](12860/14999) || training loss 0.1814 || training accuracy 94.38% || lr 2.522496488059402e-06\n","Epoch[6/10](12880/14999) || training loss 0.09855 || training accuracy 96.25% || lr 2.5214261823533343e-06\n","Epoch[6/10](12900/14999) || training loss 0.0746 || training accuracy 95.00% || lr 2.5203558766472675e-06\n","Epoch[6/10](12920/14999) || training loss 0.1144 || training accuracy 95.00% || lr 2.5192855709412e-06\n","Epoch[6/10](12940/14999) || training loss 0.06067 || training accuracy 96.88% || lr 2.5182152652351326e-06\n","Epoch[6/10](12960/14999) || training loss 0.1074 || training accuracy 96.88% || lr 2.5171449595290654e-06\n","Epoch[6/10](12980/14999) || training loss 0.02092 || training accuracy 99.38% || lr 2.516074653822998e-06\n","Epoch[6/10](13000/14999) || training loss 0.08846 || training accuracy 96.88% || lr 2.515004348116931e-06\n","Epoch[6/10](13020/14999) || training loss 0.0815 || training accuracy 97.50% || lr 2.5139340424108633e-06\n","Epoch[6/10](13040/14999) || training loss 0.05153 || training accuracy 98.75% || lr 2.512863736704796e-06\n","Epoch[6/10](13060/14999) || training loss 0.03482 || training accuracy 99.38% || lr 2.511793430998729e-06\n","Epoch[6/10](13080/14999) || training loss 0.03796 || training accuracy 98.75% || lr 2.5107231252926616e-06\n","Epoch[6/10](13100/14999) || training loss 0.05301 || training accuracy 97.50% || lr 2.5096528195865944e-06\n","Epoch[6/10](13120/14999) || training loss 0.04132 || training accuracy 97.50% || lr 2.508582513880527e-06\n","Epoch[6/10](13140/14999) || training loss 0.1253 || training accuracy 96.88% || lr 2.5075122081744595e-06\n","Epoch[6/10](13160/14999) || training loss 0.05439 || training accuracy 98.75% || lr 2.5064419024683923e-06\n","Epoch[6/10](13180/14999) || training loss 0.0659 || training accuracy 97.50% || lr 2.505371596762325e-06\n","Epoch[6/10](13200/14999) || training loss 0.06099 || training accuracy 98.75% || lr 2.504301291056258e-06\n","Epoch[6/10](13220/14999) || training loss 0.04593 || training accuracy 98.75% || lr 2.5032309853501906e-06\n","Epoch[6/10](13240/14999) || training loss 0.1115 || training accuracy 95.62% || lr 2.502160679644123e-06\n","Epoch[6/10](13260/14999) || training loss 0.06865 || training accuracy 96.88% || lr 2.501090373938056e-06\n","Epoch[6/10](13280/14999) || training loss 0.08803 || training accuracy 96.25% || lr 2.5000200682319885e-06\n","Epoch[6/10](13300/14999) || training loss 0.06978 || training accuracy 98.75% || lr 2.4989497625259217e-06\n","Epoch[6/10](13320/14999) || training loss 0.09032 || training accuracy 96.88% || lr 2.497879456819854e-06\n","Epoch[6/10](13340/14999) || training loss 0.1099 || training accuracy 96.25% || lr 2.496809151113787e-06\n","Epoch[6/10](13360/14999) || training loss 0.09262 || training accuracy 97.50% || lr 2.4957388454077196e-06\n","Epoch[6/10](13380/14999) || training loss 0.08529 || training accuracy 96.25% || lr 2.494668539701652e-06\n","Epoch[6/10](13400/14999) || training loss 0.07893 || training accuracy 96.25% || lr 2.493598233995585e-06\n","Epoch[6/10](13420/14999) || training loss 0.05989 || training accuracy 98.12% || lr 2.4925279282895175e-06\n","Epoch[6/10](13440/14999) || training loss 0.143 || training accuracy 95.00% || lr 2.4914576225834507e-06\n","Epoch[6/10](13460/14999) || training loss 0.04498 || training accuracy 98.12% || lr 2.490387316877383e-06\n","Epoch[6/10](13480/14999) || training loss 0.08375 || training accuracy 96.88% || lr 2.4893170111713154e-06\n","Epoch[6/10](13500/14999) || training loss 0.09026 || training accuracy 95.62% || lr 2.4882467054652486e-06\n","Epoch[6/10](13520/14999) || training loss 0.05165 || training accuracy 97.50% || lr 2.487176399759181e-06\n","Epoch[6/10](13540/14999) || training loss 0.06063 || training accuracy 98.12% || lr 2.486106094053114e-06\n","Epoch[6/10](13560/14999) || training loss 0.1103 || training accuracy 97.50% || lr 2.4850357883470465e-06\n","Epoch[6/10](13580/14999) || training loss 0.05952 || training accuracy 98.12% || lr 2.483965482640979e-06\n","Epoch[6/10](13600/14999) || training loss 0.13 || training accuracy 96.25% || lr 2.482895176934912e-06\n","Epoch[6/10](13620/14999) || training loss 0.03472 || training accuracy 99.38% || lr 2.4818248712288444e-06\n","Epoch[6/10](13640/14999) || training loss 0.05795 || training accuracy 98.75% || lr 2.4807545655227776e-06\n","Epoch[6/10](13660/14999) || training loss 0.07787 || training accuracy 97.50% || lr 2.47968425981671e-06\n","Epoch[6/10](13680/14999) || training loss 0.07926 || training accuracy 98.75% || lr 2.4786139541106427e-06\n","Epoch[6/10](13700/14999) || training loss 0.05452 || training accuracy 98.12% || lr 2.4775436484045755e-06\n","Epoch[6/10](13720/14999) || training loss 0.1078 || training accuracy 96.25% || lr 2.4764733426985083e-06\n","Epoch[6/10](13740/14999) || training loss 0.0583 || training accuracy 98.12% || lr 2.475403036992441e-06\n","Epoch[6/10](13760/14999) || training loss 0.1025 || training accuracy 96.25% || lr 2.4743327312863734e-06\n","Epoch[6/10](13780/14999) || training loss 0.03285 || training accuracy 99.38% || lr 2.473262425580306e-06\n","Epoch[6/10](13800/14999) || training loss 0.03223 || training accuracy 99.38% || lr 2.472192119874239e-06\n","Epoch[6/10](13820/14999) || training loss 0.09762 || training accuracy 96.25% || lr 2.4711218141681717e-06\n","Epoch[6/10](13840/14999) || training loss 0.03863 || training accuracy 98.75% || lr 2.4700515084621045e-06\n","Epoch[6/10](13860/14999) || training loss 0.04802 || training accuracy 98.12% || lr 2.4689812027560372e-06\n","Epoch[6/10](13880/14999) || training loss 0.08943 || training accuracy 96.88% || lr 2.4679108970499696e-06\n","Epoch[6/10](13900/14999) || training loss 0.07011 || training accuracy 98.12% || lr 2.4668405913439024e-06\n","Epoch[6/10](13920/14999) || training loss 0.1453 || training accuracy 95.62% || lr 2.465770285637835e-06\n","Epoch[6/10](13940/14999) || training loss 0.08843 || training accuracy 97.50% || lr 2.464699979931768e-06\n","Epoch[6/10](13960/14999) || training loss 0.04074 || training accuracy 98.12% || lr 2.4636296742257007e-06\n","Epoch[6/10](13980/14999) || training loss 0.0455 || training accuracy 98.12% || lr 2.4625593685196335e-06\n","Epoch[6/10](14000/14999) || training loss 0.07534 || training accuracy 98.12% || lr 2.4614890628135662e-06\n","Epoch[6/10](14020/14999) || training loss 0.05001 || training accuracy 98.12% || lr 2.4604187571074986e-06\n","Epoch[6/10](14040/14999) || training loss 0.1316 || training accuracy 96.25% || lr 2.4593484514014314e-06\n","Epoch[6/10](14060/14999) || training loss 0.1186 || training accuracy 93.75% || lr 2.458278145695364e-06\n","Epoch[6/10](14080/14999) || training loss 0.1157 || training accuracy 96.25% || lr 2.457207839989297e-06\n","Epoch[6/10](14100/14999) || training loss 0.06177 || training accuracy 98.12% || lr 2.4561375342832297e-06\n","Epoch[6/10](14120/14999) || training loss 0.07295 || training accuracy 97.50% || lr 2.455067228577162e-06\n","Epoch[6/10](14140/14999) || training loss 0.1067 || training accuracy 96.88% || lr 2.4539969228710952e-06\n","Epoch[6/10](14160/14999) || training loss 0.05767 || training accuracy 98.12% || lr 2.4529266171650276e-06\n","Epoch[6/10](14180/14999) || training loss 0.04738 || training accuracy 98.12% || lr 2.4518563114589604e-06\n","Epoch[6/10](14200/14999) || training loss 0.1007 || training accuracy 96.88% || lr 2.450786005752893e-06\n","Epoch[6/10](14220/14999) || training loss 0.0387 || training accuracy 99.38% || lr 2.4497157000468255e-06\n","Epoch[6/10](14240/14999) || training loss 0.09277 || training accuracy 96.88% || lr 2.4486453943407587e-06\n","Epoch[6/10](14260/14999) || training loss 0.03736 || training accuracy 98.75% || lr 2.447575088634691e-06\n","Epoch[6/10](14280/14999) || training loss 0.1302 || training accuracy 95.62% || lr 2.4465047829286242e-06\n","Epoch[6/10](14300/14999) || training loss 0.05583 || training accuracy 98.75% || lr 2.4454344772225566e-06\n","Epoch[6/10](14320/14999) || training loss 0.09267 || training accuracy 95.62% || lr 2.444364171516489e-06\n","Epoch[6/10](14340/14999) || training loss 0.06725 || training accuracy 96.88% || lr 2.443293865810422e-06\n","Epoch[6/10](14360/14999) || training loss 0.1159 || training accuracy 95.62% || lr 2.4422235601043545e-06\n","Epoch[6/10](14380/14999) || training loss 0.03677 || training accuracy 98.75% || lr 2.4411532543982877e-06\n","Epoch[6/10](14400/14999) || training loss 0.02235 || training accuracy 99.38% || lr 2.44008294869222e-06\n","Epoch[6/10](14420/14999) || training loss 0.05323 || training accuracy 98.75% || lr 2.439012642986153e-06\n","Epoch[6/10](14440/14999) || training loss 0.09647 || training accuracy 95.62% || lr 2.4379423372800856e-06\n","Epoch[6/10](14460/14999) || training loss 0.06199 || training accuracy 98.75% || lr 2.436872031574018e-06\n","Epoch[6/10](14480/14999) || training loss 0.06958 || training accuracy 96.25% || lr 2.435801725867951e-06\n","Epoch[6/10](14500/14999) || training loss 0.06845 || training accuracy 98.12% || lr 2.4347314201618835e-06\n","Epoch[6/10](14520/14999) || training loss 0.07732 || training accuracy 98.75% || lr 2.4336611144558162e-06\n","Epoch[6/10](14540/14999) || training loss 0.02943 || training accuracy 98.75% || lr 2.432590808749749e-06\n","Epoch[6/10](14560/14999) || training loss 0.0859 || training accuracy 96.88% || lr 2.4315205030436818e-06\n","Epoch[6/10](14580/14999) || training loss 0.04327 || training accuracy 97.50% || lr 2.4304501973376146e-06\n","Epoch[6/10](14600/14999) || training loss 0.04164 || training accuracy 98.75% || lr 2.429379891631547e-06\n","Epoch[6/10](14620/14999) || training loss 0.0165 || training accuracy 99.38% || lr 2.42830958592548e-06\n","Epoch[6/10](14640/14999) || training loss 0.1137 || training accuracy 95.00% || lr 2.4272392802194125e-06\n","Epoch[6/10](14660/14999) || training loss 0.1532 || training accuracy 96.25% || lr 2.4261689745133452e-06\n","Epoch[6/10](14680/14999) || training loss 0.0634 || training accuracy 97.50% || lr 2.425098668807278e-06\n","Epoch[6/10](14700/14999) || training loss 0.09892 || training accuracy 97.50% || lr 2.4240283631012108e-06\n","Epoch[6/10](14720/14999) || training loss 0.07825 || training accuracy 97.50% || lr 2.4229580573951435e-06\n","Epoch[6/10](14740/14999) || training loss 0.02527 || training accuracy 98.75% || lr 2.421887751689076e-06\n","Epoch[6/10](14760/14999) || training loss 0.06449 || training accuracy 97.50% || lr 2.4208174459830087e-06\n","Epoch[6/10](14780/14999) || training loss 0.03257 || training accuracy 98.75% || lr 2.4197471402769414e-06\n","Epoch[6/10](14800/14999) || training loss 0.1143 || training accuracy 95.62% || lr 2.4186768345708742e-06\n","Epoch[6/10](14820/14999) || training loss 0.1624 || training accuracy 95.00% || lr 2.417606528864807e-06\n","Epoch[6/10](14840/14999) || training loss 0.07625 || training accuracy 97.50% || lr 2.4165362231587398e-06\n","Epoch[6/10](14860/14999) || training loss 0.07752 || training accuracy 96.25% || lr 2.415465917452672e-06\n","Epoch[6/10](14880/14999) || training loss 0.1213 || training accuracy 96.88% || lr 2.4143956117466053e-06\n","Epoch[6/10](14900/14999) || training loss 0.104 || training accuracy 96.88% || lr 2.4133253060405377e-06\n","Epoch[6/10](14920/14999) || training loss 0.07704 || training accuracy 98.12% || lr 2.4122550003344704e-06\n","Epoch[6/10](14940/14999) || training loss 0.04672 || training accuracy 97.50% || lr 2.411184694628403e-06\n","Epoch[6/10](14960/14999) || training loss 0.0688 || training accuracy 97.50% || lr 2.4101143889223356e-06\n","Epoch[6/10](14980/14999) || training loss 0.05831 || training accuracy 96.88% || lr 2.4090440832162688e-06\n","Calculating validation results...\n","100% 235/235 [02:22<00:00,  1.65it/s]\n","[Val] acc : 90.85%, loss: 0.3323, F1 : 0.9085 || best acc : 91.14%, best loss: 0.2442\n","Time elapsed:  467.46 min\n","\n","Epoch[7/10](20/14999) || training loss 0.09072 || training accuracy 98.12% || lr 2.4069569870894373e-06\n","Epoch[7/10](40/14999) || training loss 0.06363 || training accuracy 98.12% || lr 2.4058866813833696e-06\n","Epoch[7/10](60/14999) || training loss 0.04602 || training accuracy 98.12% || lr 2.404816375677303e-06\n","Epoch[7/10](80/14999) || training loss 0.05943 || training accuracy 98.12% || lr 2.403746069971235e-06\n","Epoch[7/10](100/14999) || training loss 0.07797 || training accuracy 98.12% || lr 2.4026757642651684e-06\n","Epoch[7/10](120/14999) || training loss 0.05074 || training accuracy 98.12% || lr 2.4016054585591007e-06\n","Epoch[7/10](140/14999) || training loss 0.04628 || training accuracy 97.50% || lr 2.4005351528530335e-06\n","Epoch[7/10](160/14999) || training loss 0.02341 || training accuracy 99.38% || lr 2.3994648471469663e-06\n","Epoch[7/10](180/14999) || training loss 0.08222 || training accuracy 95.00% || lr 2.3983945414408986e-06\n","Epoch[7/10](200/14999) || training loss 0.02888 || training accuracy 98.75% || lr 2.397324235734832e-06\n","Epoch[7/10](220/14999) || training loss 0.04941 || training accuracy 98.12% || lr 2.396253930028764e-06\n","Epoch[7/10](240/14999) || training loss 0.04378 || training accuracy 98.12% || lr 2.3951836243226974e-06\n","Epoch[7/10](260/14999) || training loss 0.03292 || training accuracy 98.75% || lr 2.3941133186166297e-06\n","Epoch[7/10](280/14999) || training loss 0.03711 || training accuracy 97.50% || lr 2.3930430129105625e-06\n","Epoch[7/10](300/14999) || training loss 0.06373 || training accuracy 96.25% || lr 2.3919727072044953e-06\n","Epoch[7/10](320/14999) || training loss 0.074 || training accuracy 97.50% || lr 2.3909024014984276e-06\n","Epoch[7/10](340/14999) || training loss 0.06196 || training accuracy 98.12% || lr 2.389832095792361e-06\n","Epoch[7/10](360/14999) || training loss 0.06317 || training accuracy 98.12% || lr 2.388761790086293e-06\n","Epoch[7/10](380/14999) || training loss 0.05989 || training accuracy 96.88% || lr 2.387691484380226e-06\n","Epoch[7/10](400/14999) || training loss 0.03834 || training accuracy 98.75% || lr 2.3866211786741587e-06\n","Epoch[7/10](420/14999) || training loss 0.04183 || training accuracy 98.12% || lr 2.3855508729680915e-06\n","Epoch[7/10](440/14999) || training loss 0.05561 || training accuracy 98.12% || lr 2.3844805672620243e-06\n","Epoch[7/10](460/14999) || training loss 0.03949 || training accuracy 98.75% || lr 2.3834102615559566e-06\n","Epoch[7/10](480/14999) || training loss 0.0241 || training accuracy 99.38% || lr 2.3823399558498894e-06\n","Epoch[7/10](500/14999) || training loss 0.0662 || training accuracy 97.50% || lr 2.381269650143822e-06\n","Epoch[7/10](520/14999) || training loss 0.04358 || training accuracy 98.75% || lr 2.380199344437755e-06\n","Epoch[7/10](540/14999) || training loss 0.07999 || training accuracy 97.50% || lr 2.3791290387316877e-06\n","Epoch[7/10](560/14999) || training loss 0.0514 || training accuracy 98.12% || lr 2.3780587330256205e-06\n","Epoch[7/10](580/14999) || training loss 0.013 || training accuracy 100.00% || lr 2.376988427319553e-06\n","Epoch[7/10](600/14999) || training loss 0.04099 || training accuracy 98.75% || lr 2.3759181216134856e-06\n","Epoch[7/10](620/14999) || training loss 0.0787 || training accuracy 97.50% || lr 2.3748478159074184e-06\n","Epoch[7/10](640/14999) || training loss 0.03675 || training accuracy 98.75% || lr 2.373777510201351e-06\n","Epoch[7/10](660/14999) || training loss 0.1067 || training accuracy 97.50% || lr 2.372707204495284e-06\n","Epoch[7/10](680/14999) || training loss 0.04456 || training accuracy 99.38% || lr 2.3716368987892163e-06\n","Epoch[7/10](700/14999) || training loss 0.04939 || training accuracy 97.50% || lr 2.3705665930831495e-06\n","Epoch[7/10](720/14999) || training loss 0.03601 || training accuracy 98.75% || lr 2.369496287377082e-06\n","Epoch[7/10](740/14999) || training loss 0.02208 || training accuracy 99.38% || lr 2.368425981671015e-06\n","Epoch[7/10](760/14999) || training loss 0.03359 || training accuracy 98.75% || lr 2.3673556759649474e-06\n","Epoch[7/10](780/14999) || training loss 0.03839 || training accuracy 99.38% || lr 2.3662853702588797e-06\n","Epoch[7/10](800/14999) || training loss 0.08521 || training accuracy 97.50% || lr 2.365215064552813e-06\n","Epoch[7/10](820/14999) || training loss 0.04021 || training accuracy 97.50% || lr 2.3641447588467453e-06\n","Epoch[7/10](840/14999) || training loss 0.08299 || training accuracy 97.50% || lr 2.3630744531406785e-06\n","Epoch[7/10](860/14999) || training loss 0.0746 || training accuracy 98.12% || lr 2.362004147434611e-06\n","Epoch[7/10](880/14999) || training loss 0.0829 || training accuracy 96.88% || lr 2.360933841728544e-06\n","Epoch[7/10](900/14999) || training loss 0.09478 || training accuracy 95.62% || lr 2.3598635360224764e-06\n","Epoch[7/10](920/14999) || training loss 0.06492 || training accuracy 97.50% || lr 2.3587932303164087e-06\n","Epoch[7/10](940/14999) || training loss 0.03641 || training accuracy 98.75% || lr 2.357722924610342e-06\n","Epoch[7/10](960/14999) || training loss 0.02054 || training accuracy 98.75% || lr 2.3566526189042743e-06\n","Epoch[7/10](980/14999) || training loss 0.08491 || training accuracy 98.12% || lr 2.3555823131982075e-06\n","Epoch[7/10](1000/14999) || training loss 0.01874 || training accuracy 100.00% || lr 2.35451200749214e-06\n","Epoch[7/10](1020/14999) || training loss 0.04279 || training accuracy 98.12% || lr 2.3534417017860726e-06\n","Epoch[7/10](1040/14999) || training loss 0.04478 || training accuracy 98.12% || lr 2.3523713960800054e-06\n","Epoch[7/10](1060/14999) || training loss 0.09962 || training accuracy 97.50% || lr 2.3513010903739377e-06\n","Epoch[7/10](1080/14999) || training loss 0.05546 || training accuracy 98.12% || lr 2.350230784667871e-06\n","Epoch[7/10](1100/14999) || training loss 0.06342 || training accuracy 97.50% || lr 2.3491604789618033e-06\n","Epoch[7/10](1120/14999) || training loss 0.05765 || training accuracy 98.12% || lr 2.348090173255736e-06\n","Epoch[7/10](1140/14999) || training loss 0.04128 || training accuracy 98.12% || lr 2.347019867549669e-06\n","Epoch[7/10](1160/14999) || training loss 0.09291 || training accuracy 96.88% || lr 2.3459495618436016e-06\n","Epoch[7/10](1180/14999) || training loss 0.06274 || training accuracy 98.12% || lr 2.3448792561375343e-06\n","Epoch[7/10](1200/14999) || training loss 0.1278 || training accuracy 96.25% || lr 2.3438089504314667e-06\n","Epoch[7/10](1220/14999) || training loss 0.03593 || training accuracy 99.38% || lr 2.3427386447253995e-06\n","Epoch[7/10](1240/14999) || training loss 0.09783 || training accuracy 95.62% || lr 2.3416683390193322e-06\n","Epoch[7/10](1260/14999) || training loss 0.04556 || training accuracy 97.50% || lr 2.340598033313265e-06\n","Epoch[7/10](1280/14999) || training loss 0.0517 || training accuracy 96.88% || lr 2.339527727607198e-06\n","Epoch[7/10](1300/14999) || training loss 0.07986 || training accuracy 95.62% || lr 2.3384574219011306e-06\n","Epoch[7/10](1320/14999) || training loss 0.06634 || training accuracy 98.12% || lr 2.337387116195063e-06\n","Epoch[7/10](1340/14999) || training loss 0.02734 || training accuracy 99.38% || lr 2.3363168104889957e-06\n","Epoch[7/10](1360/14999) || training loss 0.04195 || training accuracy 98.75% || lr 2.3352465047829285e-06\n","Epoch[7/10](1380/14999) || training loss 0.0126 || training accuracy 100.00% || lr 2.3341761990768612e-06\n","Epoch[7/10](1400/14999) || training loss 0.01646 || training accuracy 100.00% || lr 2.333105893370794e-06\n","Epoch[7/10](1420/14999) || training loss 0.04853 || training accuracy 99.38% || lr 2.3320355876647264e-06\n","Epoch[7/10](1440/14999) || training loss 0.04989 || training accuracy 98.75% || lr 2.3309652819586596e-06\n","Epoch[7/10](1460/14999) || training loss 0.07475 || training accuracy 98.12% || lr 2.329894976252592e-06\n","Epoch[7/10](1480/14999) || training loss 0.08527 || training accuracy 96.88% || lr 2.3288246705465247e-06\n","Epoch[7/10](1500/14999) || training loss 0.07751 || training accuracy 97.50% || lr 2.3277543648404575e-06\n","Epoch[7/10](1520/14999) || training loss 0.04942 || training accuracy 98.12% || lr 2.3266840591343902e-06\n","Epoch[7/10](1540/14999) || training loss 0.05587 || training accuracy 98.12% || lr 2.325613753428323e-06\n","Epoch[7/10](1560/14999) || training loss 0.05175 || training accuracy 99.38% || lr 2.3245434477222554e-06\n","Epoch[7/10](1580/14999) || training loss 0.04376 || training accuracy 98.75% || lr 2.3234731420161885e-06\n","Epoch[7/10](1600/14999) || training loss 0.04982 || training accuracy 97.50% || lr 2.322402836310121e-06\n","Epoch[7/10](1620/14999) || training loss 0.06479 || training accuracy 96.25% || lr 2.3213325306040537e-06\n","Epoch[7/10](1640/14999) || training loss 0.07412 || training accuracy 98.12% || lr 2.3202622248979864e-06\n","Epoch[7/10](1660/14999) || training loss 0.1513 || training accuracy 96.88% || lr 2.319191919191919e-06\n","Epoch[7/10](1680/14999) || training loss 0.07934 || training accuracy 96.88% || lr 2.318121613485852e-06\n","Epoch[7/10](1700/14999) || training loss 0.04049 || training accuracy 98.75% || lr 2.3170513077797843e-06\n","Epoch[7/10](1720/14999) || training loss 0.05794 || training accuracy 98.12% || lr 2.3159810020737175e-06\n","Epoch[7/10](1740/14999) || training loss 0.03203 || training accuracy 98.75% || lr 2.31491069636765e-06\n","Epoch[7/10](1760/14999) || training loss 0.04646 || training accuracy 97.50% || lr 2.3138403906615822e-06\n","Epoch[7/10](1780/14999) || training loss 0.0441 || training accuracy 98.75% || lr 2.3127700849555154e-06\n","Epoch[7/10](1800/14999) || training loss 0.07291 || training accuracy 98.12% || lr 2.3116997792494478e-06\n","Epoch[7/10](1820/14999) || training loss 0.04145 || training accuracy 98.75% || lr 2.310629473543381e-06\n","Epoch[7/10](1840/14999) || training loss 0.02358 || training accuracy 98.75% || lr 2.3095591678373133e-06\n","Epoch[7/10](1860/14999) || training loss 0.04125 || training accuracy 98.12% || lr 2.308488862131246e-06\n","Epoch[7/10](1880/14999) || training loss 0.01593 || training accuracy 99.38% || lr 2.307418556425179e-06\n","Epoch[7/10](1900/14999) || training loss 0.08869 || training accuracy 98.12% || lr 2.3063482507191112e-06\n","Epoch[7/10](1920/14999) || training loss 0.02733 || training accuracy 99.38% || lr 2.3052779450130444e-06\n","Epoch[7/10](1940/14999) || training loss 0.01316 || training accuracy 100.00% || lr 2.3042076393069768e-06\n","Epoch[7/10](1960/14999) || training loss 0.009559 || training accuracy 100.00% || lr 2.3031373336009096e-06\n","Epoch[7/10](1980/14999) || training loss 0.1042 || training accuracy 98.12% || lr 2.3020670278948423e-06\n","Epoch[7/10](2000/14999) || training loss 0.06548 || training accuracy 97.50% || lr 2.300996722188775e-06\n","Epoch[7/10](2020/14999) || training loss 0.07327 || training accuracy 97.50% || lr 2.299926416482708e-06\n","Epoch[7/10](2040/14999) || training loss 0.1094 || training accuracy 97.50% || lr 2.2988561107766402e-06\n","Epoch[7/10](2060/14999) || training loss 0.07791 || training accuracy 97.50% || lr 2.297785805070573e-06\n","Epoch[7/10](2080/14999) || training loss 0.06066 || training accuracy 99.38% || lr 2.2967154993645058e-06\n","Epoch[7/10](2100/14999) || training loss 0.07468 || training accuracy 98.12% || lr 2.2956451936584385e-06\n","Epoch[7/10](2120/14999) || training loss 0.02601 || training accuracy 100.00% || lr 2.2945748879523713e-06\n","Epoch[7/10](2140/14999) || training loss 0.06404 || training accuracy 96.88% || lr 2.293504582246304e-06\n","Epoch[7/10](2160/14999) || training loss 0.1725 || training accuracy 95.62% || lr 2.292434276540237e-06\n","Epoch[7/10](2180/14999) || training loss 0.06933 || training accuracy 97.50% || lr 2.2913639708341696e-06\n","Epoch[7/10](2200/14999) || training loss 0.03923 || training accuracy 98.12% || lr 2.290293665128102e-06\n","Epoch[7/10](2220/14999) || training loss 0.0656 || training accuracy 98.12% || lr 2.2892233594220348e-06\n","Epoch[7/10](2240/14999) || training loss 0.07825 || training accuracy 98.12% || lr 2.2881530537159675e-06\n","Epoch[7/10](2260/14999) || training loss 0.09659 || training accuracy 95.62% || lr 2.2870827480099003e-06\n","Epoch[7/10](2280/14999) || training loss 0.07049 || training accuracy 97.50% || lr 2.286012442303833e-06\n","Epoch[7/10](2300/14999) || training loss 0.04761 || training accuracy 97.50% || lr 2.2849421365977654e-06\n","Epoch[7/10](2320/14999) || training loss 0.03973 || training accuracy 98.75% || lr 2.2838718308916986e-06\n","Epoch[7/10](2340/14999) || training loss 0.02892 || training accuracy 99.38% || lr 2.282801525185631e-06\n","Epoch[7/10](2360/14999) || training loss 0.04915 || training accuracy 98.75% || lr 2.2817312194795638e-06\n","Epoch[7/10](2380/14999) || training loss 0.01522 || training accuracy 99.38% || lr 2.2806609137734965e-06\n","Epoch[7/10](2400/14999) || training loss 0.01549 || training accuracy 99.38% || lr 2.279590608067429e-06\n","Epoch[7/10](2420/14999) || training loss 0.07301 || training accuracy 96.88% || lr 2.278520302361362e-06\n","Epoch[7/10](2440/14999) || training loss 0.03962 || training accuracy 98.12% || lr 2.2774499966552944e-06\n","Epoch[7/10](2460/14999) || training loss 0.1009 || training accuracy 95.62% || lr 2.2763796909492276e-06\n","Epoch[7/10](2480/14999) || training loss 0.01969 || training accuracy 99.38% || lr 2.27530938524316e-06\n","Epoch[7/10](2500/14999) || training loss 0.03605 || training accuracy 99.38% || lr 2.2742390795370923e-06\n","Epoch[7/10](2520/14999) || training loss 0.05191 || training accuracy 97.50% || lr 2.2731687738310255e-06\n","Epoch[7/10](2540/14999) || training loss 0.03247 || training accuracy 98.75% || lr 2.272098468124958e-06\n","Epoch[7/10](2560/14999) || training loss 0.0409 || training accuracy 98.12% || lr 2.271028162418891e-06\n","Epoch[7/10](2580/14999) || training loss 0.03923 || training accuracy 98.75% || lr 2.2699578567128234e-06\n","Epoch[7/10](2600/14999) || training loss 0.05463 || training accuracy 98.75% || lr 2.268887551006756e-06\n","Epoch[7/10](2620/14999) || training loss 0.06023 || training accuracy 97.50% || lr 2.267817245300689e-06\n","Epoch[7/10](2640/14999) || training loss 0.03871 || training accuracy 98.12% || lr 2.2667469395946213e-06\n","Epoch[7/10](2660/14999) || training loss 0.1164 || training accuracy 95.62% || lr 2.2656766338885545e-06\n","Epoch[7/10](2680/14999) || training loss 0.05241 || training accuracy 98.75% || lr 2.264606328182487e-06\n","Epoch[7/10](2700/14999) || training loss 0.0617 || training accuracy 98.12% || lr 2.2635360224764196e-06\n","Epoch[7/10](2720/14999) || training loss 0.06808 || training accuracy 98.75% || lr 2.2624657167703524e-06\n","Epoch[7/10](2740/14999) || training loss 0.0697 || training accuracy 96.88% || lr 2.261395411064285e-06\n","Epoch[7/10](2760/14999) || training loss 0.04444 || training accuracy 99.38% || lr 2.260325105358218e-06\n","Epoch[7/10](2780/14999) || training loss 0.03701 || training accuracy 98.12% || lr 2.2592547996521503e-06\n","Epoch[7/10](2800/14999) || training loss 0.04062 || training accuracy 98.12% || lr 2.2581844939460835e-06\n","Epoch[7/10](2820/14999) || training loss 0.04212 || training accuracy 98.12% || lr 2.257114188240016e-06\n","Epoch[7/10](2840/14999) || training loss 0.03075 || training accuracy 98.75% || lr 2.2560438825339486e-06\n","Epoch[7/10](2860/14999) || training loss 0.04926 || training accuracy 98.12% || lr 2.2549735768278814e-06\n","Epoch[7/10](2880/14999) || training loss 0.01753 || training accuracy 100.00% || lr 2.253903271121814e-06\n","Epoch[7/10](2900/14999) || training loss 0.05725 || training accuracy 98.12% || lr 2.252832965415747e-06\n","Epoch[7/10](2920/14999) || training loss 0.05921 || training accuracy 98.12% || lr 2.2517626597096793e-06\n","Epoch[7/10](2940/14999) || training loss 0.06134 || training accuracy 98.75% || lr 2.250692354003612e-06\n","Epoch[7/10](2960/14999) || training loss 0.08618 || training accuracy 97.50% || lr 2.249622048297545e-06\n","Epoch[7/10](2980/14999) || training loss 0.1479 || training accuracy 94.38% || lr 2.2485517425914776e-06\n","Epoch[7/10](3000/14999) || training loss 0.02454 || training accuracy 100.00% || lr 2.2474814368854104e-06\n","Epoch[7/10](3020/14999) || training loss 0.02624 || training accuracy 99.38% || lr 2.246411131179343e-06\n","Epoch[7/10](3040/14999) || training loss 0.09728 || training accuracy 96.88% || lr 2.2453408254732755e-06\n","Epoch[7/10](3060/14999) || training loss 0.05265 || training accuracy 98.12% || lr 2.2442705197672083e-06\n","Epoch[7/10](3080/14999) || training loss 0.1198 || training accuracy 95.62% || lr 2.243200214061141e-06\n","Epoch[7/10](3100/14999) || training loss 0.06629 || training accuracy 98.75% || lr 2.242129908355074e-06\n","Epoch[7/10](3120/14999) || training loss 0.1038 || training accuracy 96.88% || lr 2.2410596026490066e-06\n","Epoch[7/10](3140/14999) || training loss 0.02591 || training accuracy 99.38% || lr 2.239989296942939e-06\n","Epoch[7/10](3160/14999) || training loss 0.05057 || training accuracy 98.12% || lr 2.238918991236872e-06\n","Epoch[7/10](3180/14999) || training loss 0.03807 || training accuracy 98.75% || lr 2.2378486855308045e-06\n","Epoch[7/10](3200/14999) || training loss 0.02618 || training accuracy 98.75% || lr 2.2367783798247373e-06\n","Epoch[7/10](3220/14999) || training loss 0.03454 || training accuracy 98.75% || lr 2.23570807411867e-06\n","Epoch[7/10](3240/14999) || training loss 0.03351 || training accuracy 99.38% || lr 2.2346377684126024e-06\n","Epoch[7/10](3260/14999) || training loss 0.03178 || training accuracy 98.75% || lr 2.2335674627065356e-06\n","Epoch[7/10](3280/14999) || training loss 0.0422 || training accuracy 98.12% || lr 2.232497157000468e-06\n","Epoch[7/10](3300/14999) || training loss 0.03732 || training accuracy 98.75% || lr 2.231426851294401e-06\n","Epoch[7/10](3320/14999) || training loss 0.03139 || training accuracy 98.12% || lr 2.2303565455883335e-06\n","Epoch[7/10](3340/14999) || training loss 0.05063 || training accuracy 98.12% || lr 2.229286239882266e-06\n","Epoch[7/10](3360/14999) || training loss 0.04384 || training accuracy 98.12% || lr 2.228215934176199e-06\n","Epoch[7/10](3380/14999) || training loss 0.04806 || training accuracy 98.12% || lr 2.2271456284701314e-06\n","Epoch[7/10](3400/14999) || training loss 0.06754 || training accuracy 98.12% || lr 2.2260753227640646e-06\n","Epoch[7/10](3420/14999) || training loss 0.03473 || training accuracy 98.12% || lr 2.225005017057997e-06\n","Epoch[7/10](3440/14999) || training loss 0.05531 || training accuracy 97.50% || lr 2.22393471135193e-06\n","Epoch[7/10](3460/14999) || training loss 0.04214 || training accuracy 99.38% || lr 2.2228644056458625e-06\n","Epoch[7/10](3480/14999) || training loss 0.05509 || training accuracy 98.12% || lr 2.221794099939795e-06\n","Epoch[7/10](3500/14999) || training loss 0.1102 || training accuracy 98.12% || lr 2.220723794233728e-06\n","Epoch[7/10](3520/14999) || training loss 0.05707 || training accuracy 97.50% || lr 2.2196534885276604e-06\n","Epoch[7/10](3540/14999) || training loss 0.06671 || training accuracy 97.50% || lr 2.2185831828215936e-06\n","Epoch[7/10](3560/14999) || training loss 0.05095 || training accuracy 98.12% || lr 2.217512877115526e-06\n","Epoch[7/10](3580/14999) || training loss 0.04236 || training accuracy 98.12% || lr 2.2164425714094587e-06\n","Epoch[7/10](3600/14999) || training loss 0.04336 || training accuracy 98.75% || lr 2.2153722657033915e-06\n","Epoch[7/10](3620/14999) || training loss 0.03315 || training accuracy 99.38% || lr 2.2143019599973243e-06\n","Epoch[7/10](3640/14999) || training loss 0.05241 || training accuracy 96.88% || lr 2.213231654291257e-06\n","Epoch[7/10](3660/14999) || training loss 0.06866 || training accuracy 98.12% || lr 2.2121613485851894e-06\n","Epoch[7/10](3680/14999) || training loss 0.0538 || training accuracy 98.75% || lr 2.211091042879122e-06\n","Epoch[7/10](3700/14999) || training loss 0.06504 || training accuracy 97.50% || lr 2.210020737173055e-06\n","Epoch[7/10](3720/14999) || training loss 0.05023 || training accuracy 98.75% || lr 2.2089504314669877e-06\n","Epoch[7/10](3740/14999) || training loss 0.09895 || training accuracy 98.12% || lr 2.2078801257609205e-06\n","Epoch[7/10](3760/14999) || training loss 0.04555 || training accuracy 98.75% || lr 2.2068098200548533e-06\n","Epoch[7/10](3780/14999) || training loss 0.04795 || training accuracy 98.12% || lr 2.2057395143487856e-06\n","Epoch[7/10](3800/14999) || training loss 0.03231 || training accuracy 98.75% || lr 2.2046692086427184e-06\n","Epoch[7/10](3820/14999) || training loss 0.1091 || training accuracy 96.88% || lr 2.203598902936651e-06\n","Epoch[7/10](3840/14999) || training loss 0.04416 || training accuracy 98.12% || lr 2.202528597230584e-06\n","Epoch[7/10](3860/14999) || training loss 0.01096 || training accuracy 100.00% || lr 2.2014582915245167e-06\n","Epoch[7/10](3880/14999) || training loss 0.04212 || training accuracy 98.12% || lr 2.200387985818449e-06\n","Epoch[7/10](3900/14999) || training loss 0.01177 || training accuracy 100.00% || lr 2.1993176801123822e-06\n","Epoch[7/10](3920/14999) || training loss 0.01567 || training accuracy 99.38% || lr 2.1982473744063146e-06\n","Epoch[7/10](3940/14999) || training loss 0.06915 || training accuracy 98.12% || lr 2.1971770687002474e-06\n","Epoch[7/10](3960/14999) || training loss 0.05027 || training accuracy 98.12% || lr 2.19610676299418e-06\n","Epoch[7/10](3980/14999) || training loss 0.06749 || training accuracy 97.50% || lr 2.1950364572881125e-06\n","Epoch[7/10](4000/14999) || training loss 0.09867 || training accuracy 96.25% || lr 2.1939661515820457e-06\n","Epoch[7/10](4020/14999) || training loss 0.02472 || training accuracy 99.38% || lr 2.192895845875978e-06\n","Epoch[7/10](4040/14999) || training loss 0.06017 || training accuracy 98.75% || lr 2.1918255401699112e-06\n","Epoch[7/10](4060/14999) || training loss 0.02545 || training accuracy 98.75% || lr 2.1907552344638436e-06\n","Epoch[7/10](4080/14999) || training loss 0.03738 || training accuracy 98.75% || lr 2.1896849287577764e-06\n","Epoch[7/10](4100/14999) || training loss 0.04545 || training accuracy 98.12% || lr 2.188614623051709e-06\n","Epoch[7/10](4120/14999) || training loss 0.07287 || training accuracy 98.12% || lr 2.1875443173456415e-06\n","Epoch[7/10](4140/14999) || training loss 0.03773 || training accuracy 98.75% || lr 2.1864740116395747e-06\n","Epoch[7/10](4160/14999) || training loss 0.09303 || training accuracy 96.88% || lr 2.185403705933507e-06\n","Epoch[7/10](4180/14999) || training loss 0.1431 || training accuracy 96.88% || lr 2.1843334002274402e-06\n","Epoch[7/10](4200/14999) || training loss 0.05824 || training accuracy 98.12% || lr 2.1832630945213726e-06\n","Epoch[7/10](4220/14999) || training loss 0.01506 || training accuracy 100.00% || lr 2.182192788815305e-06\n","Epoch[7/10](4240/14999) || training loss 0.03358 || training accuracy 98.75% || lr 2.181122483109238e-06\n","Epoch[7/10](4260/14999) || training loss 0.05561 || training accuracy 99.38% || lr 2.1800521774031705e-06\n","Epoch[7/10](4280/14999) || training loss 0.04419 || training accuracy 97.50% || lr 2.1789818716971037e-06\n","Epoch[7/10](4300/14999) || training loss 0.03632 || training accuracy 98.75% || lr 2.177911565991036e-06\n","Epoch[7/10](4320/14999) || training loss 0.07674 || training accuracy 97.50% || lr 2.176841260284969e-06\n","Epoch[7/10](4340/14999) || training loss 0.062 || training accuracy 96.88% || lr 2.1757709545789016e-06\n","Epoch[7/10](4360/14999) || training loss 0.03595 || training accuracy 98.75% || lr 2.174700648872834e-06\n","Epoch[7/10](4380/14999) || training loss 0.05687 || training accuracy 96.25% || lr 2.173630343166767e-06\n","Epoch[7/10](4400/14999) || training loss 0.03385 || training accuracy 98.75% || lr 2.1725600374606995e-06\n","Epoch[7/10](4420/14999) || training loss 0.08649 || training accuracy 95.62% || lr 2.1714897317546322e-06\n","Epoch[7/10](4440/14999) || training loss 0.02119 || training accuracy 99.38% || lr 2.170419426048565e-06\n","Epoch[7/10](4460/14999) || training loss 0.04086 || training accuracy 96.88% || lr 2.169349120342498e-06\n","Epoch[7/10](4480/14999) || training loss 0.06567 || training accuracy 98.12% || lr 2.1682788146364306e-06\n","Epoch[7/10](4500/14999) || training loss 0.02776 || training accuracy 98.75% || lr 2.167208508930363e-06\n","Epoch[7/10](4520/14999) || training loss 0.05542 || training accuracy 98.12% || lr 2.1661382032242957e-06\n","Epoch[7/10](4540/14999) || training loss 0.05398 || training accuracy 97.50% || lr 2.1650678975182285e-06\n","Epoch[7/10](4560/14999) || training loss 0.03915 || training accuracy 99.38% || lr 2.1639975918121612e-06\n","Epoch[7/10](4580/14999) || training loss 0.03271 || training accuracy 97.50% || lr 2.162927286106094e-06\n","Epoch[7/10](4600/14999) || training loss 0.06465 || training accuracy 98.12% || lr 2.1618569804000268e-06\n","Epoch[7/10](4620/14999) || training loss 0.03586 || training accuracy 98.75% || lr 2.160786674693959e-06\n","Epoch[7/10](4640/14999) || training loss 0.04872 || training accuracy 98.12% || lr 2.159716368987892e-06\n","Epoch[7/10](4660/14999) || training loss 0.04194 || training accuracy 99.38% || lr 2.1586460632818247e-06\n","Epoch[7/10](4680/14999) || training loss 0.05641 || training accuracy 97.50% || lr 2.1575757575757575e-06\n","Epoch[7/10](4700/14999) || training loss 0.04449 || training accuracy 96.88% || lr 2.1565054518696902e-06\n","Epoch[7/10](4720/14999) || training loss 0.03121 || training accuracy 99.38% || lr 2.155435146163623e-06\n","Epoch[7/10](4740/14999) || training loss 0.03489 || training accuracy 98.75% || lr 2.1543648404575558e-06\n","Epoch[7/10](4760/14999) || training loss 0.09035 || training accuracy 97.50% || lr 2.153294534751488e-06\n","Epoch[7/10](4780/14999) || training loss 0.07053 || training accuracy 98.12% || lr 2.1522242290454213e-06\n","Epoch[7/10](4800/14999) || training loss 0.07073 || training accuracy 97.50% || lr 2.1511539233393537e-06\n","Epoch[7/10](4820/14999) || training loss 0.04467 || training accuracy 98.12% || lr 2.1500836176332864e-06\n","Epoch[7/10](4840/14999) || training loss 0.08078 || training accuracy 98.12% || lr 2.1490133119272192e-06\n","Epoch[7/10](4860/14999) || training loss 0.05987 || training accuracy 97.50% || lr 2.1479430062211516e-06\n","Epoch[7/10](4880/14999) || training loss 0.01788 || training accuracy 100.00% || lr 2.1468727005150848e-06\n","Epoch[7/10](4900/14999) || training loss 0.08402 || training accuracy 98.12% || lr 2.145802394809017e-06\n","Epoch[7/10](4920/14999) || training loss 0.0288 || training accuracy 98.75% || lr 2.1447320891029503e-06\n","Epoch[7/10](4940/14999) || training loss 0.02933 || training accuracy 99.38% || lr 2.1436617833968827e-06\n","Epoch[7/10](4960/14999) || training loss 0.0521 || training accuracy 98.12% || lr 2.142591477690815e-06\n","Epoch[7/10](4980/14999) || training loss 0.06969 || training accuracy 96.88% || lr 2.141521171984748e-06\n","Epoch[7/10](5000/14999) || training loss 0.04243 || training accuracy 97.50% || lr 2.1404508662786806e-06\n","Epoch[7/10](5020/14999) || training loss 0.05618 || training accuracy 99.38% || lr 2.1393805605726138e-06\n","Epoch[7/10](5040/14999) || training loss 0.06488 || training accuracy 98.12% || lr 2.138310254866546e-06\n","Epoch[7/10](5060/14999) || training loss 0.02331 || training accuracy 98.75% || lr 2.137239949160479e-06\n","Epoch[7/10](5080/14999) || training loss 0.08757 || training accuracy 97.50% || lr 2.1361696434544117e-06\n","Epoch[7/10](5100/14999) || training loss 0.07572 || training accuracy 98.75% || lr 2.135099337748344e-06\n","Epoch[7/10](5120/14999) || training loss 0.05019 || training accuracy 98.75% || lr 2.134029032042277e-06\n","Epoch[7/10](5140/14999) || training loss 0.02372 || training accuracy 99.38% || lr 2.1329587263362096e-06\n","Epoch[7/10](5160/14999) || training loss 0.0799 || training accuracy 96.25% || lr 2.1318884206301423e-06\n","Epoch[7/10](5180/14999) || training loss 0.03798 || training accuracy 98.75% || lr 2.130818114924075e-06\n","Epoch[7/10](5200/14999) || training loss 0.04561 || training accuracy 97.50% || lr 2.129747809218008e-06\n","Epoch[7/10](5220/14999) || training loss 0.06503 || training accuracy 98.12% || lr 2.1286775035119406e-06\n","Epoch[7/10](5240/14999) || training loss 0.1909 || training accuracy 96.25% || lr 2.127607197805873e-06\n","Epoch[7/10](5260/14999) || training loss 0.04034 || training accuracy 98.12% || lr 2.1265368920998058e-06\n","Epoch[7/10](5280/14999) || training loss 0.04673 || training accuracy 97.50% || lr 2.1254665863937385e-06\n","Epoch[7/10](5300/14999) || training loss 0.09588 || training accuracy 97.50% || lr 2.1243962806876713e-06\n","Epoch[7/10](5320/14999) || training loss 0.03879 || training accuracy 98.75% || lr 2.123325974981604e-06\n","Epoch[7/10](5340/14999) || training loss 0.07814 || training accuracy 96.25% || lr 2.122255669275537e-06\n","Epoch[7/10](5360/14999) || training loss 0.06726 || training accuracy 98.12% || lr 2.1211853635694696e-06\n","Epoch[7/10](5380/14999) || training loss 0.04054 || training accuracy 98.75% || lr 2.120115057863402e-06\n","Epoch[7/10](5400/14999) || training loss 0.06273 || training accuracy 96.88% || lr 2.1190447521573348e-06\n","Epoch[7/10](5420/14999) || training loss 0.04594 || training accuracy 97.50% || lr 2.1179744464512675e-06\n","Epoch[7/10](5440/14999) || training loss 0.03782 || training accuracy 99.38% || lr 2.1169041407452003e-06\n","Epoch[7/10](5460/14999) || training loss 0.05636 || training accuracy 96.88% || lr 2.115833835039133e-06\n","Epoch[7/10](5480/14999) || training loss 0.05555 || training accuracy 98.75% || lr 2.114763529333066e-06\n","Epoch[7/10](5500/14999) || training loss 0.09064 || training accuracy 96.88% || lr 2.113693223626998e-06\n","Epoch[7/10](5520/14999) || training loss 0.03513 || training accuracy 98.12% || lr 2.112622917920931e-06\n","Epoch[7/10](5540/14999) || training loss 0.02201 || training accuracy 99.38% || lr 2.1115526122148638e-06\n","Epoch[7/10](5560/14999) || training loss 0.1048 || training accuracy 96.88% || lr 2.1104823065087965e-06\n","Epoch[7/10](5580/14999) || training loss 0.03952 || training accuracy 98.75% || lr 2.1094120008027293e-06\n","Epoch[7/10](5600/14999) || training loss 0.03885 || training accuracy 98.12% || lr 2.1083416950966617e-06\n","Epoch[7/10](5620/14999) || training loss 0.06526 || training accuracy 97.50% || lr 2.107271389390595e-06\n","Epoch[7/10](5640/14999) || training loss 0.1528 || training accuracy 93.75% || lr 2.106201083684527e-06\n","Epoch[7/10](5660/14999) || training loss 0.05203 || training accuracy 98.12% || lr 2.10513077797846e-06\n","Epoch[7/10](5680/14999) || training loss 0.03918 || training accuracy 98.75% || lr 2.1040604722723927e-06\n","Epoch[7/10](5700/14999) || training loss 0.04727 || training accuracy 98.12% || lr 2.102990166566325e-06\n","Epoch[7/10](5720/14999) || training loss 0.05761 || training accuracy 98.12% || lr 2.1019198608602583e-06\n","Epoch[7/10](5740/14999) || training loss 0.04963 || training accuracy 97.50% || lr 2.1008495551541906e-06\n","Epoch[7/10](5760/14999) || training loss 0.02167 || training accuracy 100.00% || lr 2.099779249448124e-06\n","Epoch[7/10](5780/14999) || training loss 0.04155 || training accuracy 98.75% || lr 2.098708943742056e-06\n","Epoch[7/10](5800/14999) || training loss 0.1326 || training accuracy 95.62% || lr 2.0976386380359885e-06\n","Epoch[7/10](5820/14999) || training loss 0.07961 || training accuracy 96.88% || lr 2.0965683323299217e-06\n","Epoch[7/10](5840/14999) || training loss 0.03275 || training accuracy 98.12% || lr 2.095498026623854e-06\n","Epoch[7/10](5860/14999) || training loss 0.09518 || training accuracy 96.88% || lr 2.0944277209177873e-06\n","Epoch[7/10](5880/14999) || training loss 0.04649 || training accuracy 97.50% || lr 2.0933574152117196e-06\n","Epoch[7/10](5900/14999) || training loss 0.06935 || training accuracy 98.12% || lr 2.0922871095056524e-06\n","Epoch[7/10](5920/14999) || training loss 0.08934 || training accuracy 96.88% || lr 2.091216803799585e-06\n","Epoch[7/10](5940/14999) || training loss 0.05366 || training accuracy 98.12% || lr 2.0901464980935175e-06\n","Epoch[7/10](5960/14999) || training loss 0.04114 || training accuracy 98.12% || lr 2.0890761923874507e-06\n","Epoch[7/10](5980/14999) || training loss 0.06236 || training accuracy 98.12% || lr 2.088005886681383e-06\n","Epoch[7/10](6000/14999) || training loss 0.04886 || training accuracy 97.50% || lr 2.0869355809753163e-06\n","Epoch[7/10](6020/14999) || training loss 0.08792 || training accuracy 96.88% || lr 2.0858652752692486e-06\n","Epoch[7/10](6040/14999) || training loss 0.05076 || training accuracy 97.50% || lr 2.0847949695631814e-06\n","Epoch[7/10](6060/14999) || training loss 0.06311 || training accuracy 98.12% || lr 2.083724663857114e-06\n","Epoch[7/10](6080/14999) || training loss 0.02334 || training accuracy 99.38% || lr 2.0826543581510465e-06\n","Epoch[7/10](6100/14999) || training loss 0.07915 || training accuracy 98.75% || lr 2.0815840524449797e-06\n","Epoch[7/10](6120/14999) || training loss 0.0193 || training accuracy 99.38% || lr 2.080513746738912e-06\n","Epoch[7/10](6140/14999) || training loss 0.1057 || training accuracy 97.50% || lr 2.079443441032845e-06\n","Epoch[7/10](6160/14999) || training loss 0.03062 || training accuracy 99.38% || lr 2.0783731353267776e-06\n","Epoch[7/10](6180/14999) || training loss 0.01926 || training accuracy 100.00% || lr 2.0773028296207104e-06\n","Epoch[7/10](6200/14999) || training loss 0.05343 || training accuracy 98.12% || lr 2.076232523914643e-06\n","Epoch[7/10](6220/14999) || training loss 0.07869 || training accuracy 97.50% || lr 2.075162218208576e-06\n","Epoch[7/10](6240/14999) || training loss 0.1176 || training accuracy 95.62% || lr 2.0740919125025083e-06\n","Epoch[7/10](6260/14999) || training loss 0.03866 || training accuracy 98.75% || lr 2.073021606796441e-06\n","Epoch[7/10](6280/14999) || training loss 0.04135 || training accuracy 98.12% || lr 2.071951301090374e-06\n","Epoch[7/10](6300/14999) || training loss 0.04156 || training accuracy 98.12% || lr 2.0708809953843066e-06\n","Epoch[7/10](6320/14999) || training loss 0.02264 || training accuracy 99.38% || lr 2.0698106896782394e-06\n","Epoch[7/10](6340/14999) || training loss 0.0636 || training accuracy 97.50% || lr 2.0687403839721717e-06\n","Epoch[7/10](6360/14999) || training loss 0.02891 || training accuracy 98.75% || lr 2.067670078266105e-06\n","Epoch[7/10](6380/14999) || training loss 0.03038 || training accuracy 98.12% || lr 2.0665997725600373e-06\n","Epoch[7/10](6400/14999) || training loss 0.03837 || training accuracy 99.38% || lr 2.06552946685397e-06\n","Epoch[7/10](6420/14999) || training loss 0.07137 || training accuracy 97.50% || lr 2.064459161147903e-06\n","Epoch[7/10](6440/14999) || training loss 0.04074 || training accuracy 98.75% || lr 2.063388855441835e-06\n","Epoch[7/10](6460/14999) || training loss 0.1107 || training accuracy 96.25% || lr 2.0623185497357684e-06\n","Epoch[7/10](6480/14999) || training loss 0.06438 || training accuracy 97.50% || lr 2.0612482440297007e-06\n","Epoch[7/10](6500/14999) || training loss 0.06563 || training accuracy 96.88% || lr 2.060177938323634e-06\n","Epoch[7/10](6520/14999) || training loss 0.05734 || training accuracy 97.50% || lr 2.0591076326175663e-06\n","Epoch[7/10](6540/14999) || training loss 0.0217 || training accuracy 99.38% || lr 2.0580373269114986e-06\n","Epoch[7/10](6560/14999) || training loss 0.09582 || training accuracy 96.88% || lr 2.056967021205432e-06\n","Epoch[7/10](6580/14999) || training loss 0.0769 || training accuracy 97.50% || lr 2.055896715499364e-06\n","Epoch[7/10](6600/14999) || training loss 0.06785 || training accuracy 98.75% || lr 2.0548264097932974e-06\n","Epoch[7/10](6620/14999) || training loss 0.06299 || training accuracy 97.50% || lr 2.0537561040872297e-06\n","Epoch[7/10](6640/14999) || training loss 0.1186 || training accuracy 96.88% || lr 2.052685798381163e-06\n","Epoch[7/10](6660/14999) || training loss 0.02146 || training accuracy 100.00% || lr 2.0516154926750953e-06\n","Epoch[7/10](6680/14999) || training loss 0.04194 || training accuracy 99.38% || lr 2.0505451869690276e-06\n","Epoch[7/10](6700/14999) || training loss 0.02061 || training accuracy 100.00% || lr 2.049474881262961e-06\n","Epoch[7/10](6720/14999) || training loss 0.0185 || training accuracy 99.38% || lr 2.048404575556893e-06\n","Epoch[7/10](6740/14999) || training loss 0.05017 || training accuracy 97.50% || lr 2.0473342698508264e-06\n","Epoch[7/10](6760/14999) || training loss 0.07908 || training accuracy 98.12% || lr 2.0462639641447587e-06\n","Epoch[7/10](6780/14999) || training loss 0.08699 || training accuracy 98.12% || lr 2.0451936584386915e-06\n","Epoch[7/10](6800/14999) || training loss 0.08186 || training accuracy 96.88% || lr 2.0441233527326243e-06\n","Epoch[7/10](6820/14999) || training loss 0.02113 || training accuracy 99.38% || lr 2.0430530470265566e-06\n","Epoch[7/10](6840/14999) || training loss 0.02515 || training accuracy 98.75% || lr 2.04198274132049e-06\n","Epoch[7/10](6860/14999) || training loss 0.04023 || training accuracy 98.75% || lr 2.040912435614422e-06\n","Epoch[7/10](6880/14999) || training loss 0.04203 || training accuracy 98.12% || lr 2.039842129908355e-06\n","Epoch[7/10](6900/14999) || training loss 0.1009 || training accuracy 95.62% || lr 2.0387718242022877e-06\n","Epoch[7/10](6920/14999) || training loss 0.02962 || training accuracy 98.75% || lr 2.0377015184962205e-06\n","Epoch[7/10](6940/14999) || training loss 0.05474 || training accuracy 99.38% || lr 2.0366312127901533e-06\n","Epoch[7/10](6960/14999) || training loss 0.09025 || training accuracy 96.88% || lr 2.0355609070840856e-06\n","Epoch[7/10](6980/14999) || training loss 0.05816 || training accuracy 98.12% || lr 2.0344906013780184e-06\n","Epoch[7/10](7000/14999) || training loss 0.05949 || training accuracy 97.50% || lr 2.033420295671951e-06\n","Epoch[7/10](7020/14999) || training loss 0.1285 || training accuracy 96.25% || lr 2.032349989965884e-06\n","Epoch[7/10](7040/14999) || training loss 0.09782 || training accuracy 96.88% || lr 2.0312796842598167e-06\n","Epoch[7/10](7060/14999) || training loss 0.05706 || training accuracy 97.50% || lr 2.0302093785537495e-06\n","Epoch[7/10](7080/14999) || training loss 0.08264 || training accuracy 97.50% || lr 2.029139072847682e-06\n","Epoch[7/10](7100/14999) || training loss 0.0455 || training accuracy 99.38% || lr 2.0280687671416146e-06\n","Epoch[7/10](7120/14999) || training loss 0.08964 || training accuracy 97.50% || lr 2.0269984614355474e-06\n","Epoch[7/10](7140/14999) || training loss 0.03159 || training accuracy 98.75% || lr 2.02592815572948e-06\n","Epoch[7/10](7160/14999) || training loss 0.09197 || training accuracy 97.50% || lr 2.024857850023413e-06\n","Epoch[7/10](7180/14999) || training loss 0.06996 || training accuracy 98.12% || lr 2.0237875443173453e-06\n","Epoch[7/10](7200/14999) || training loss 0.01922 || training accuracy 99.38% || lr 2.0227172386112785e-06\n","Epoch[7/10](7220/14999) || training loss 0.02022 || training accuracy 98.75% || lr 2.021646932905211e-06\n","Epoch[7/10](7240/14999) || training loss 0.04376 || training accuracy 98.75% || lr 2.0205766271991436e-06\n","Epoch[7/10](7260/14999) || training loss 0.02679 || training accuracy 99.38% || lr 2.0195063214930764e-06\n","Epoch[7/10](7280/14999) || training loss 0.05915 || training accuracy 97.50% || lr 2.018436015787009e-06\n","Epoch[7/10](7300/14999) || training loss 0.08543 || training accuracy 97.50% || lr 2.017365710080942e-06\n","Epoch[7/10](7320/14999) || training loss 0.03407 || training accuracy 98.75% || lr 2.0162954043748743e-06\n","Epoch[7/10](7340/14999) || training loss 0.04667 || training accuracy 98.12% || lr 2.0152250986688075e-06\n","Epoch[7/10](7360/14999) || training loss 0.06543 || training accuracy 99.38% || lr 2.01415479296274e-06\n","Epoch[7/10](7380/14999) || training loss 0.04286 || training accuracy 97.50% || lr 2.013084487256673e-06\n","Epoch[7/10](7400/14999) || training loss 0.01858 || training accuracy 100.00% || lr 2.0120141815506054e-06\n","Epoch[7/10](7420/14999) || training loss 0.09176 || training accuracy 96.88% || lr 2.0109438758445377e-06\n","Epoch[7/10](7440/14999) || training loss 0.02982 || training accuracy 99.38% || lr 2.009873570138471e-06\n","Epoch[7/10](7460/14999) || training loss 0.04386 || training accuracy 98.12% || lr 2.0088032644324032e-06\n","Epoch[7/10](7480/14999) || training loss 0.008639 || training accuracy 100.00% || lr 2.0077329587263364e-06\n","Epoch[7/10](7500/14999) || training loss 0.05546 || training accuracy 98.75% || lr 2.006662653020269e-06\n","Epoch[7/10](7520/14999) || training loss 0.06919 || training accuracy 97.50% || lr 2.005592347314201e-06\n","Epoch[7/10](7540/14999) || training loss 0.02571 || training accuracy 98.12% || lr 2.0045220416081343e-06\n","Epoch[7/10](7560/14999) || training loss 0.05784 || training accuracy 97.50% || lr 2.0034517359020667e-06\n","Epoch[7/10](7580/14999) || training loss 0.09365 || training accuracy 94.38% || lr 2.002381430196e-06\n","Epoch[7/10](7600/14999) || training loss 0.05005 || training accuracy 97.50% || lr 2.0013111244899322e-06\n","Epoch[7/10](7620/14999) || training loss 0.02744 || training accuracy 98.75% || lr 2.000240818783865e-06\n","Epoch[7/10](7640/14999) || training loss 0.05336 || training accuracy 98.75% || lr 1.9991705130777978e-06\n","Epoch[7/10](7660/14999) || training loss 0.1412 || training accuracy 95.00% || lr 1.9981002073717306e-06\n","Epoch[7/10](7680/14999) || training loss 0.04225 || training accuracy 98.75% || lr 1.997029901665663e-06\n","Epoch[7/10](7700/14999) || training loss 0.02724 || training accuracy 99.38% || lr 1.9959595959595957e-06\n","Epoch[7/10](7720/14999) || training loss 0.08151 || training accuracy 98.12% || lr 1.9948892902535285e-06\n","Epoch[7/10](7740/14999) || training loss 0.06194 || training accuracy 97.50% || lr 1.9938189845474612e-06\n","Epoch[7/10](7760/14999) || training loss 0.08846 || training accuracy 96.88% || lr 1.992748678841394e-06\n","Epoch[7/10](7780/14999) || training loss 0.05998 || training accuracy 97.50% || lr 1.9916783731353268e-06\n","Epoch[7/10](7800/14999) || training loss 0.0648 || training accuracy 98.12% || lr 1.9906080674292596e-06\n","Epoch[7/10](7820/14999) || training loss 0.01706 || training accuracy 99.38% || lr 1.989537761723192e-06\n","Epoch[7/10](7840/14999) || training loss 0.02215 || training accuracy 98.75% || lr 1.9884674560171247e-06\n","Epoch[7/10](7860/14999) || training loss 0.0141 || training accuracy 99.38% || lr 1.9873971503110575e-06\n","Epoch[7/10](7880/14999) || training loss 0.1253 || training accuracy 97.50% || lr 1.9863268446049902e-06\n","Epoch[7/10](7900/14999) || training loss 0.07113 || training accuracy 98.12% || lr 1.985256538898923e-06\n","Epoch[7/10](7920/14999) || training loss 0.07542 || training accuracy 96.88% || lr 1.9841862331928558e-06\n","Epoch[7/10](7940/14999) || training loss 0.0575 || training accuracy 98.12% || lr 1.9831159274867885e-06\n","Epoch[7/10](7960/14999) || training loss 0.07103 || training accuracy 97.50% || lr 1.982045621780721e-06\n","Epoch[7/10](7980/14999) || training loss 0.05269 || training accuracy 98.12% || lr 1.9809753160746537e-06\n","Epoch[7/10](8000/14999) || training loss 0.08428 || training accuracy 98.12% || lr 1.9799050103685864e-06\n","Epoch[7/10](8020/14999) || training loss 0.02832 || training accuracy 99.38% || lr 1.9788347046625192e-06\n","Epoch[7/10](8040/14999) || training loss 0.08337 || training accuracy 97.50% || lr 1.977764398956452e-06\n","Epoch[7/10](8060/14999) || training loss 0.05362 || training accuracy 97.50% || lr 1.9766940932503848e-06\n","Epoch[7/10](8080/14999) || training loss 0.02588 || training accuracy 99.38% || lr 1.9756237875443175e-06\n","Epoch[7/10](8100/14999) || training loss 0.07181 || training accuracy 98.12% || lr 1.97455348183825e-06\n","Epoch[7/10](8120/14999) || training loss 0.04966 || training accuracy 98.75% || lr 1.9734831761321827e-06\n","Epoch[7/10](8140/14999) || training loss 0.02453 || training accuracy 98.75% || lr 1.9724128704261154e-06\n","Epoch[7/10](8160/14999) || training loss 0.04769 || training accuracy 96.88% || lr 1.971342564720048e-06\n","Epoch[7/10](8180/14999) || training loss 0.05969 || training accuracy 96.88% || lr 1.970272259013981e-06\n","Epoch[7/10](8200/14999) || training loss 0.07568 || training accuracy 98.12% || lr 1.9692019533079133e-06\n","Epoch[7/10](8220/14999) || training loss 0.1021 || training accuracy 95.62% || lr 1.968131647601846e-06\n","Epoch[7/10](8240/14999) || training loss 0.09789 || training accuracy 96.88% || lr 1.967061341895779e-06\n","Epoch[7/10](8260/14999) || training loss 0.0185 || training accuracy 100.00% || lr 1.9659910361897117e-06\n","Epoch[7/10](8280/14999) || training loss 0.07793 || training accuracy 98.12% || lr 1.9649207304836444e-06\n","Epoch[7/10](8300/14999) || training loss 0.08122 || training accuracy 98.12% || lr 1.9638504247775768e-06\n","Epoch[7/10](8320/14999) || training loss 0.05956 || training accuracy 98.12% || lr 1.9627801190715096e-06\n","Epoch[7/10](8340/14999) || training loss 0.05044 || training accuracy 97.50% || lr 1.9617098133654423e-06\n","Epoch[7/10](8360/14999) || training loss 0.05622 || training accuracy 97.50% || lr 1.960639507659375e-06\n","Epoch[7/10](8380/14999) || training loss 0.06858 || training accuracy 96.88% || lr 1.959569201953308e-06\n","Epoch[7/10](8400/14999) || training loss 0.04965 || training accuracy 98.75% || lr 1.9584988962472406e-06\n","Epoch[7/10](8420/14999) || training loss 0.03097 || training accuracy 99.38% || lr 1.957428590541173e-06\n","Epoch[7/10](8440/14999) || training loss 0.02234 || training accuracy 99.38% || lr 1.9563582848351058e-06\n","Epoch[7/10](8460/14999) || training loss 0.1218 || training accuracy 95.62% || lr 1.9552879791290385e-06\n","Epoch[7/10](8480/14999) || training loss 0.03169 || training accuracy 99.38% || lr 1.9542176734229713e-06\n","Epoch[7/10](8500/14999) || training loss 0.04933 || training accuracy 98.12% || lr 1.953147367716904e-06\n","Epoch[7/10](8520/14999) || training loss 0.04048 || training accuracy 98.75% || lr 1.952077062010837e-06\n","Epoch[7/10](8540/14999) || training loss 0.03474 || training accuracy 98.75% || lr 1.9510067563047692e-06\n","Epoch[7/10](8560/14999) || training loss 0.1131 || training accuracy 97.50% || lr 1.949936450598702e-06\n","Epoch[7/10](8580/14999) || training loss 0.08534 || training accuracy 96.88% || lr 1.9488661448926348e-06\n","Epoch[7/10](8600/14999) || training loss 0.02569 || training accuracy 99.38% || lr 1.9477958391865675e-06\n","Epoch[7/10](8620/14999) || training loss 0.03794 || training accuracy 99.38% || lr 1.9467255334805003e-06\n","Epoch[7/10](8640/14999) || training loss 0.0294 || training accuracy 98.12% || lr 1.945655227774433e-06\n","Epoch[7/10](8660/14999) || training loss 0.05794 || training accuracy 99.38% || lr 1.944584922068366e-06\n","Epoch[7/10](8680/14999) || training loss 0.05117 || training accuracy 98.12% || lr 1.943514616362298e-06\n","Epoch[7/10](8700/14999) || training loss 0.04604 || training accuracy 97.50% || lr 1.942444310656231e-06\n","Epoch[7/10](8720/14999) || training loss 0.04787 || training accuracy 98.75% || lr 1.9413740049501638e-06\n","Epoch[7/10](8740/14999) || training loss 0.05797 || training accuracy 99.38% || lr 1.9403036992440965e-06\n","Epoch[7/10](8760/14999) || training loss 0.07667 || training accuracy 98.12% || lr 1.9392333935380293e-06\n","Epoch[7/10](8780/14999) || training loss 0.08833 || training accuracy 98.12% || lr 1.938163087831962e-06\n","Epoch[7/10](8800/14999) || training loss 0.03827 || training accuracy 99.38% || lr 1.937092782125895e-06\n","Epoch[7/10](8820/14999) || training loss 0.05539 || training accuracy 98.12% || lr 1.9360224764198276e-06\n","Epoch[7/10](8840/14999) || training loss 0.03358 || training accuracy 99.38% || lr 1.93495217071376e-06\n","Epoch[7/10](8860/14999) || training loss 0.08637 || training accuracy 96.25% || lr 1.9338818650076927e-06\n","Epoch[7/10](8880/14999) || training loss 0.04871 || training accuracy 98.12% || lr 1.9328115593016255e-06\n","Epoch[7/10](8900/14999) || training loss 0.0686 || training accuracy 98.75% || lr 1.9317412535955583e-06\n","Epoch[7/10](8920/14999) || training loss 0.0296 || training accuracy 99.38% || lr 1.930670947889491e-06\n","Epoch[7/10](8940/14999) || training loss 0.03032 || training accuracy 98.12% || lr 1.9296006421834234e-06\n","Epoch[7/10](8960/14999) || training loss 0.07795 || training accuracy 98.12% || lr 1.928530336477356e-06\n","Epoch[7/10](8980/14999) || training loss 0.0298 || training accuracy 99.38% || lr 1.927460030771289e-06\n","Epoch[7/10](9000/14999) || training loss 0.02757 || training accuracy 99.38% || lr 1.9263897250652217e-06\n","Epoch[7/10](9020/14999) || training loss 0.01943 || training accuracy 100.00% || lr 1.9253194193591545e-06\n","Epoch[7/10](9040/14999) || training loss 0.04097 || training accuracy 99.38% || lr 1.9242491136530873e-06\n","Epoch[7/10](9060/14999) || training loss 0.01088 || training accuracy 99.38% || lr 1.9231788079470196e-06\n","Epoch[7/10](9080/14999) || training loss 0.05524 || training accuracy 98.12% || lr 1.9221085022409524e-06\n","Epoch[7/10](9100/14999) || training loss 0.07219 || training accuracy 98.12% || lr 1.921038196534885e-06\n","Epoch[7/10](9120/14999) || training loss 0.07844 || training accuracy 98.12% || lr 1.919967890828818e-06\n","Epoch[7/10](9140/14999) || training loss 0.06192 || training accuracy 98.12% || lr 1.9188975851227507e-06\n","Epoch[7/10](9160/14999) || training loss 0.02947 || training accuracy 98.12% || lr 1.917827279416683e-06\n","Epoch[7/10](9180/14999) || training loss 0.08536 || training accuracy 98.12% || lr 1.916756973710616e-06\n","Epoch[7/10](9200/14999) || training loss 0.09208 || training accuracy 96.25% || lr 1.9156866680045486e-06\n","Epoch[7/10](9220/14999) || training loss 0.01748 || training accuracy 99.38% || lr 1.9146163622984814e-06\n","Epoch[7/10](9240/14999) || training loss 0.05553 || training accuracy 97.50% || lr 1.913546056592414e-06\n","Epoch[7/10](9260/14999) || training loss 0.07812 || training accuracy 97.50% || lr 1.9124757508863465e-06\n","Epoch[7/10](9280/14999) || training loss 0.06188 || training accuracy 97.50% || lr 1.9114054451802793e-06\n","Epoch[7/10](9300/14999) || training loss 0.02761 || training accuracy 98.75% || lr 1.910335139474212e-06\n","Epoch[7/10](9320/14999) || training loss 0.05303 || training accuracy 98.12% || lr 1.909264833768145e-06\n","Epoch[7/10](9340/14999) || training loss 0.04528 || training accuracy 98.12% || lr 1.9081945280620776e-06\n","Epoch[7/10](9360/14999) || training loss 0.07934 || training accuracy 97.50% || lr 1.9071242223560104e-06\n","Epoch[7/10](9380/14999) || training loss 0.06895 || training accuracy 98.12% || lr 1.906053916649943e-06\n","Epoch[7/10](9400/14999) || training loss 0.06705 || training accuracy 98.75% || lr 1.9049836109438757e-06\n","Epoch[7/10](9420/14999) || training loss 0.04967 || training accuracy 98.75% || lr 1.9039133052378085e-06\n","Epoch[7/10](9440/14999) || training loss 0.02714 || training accuracy 98.75% || lr 1.9028429995317413e-06\n","Epoch[7/10](9460/14999) || training loss 0.063 || training accuracy 98.75% || lr 1.901772693825674e-06\n","Epoch[7/10](9480/14999) || training loss 0.07158 || training accuracy 96.88% || lr 1.9007023881196064e-06\n","Epoch[7/10](9500/14999) || training loss 0.03586 || training accuracy 98.12% || lr 1.8996320824135392e-06\n","Epoch[7/10](9520/14999) || training loss 0.02863 || training accuracy 99.38% || lr 1.898561776707472e-06\n","Epoch[7/10](9540/14999) || training loss 0.01656 || training accuracy 99.38% || lr 1.8974914710014047e-06\n","Epoch[7/10](9560/14999) || training loss 0.0545 || training accuracy 98.12% || lr 1.8964211652953375e-06\n","Epoch[7/10](9580/14999) || training loss 0.02664 || training accuracy 99.38% || lr 1.89535085958927e-06\n","Epoch[7/10](9600/14999) || training loss 0.06943 || training accuracy 97.50% || lr 1.8942805538832028e-06\n","Epoch[7/10](9620/14999) || training loss 0.01666 || training accuracy 100.00% || lr 1.8932102481771354e-06\n","Epoch[7/10](9640/14999) || training loss 0.0901 || training accuracy 96.88% || lr 1.8921399424710682e-06\n","Epoch[7/10](9660/14999) || training loss 0.03703 || training accuracy 98.75% || lr 1.891069636765001e-06\n","Epoch[7/10](9680/14999) || training loss 0.06944 || training accuracy 97.50% || lr 1.8899993310589337e-06\n","Epoch[7/10](9700/14999) || training loss 0.09277 || training accuracy 96.88% || lr 1.8889290253528663e-06\n","Epoch[7/10](9720/14999) || training loss 0.03295 || training accuracy 98.75% || lr 1.887858719646799e-06\n","Epoch[7/10](9740/14999) || training loss 0.06766 || training accuracy 98.12% || lr 1.8867884139407318e-06\n","Epoch[7/10](9760/14999) || training loss 0.01626 || training accuracy 99.38% || lr 1.8857181082346644e-06\n","Epoch[7/10](9780/14999) || training loss 0.1092 || training accuracy 96.25% || lr 1.8846478025285972e-06\n","Epoch[7/10](9800/14999) || training loss 0.07817 || training accuracy 98.12% || lr 1.8835774968225297e-06\n","Epoch[7/10](9820/14999) || training loss 0.01284 || training accuracy 99.38% || lr 1.8825071911164625e-06\n","Epoch[7/10](9840/14999) || training loss 0.09376 || training accuracy 96.25% || lr 1.8814368854103953e-06\n","Epoch[7/10](9860/14999) || training loss 0.04529 || training accuracy 98.12% || lr 1.880366579704328e-06\n","Epoch[7/10](9880/14999) || training loss 0.1004 || training accuracy 96.25% || lr 1.8792962739982608e-06\n","Epoch[7/10](9900/14999) || training loss 0.03529 || training accuracy 99.38% || lr 1.8782259682921932e-06\n","Epoch[7/10](9920/14999) || training loss 0.04415 || training accuracy 98.12% || lr 1.877155662586126e-06\n","Epoch[7/10](9940/14999) || training loss 0.05254 || training accuracy 98.75% || lr 1.8760853568800587e-06\n","Epoch[7/10](9960/14999) || training loss 0.07003 || training accuracy 96.25% || lr 1.8750150511739915e-06\n","Epoch[7/10](9980/14999) || training loss 0.063 || training accuracy 98.75% || lr 1.8739447454679243e-06\n","Epoch[7/10](10000/14999) || training loss 0.07371 || training accuracy 97.50% || lr 1.872874439761857e-06\n","Epoch[7/10](10020/14999) || training loss 0.06814 || training accuracy 96.88% || lr 1.8718041340557896e-06\n","Epoch[7/10](10040/14999) || training loss 0.03611 || training accuracy 98.75% || lr 1.8707338283497224e-06\n","Epoch[7/10](10060/14999) || training loss 0.07036 || training accuracy 98.12% || lr 1.869663522643655e-06\n","Epoch[7/10](10080/14999) || training loss 0.01543 || training accuracy 100.00% || lr 1.8685932169375877e-06\n","Epoch[7/10](10100/14999) || training loss 0.04306 || training accuracy 98.75% || lr 1.8675229112315205e-06\n","Epoch[7/10](10120/14999) || training loss 0.05668 || training accuracy 98.12% || lr 1.866452605525453e-06\n","Epoch[7/10](10140/14999) || training loss 0.02742 || training accuracy 98.75% || lr 1.8653822998193858e-06\n","Epoch[7/10](10160/14999) || training loss 0.01762 || training accuracy 100.00% || lr 1.8643119941133186e-06\n","Epoch[7/10](10180/14999) || training loss 0.0278 || training accuracy 99.38% || lr 1.8632416884072514e-06\n","Epoch[7/10](10200/14999) || training loss 0.02583 || training accuracy 98.12% || lr 1.862171382701184e-06\n","Epoch[7/10](10220/14999) || training loss 0.05537 || training accuracy 98.12% || lr 1.8611010769951165e-06\n","Epoch[7/10](10240/14999) || training loss 0.06874 || training accuracy 98.12% || lr 1.8600307712890493e-06\n","Epoch[7/10](10260/14999) || training loss 0.07957 || training accuracy 98.12% || lr 1.858960465582982e-06\n","Epoch[7/10](10280/14999) || training loss 0.09181 || training accuracy 96.88% || lr 1.8578901598769148e-06\n","Epoch[7/10](10300/14999) || training loss 0.05879 || training accuracy 98.12% || lr 1.8568198541708476e-06\n","Epoch[7/10](10320/14999) || training loss 0.0452 || training accuracy 98.75% || lr 1.8557495484647804e-06\n","Epoch[7/10](10340/14999) || training loss 0.03875 || training accuracy 99.38% || lr 1.8546792427587127e-06\n","Epoch[7/10](10360/14999) || training loss 0.09502 || training accuracy 96.88% || lr 1.8536089370526455e-06\n","Epoch[7/10](10380/14999) || training loss 0.08938 || training accuracy 96.88% || lr 1.8525386313465782e-06\n","Epoch[7/10](10400/14999) || training loss 0.06876 || training accuracy 96.88% || lr 1.851468325640511e-06\n","Epoch[7/10](10420/14999) || training loss 0.02658 || training accuracy 98.75% || lr 1.8503980199344438e-06\n","Epoch[7/10](10440/14999) || training loss 0.06839 || training accuracy 98.12% || lr 1.8493277142283764e-06\n","Epoch[7/10](10460/14999) || training loss 0.0359 || training accuracy 99.38% || lr 1.8482574085223091e-06\n","Epoch[7/10](10480/14999) || training loss 0.03342 || training accuracy 98.75% || lr 1.8471871028162417e-06\n","Epoch[7/10](10500/14999) || training loss 0.009654 || training accuracy 100.00% || lr 1.8461167971101745e-06\n","Epoch[7/10](10520/14999) || training loss 0.04086 || training accuracy 98.12% || lr 1.8450464914041072e-06\n","Epoch[7/10](10540/14999) || training loss 0.1044 || training accuracy 96.88% || lr 1.8439761856980398e-06\n","Epoch[7/10](10560/14999) || training loss 0.02196 || training accuracy 99.38% || lr 1.8429058799919726e-06\n","Epoch[7/10](10580/14999) || training loss 0.09714 || training accuracy 98.12% || lr 1.8418355742859053e-06\n","Epoch[7/10](10600/14999) || training loss 0.0994 || training accuracy 96.88% || lr 1.8407652685798381e-06\n","Epoch[7/10](10620/14999) || training loss 0.09813 || training accuracy 96.25% || lr 1.839694962873771e-06\n","Epoch[7/10](10640/14999) || training loss 0.09332 || training accuracy 97.50% || lr 1.8386246571677035e-06\n","Epoch[7/10](10660/14999) || training loss 0.02793 || training accuracy 99.38% || lr 1.837554351461636e-06\n","Epoch[7/10](10680/14999) || training loss 0.1246 || training accuracy 96.25% || lr 1.8364840457555688e-06\n","Epoch[7/10](10700/14999) || training loss 0.133 || training accuracy 96.25% || lr 1.8354137400495016e-06\n","Epoch[7/10](10720/14999) || training loss 0.04181 || training accuracy 98.75% || lr 1.8343434343434343e-06\n","Epoch[7/10](10740/14999) || training loss 0.0429 || training accuracy 98.12% || lr 1.8332731286373671e-06\n","Epoch[7/10](10760/14999) || training loss 0.02129 || training accuracy 99.38% || lr 1.8322028229312997e-06\n","Epoch[7/10](10780/14999) || training loss 0.01994 || training accuracy 98.75% || lr 1.8311325172252322e-06\n","Epoch[7/10](10800/14999) || training loss 0.04486 || training accuracy 97.50% || lr 1.830062211519165e-06\n","Epoch[7/10](10820/14999) || training loss 0.02724 || training accuracy 99.38% || lr 1.8289919058130978e-06\n","Epoch[7/10](10840/14999) || training loss 0.03091 || training accuracy 99.38% || lr 1.8279216001070306e-06\n","Epoch[7/10](10860/14999) || training loss 0.03665 || training accuracy 97.50% || lr 1.8268512944009631e-06\n","Epoch[7/10](10880/14999) || training loss 0.0767 || training accuracy 98.75% || lr 1.825780988694896e-06\n","Epoch[7/10](10900/14999) || training loss 0.05344 || training accuracy 98.12% || lr 1.8247106829888287e-06\n","Epoch[7/10](10920/14999) || training loss 0.0618 || training accuracy 98.75% || lr 1.8236403772827612e-06\n","Epoch[7/10](10940/14999) || training loss 0.04398 || training accuracy 98.12% || lr 1.822570071576694e-06\n","Epoch[7/10](10960/14999) || training loss 0.04827 || training accuracy 98.12% || lr 1.8214997658706268e-06\n","Epoch[7/10](10980/14999) || training loss 0.08819 || training accuracy 96.88% || lr 1.8204294601645593e-06\n","Epoch[7/10](11000/14999) || training loss 0.03869 || training accuracy 98.12% || lr 1.8193591544584921e-06\n","Epoch[7/10](11020/14999) || training loss 0.05653 || training accuracy 98.12% || lr 1.8182888487524249e-06\n","Epoch[7/10](11040/14999) || training loss 0.06018 || training accuracy 98.75% || lr 1.8172185430463577e-06\n","Epoch[7/10](11060/14999) || training loss 0.04314 || training accuracy 98.75% || lr 1.8161482373402902e-06\n","Epoch[7/10](11080/14999) || training loss 0.0521 || training accuracy 96.88% || lr 1.8150779316342228e-06\n","Epoch[7/10](11100/14999) || training loss 0.0176 || training accuracy 99.38% || lr 1.8140076259281556e-06\n","Epoch[7/10](11120/14999) || training loss 0.02126 || training accuracy 99.38% || lr 1.8129373202220883e-06\n","Epoch[7/10](11140/14999) || training loss 0.1064 || training accuracy 96.88% || lr 1.8118670145160211e-06\n","Epoch[7/10](11160/14999) || training loss 0.04144 || training accuracy 98.12% || lr 1.8107967088099539e-06\n","Epoch[7/10](11180/14999) || training loss 0.02254 || training accuracy 99.38% || lr 1.8097264031038864e-06\n","Epoch[7/10](11200/14999) || training loss 0.02145 || training accuracy 99.38% || lr 1.808656097397819e-06\n","Epoch[7/10](11220/14999) || training loss 0.06829 || training accuracy 98.75% || lr 1.8075857916917518e-06\n","Epoch[7/10](11240/14999) || training loss 0.05048 || training accuracy 98.75% || lr 1.8065154859856846e-06\n","Epoch[7/10](11260/14999) || training loss 0.06107 || training accuracy 96.25% || lr 1.8054451802796173e-06\n","Epoch[7/10](11280/14999) || training loss 0.02735 || training accuracy 98.75% || lr 1.80437487457355e-06\n","Epoch[7/10](11300/14999) || training loss 0.1084 || training accuracy 95.62% || lr 1.8033045688674827e-06\n","Epoch[7/10](11320/14999) || training loss 0.04969 || training accuracy 98.75% || lr 1.8022342631614154e-06\n","Epoch[7/10](11340/14999) || training loss 0.021 || training accuracy 100.00% || lr 1.8011639574553482e-06\n","Epoch[7/10](11360/14999) || training loss 0.05961 || training accuracy 98.12% || lr 1.8000936517492808e-06\n","Epoch[7/10](11380/14999) || training loss 0.06594 || training accuracy 96.88% || lr 1.7990233460432135e-06\n","Epoch[7/10](11400/14999) || training loss 0.03068 || training accuracy 99.38% || lr 1.797953040337146e-06\n","Epoch[7/10](11420/14999) || training loss 0.01292 || training accuracy 100.00% || lr 1.7968827346310789e-06\n","Epoch[7/10](11440/14999) || training loss 0.02763 || training accuracy 98.75% || lr 1.7958124289250117e-06\n","Epoch[7/10](11460/14999) || training loss 0.07967 || training accuracy 97.50% || lr 1.7947421232189444e-06\n","Epoch[7/10](11480/14999) || training loss 0.04183 || training accuracy 98.12% || lr 1.7936718175128772e-06\n","Epoch[7/10](11500/14999) || training loss 0.08732 || training accuracy 96.25% || lr 1.7926015118068095e-06\n","Epoch[7/10](11520/14999) || training loss 0.06443 || training accuracy 98.12% || lr 1.7915312061007423e-06\n","Epoch[7/10](11540/14999) || training loss 0.06619 || training accuracy 99.38% || lr 1.790460900394675e-06\n","Epoch[7/10](11560/14999) || training loss 0.1124 || training accuracy 96.88% || lr 1.7893905946886079e-06\n","Epoch[7/10](11580/14999) || training loss 0.01848 || training accuracy 100.00% || lr 1.7883202889825406e-06\n","Epoch[7/10](11600/14999) || training loss 0.1357 || training accuracy 96.88% || lr 1.7872499832764734e-06\n","Epoch[7/10](11620/14999) || training loss 0.07906 || training accuracy 98.12% || lr 1.786179677570406e-06\n","Epoch[7/10](11640/14999) || training loss 0.062 || training accuracy 98.12% || lr 1.7851093718643385e-06\n","Epoch[7/10](11660/14999) || training loss 0.03948 || training accuracy 98.12% || lr 1.7840390661582713e-06\n","Epoch[7/10](11680/14999) || training loss 0.05652 || training accuracy 98.12% || lr 1.782968760452204e-06\n","Epoch[7/10](11700/14999) || training loss 0.04421 || training accuracy 98.75% || lr 1.7818984547461369e-06\n","Epoch[7/10](11720/14999) || training loss 0.02886 || training accuracy 98.75% || lr 1.7808281490400694e-06\n","Epoch[7/10](11740/14999) || training loss 0.05372 || training accuracy 99.38% || lr 1.7797578433340022e-06\n","Epoch[7/10](11760/14999) || training loss 0.0248 || training accuracy 99.38% || lr 1.778687537627935e-06\n","Epoch[7/10](11780/14999) || training loss 0.1131 || training accuracy 97.50% || lr 1.7776172319218675e-06\n","Epoch[7/10](11800/14999) || training loss 0.06761 || training accuracy 98.12% || lr 1.7765469262158003e-06\n","Epoch[7/10](11820/14999) || training loss 0.1152 || training accuracy 95.00% || lr 1.7754766205097329e-06\n","Epoch[7/10](11840/14999) || training loss 0.07556 || training accuracy 97.50% || lr 1.7744063148036656e-06\n","Epoch[7/10](11860/14999) || training loss 0.04951 || training accuracy 98.12% || lr 1.7733360090975984e-06\n","Epoch[7/10](11880/14999) || training loss 0.02098 || training accuracy 99.38% || lr 1.7722657033915312e-06\n","Epoch[7/10](11900/14999) || training loss 0.05531 || training accuracy 97.50% || lr 1.771195397685464e-06\n","Epoch[7/10](11920/14999) || training loss 0.02652 || training accuracy 98.75% || lr 1.7701250919793967e-06\n","Epoch[7/10](11940/14999) || training loss 0.08251 || training accuracy 96.88% || lr 1.769054786273329e-06\n","Epoch[7/10](11960/14999) || training loss 0.04229 || training accuracy 98.12% || lr 1.7679844805672619e-06\n","Epoch[7/10](11980/14999) || training loss 0.08612 || training accuracy 97.50% || lr 1.7669141748611946e-06\n","Epoch[7/10](12000/14999) || training loss 0.05005 || training accuracy 97.50% || lr 1.7658438691551274e-06\n","Epoch[7/10](12020/14999) || training loss 0.02293 || training accuracy 99.38% || lr 1.7647735634490602e-06\n","Epoch[7/10](12040/14999) || training loss 0.04793 || training accuracy 98.75% || lr 1.7637032577429927e-06\n","Epoch[7/10](12060/14999) || training loss 0.04564 || training accuracy 98.75% || lr 1.7626329520369255e-06\n","Epoch[7/10](12080/14999) || training loss 0.03085 || training accuracy 98.75% || lr 1.761562646330858e-06\n","Epoch[7/10](12100/14999) || training loss 0.03315 || training accuracy 98.75% || lr 1.7604923406247909e-06\n","Epoch[7/10](12120/14999) || training loss 0.01856 || training accuracy 99.38% || lr 1.7594220349187236e-06\n","Epoch[7/10](12140/14999) || training loss 0.03111 || training accuracy 98.75% || lr 1.7583517292126562e-06\n","Epoch[7/10](12160/14999) || training loss 0.02477 || training accuracy 99.38% || lr 1.757281423506589e-06\n","Epoch[7/10](12180/14999) || training loss 0.04142 || training accuracy 98.75% || lr 1.7562111178005217e-06\n","Epoch[7/10](12200/14999) || training loss 0.0649 || training accuracy 98.12% || lr 1.7551408120944545e-06\n","Epoch[7/10](12220/14999) || training loss 0.09692 || training accuracy 98.12% || lr 1.754070506388387e-06\n","Epoch[7/10](12240/14999) || training loss 0.05954 || training accuracy 98.12% || lr 1.7530002006823198e-06\n","Epoch[7/10](12260/14999) || training loss 0.1457 || training accuracy 96.88% || lr 1.7519298949762524e-06\n","Epoch[7/10](12280/14999) || training loss 0.02198 || training accuracy 99.38% || lr 1.7508595892701852e-06\n","Epoch[7/10](12300/14999) || training loss 0.03125 || training accuracy 98.75% || lr 1.749789283564118e-06\n","Epoch[7/10](12320/14999) || training loss 0.04734 || training accuracy 98.12% || lr 1.7487189778580507e-06\n","Epoch[7/10](12340/14999) || training loss 0.04548 || training accuracy 98.75% || lr 1.7476486721519835e-06\n","Epoch[7/10](12360/14999) || training loss 0.1083 || training accuracy 95.62% || lr 1.7465783664459159e-06\n","Epoch[7/10](12380/14999) || training loss 0.09953 || training accuracy 95.62% || lr 1.7455080607398486e-06\n","Epoch[7/10](12400/14999) || training loss 0.06514 || training accuracy 98.75% || lr 1.7444377550337814e-06\n","Epoch[7/10](12420/14999) || training loss 0.01726 || training accuracy 99.38% || lr 1.7433674493277142e-06\n","Epoch[7/10](12440/14999) || training loss 0.05518 || training accuracy 99.38% || lr 1.742297143621647e-06\n","Epoch[7/10](12460/14999) || training loss 0.03137 || training accuracy 98.75% || lr 1.7412268379155795e-06\n","Epoch[7/10](12480/14999) || training loss 0.07886 || training accuracy 98.75% || lr 1.7401565322095123e-06\n","Epoch[7/10](12500/14999) || training loss 0.07162 || training accuracy 98.12% || lr 1.7390862265034448e-06\n","Epoch[7/10](12520/14999) || training loss 0.06051 || training accuracy 98.12% || lr 1.7380159207973776e-06\n","Epoch[7/10](12540/14999) || training loss 0.07099 || training accuracy 98.12% || lr 1.7369456150913104e-06\n","Epoch[7/10](12560/14999) || training loss 0.02013 || training accuracy 100.00% || lr 1.7358753093852432e-06\n","Epoch[7/10](12580/14999) || training loss 0.03332 || training accuracy 98.12% || lr 1.7348050036791757e-06\n","Epoch[7/10](12600/14999) || training loss 0.07986 || training accuracy 97.50% || lr 1.7337346979731085e-06\n","Epoch[7/10](12620/14999) || training loss 0.06677 || training accuracy 97.50% || lr 1.7326643922670413e-06\n","Epoch[7/10](12640/14999) || training loss 0.04488 || training accuracy 98.12% || lr 1.731594086560974e-06\n","Epoch[7/10](12660/14999) || training loss 0.09721 || training accuracy 96.88% || lr 1.7305237808549066e-06\n","Epoch[7/10](12680/14999) || training loss 0.05249 || training accuracy 98.75% || lr 1.7294534751488392e-06\n","Epoch[7/10](12700/14999) || training loss 0.09251 || training accuracy 98.12% || lr 1.728383169442772e-06\n","Epoch[7/10](12720/14999) || training loss 0.04974 || training accuracy 98.75% || lr 1.7273128637367047e-06\n","Epoch[7/10](12740/14999) || training loss 0.06046 || training accuracy 97.50% || lr 1.7262425580306375e-06\n","Epoch[7/10](12760/14999) || training loss 0.03389 || training accuracy 99.38% || lr 1.7251722523245703e-06\n","Epoch[7/10](12780/14999) || training loss 0.02794 || training accuracy 99.38% || lr 1.7241019466185028e-06\n","Epoch[7/10](12800/14999) || training loss 0.03332 || training accuracy 98.12% || lr 1.7230316409124354e-06\n","Epoch[7/10](12820/14999) || training loss 0.07673 || training accuracy 98.12% || lr 1.7219613352063682e-06\n","Epoch[7/10](12840/14999) || training loss 0.03086 || training accuracy 98.75% || lr 1.720891029500301e-06\n","Epoch[7/10](12860/14999) || training loss 0.01175 || training accuracy 100.00% || lr 1.7198207237942337e-06\n","Epoch[7/10](12880/14999) || training loss 0.06637 || training accuracy 97.50% || lr 1.7187504180881665e-06\n","Epoch[7/10](12900/14999) || training loss 0.05793 || training accuracy 98.12% || lr 1.717680112382099e-06\n","Epoch[7/10](12920/14999) || training loss 0.07259 || training accuracy 98.12% || lr 1.7166098066760318e-06\n","Epoch[7/10](12940/14999) || training loss 0.08415 || training accuracy 96.88% || lr 1.7155395009699644e-06\n","Epoch[7/10](12960/14999) || training loss 0.03285 || training accuracy 99.38% || lr 1.7144691952638972e-06\n","Epoch[7/10](12980/14999) || training loss 0.06386 || training accuracy 98.75% || lr 1.71339888955783e-06\n","Epoch[7/10](13000/14999) || training loss 0.05482 || training accuracy 98.12% || lr 1.7123285838517625e-06\n","Epoch[7/10](13020/14999) || training loss 0.07072 || training accuracy 96.25% || lr 1.7112582781456953e-06\n","Epoch[7/10](13040/14999) || training loss 0.0416 || training accuracy 98.75% || lr 1.710187972439628e-06\n","Epoch[7/10](13060/14999) || training loss 0.06673 || training accuracy 97.50% || lr 1.7091176667335608e-06\n","Epoch[7/10](13080/14999) || training loss 0.06125 || training accuracy 98.12% || lr 1.7080473610274934e-06\n","Epoch[7/10](13100/14999) || training loss 0.06017 || training accuracy 98.75% || lr 1.706977055321426e-06\n","Epoch[7/10](13120/14999) || training loss 0.06537 || training accuracy 98.12% || lr 1.7059067496153587e-06\n","Epoch[7/10](13140/14999) || training loss 0.1399 || training accuracy 96.25% || lr 1.7048364439092915e-06\n","Epoch[7/10](13160/14999) || training loss 0.04919 || training accuracy 97.50% || lr 1.7037661382032243e-06\n","Epoch[7/10](13180/14999) || training loss 0.06249 || training accuracy 97.50% || lr 1.702695832497157e-06\n","Epoch[7/10](13200/14999) || training loss 0.04999 || training accuracy 98.12% || lr 1.7016255267910898e-06\n","Epoch[7/10](13220/14999) || training loss 0.1017 || training accuracy 96.88% || lr 1.7005552210850222e-06\n","Epoch[7/10](13240/14999) || training loss 0.0477 || training accuracy 97.50% || lr 1.699484915378955e-06\n","Epoch[7/10](13260/14999) || training loss 0.05615 || training accuracy 98.12% || lr 1.6984146096728877e-06\n","Epoch[7/10](13280/14999) || training loss 0.05945 || training accuracy 98.12% || lr 1.6973443039668205e-06\n","Epoch[7/10](13300/14999) || training loss 0.05535 || training accuracy 96.88% || lr 1.6962739982607532e-06\n","Epoch[7/10](13320/14999) || training loss 0.07674 || training accuracy 96.88% || lr 1.6952036925546858e-06\n","Epoch[7/10](13340/14999) || training loss 0.02099 || training accuracy 99.38% || lr 1.6941333868486186e-06\n","Epoch[7/10](13360/14999) || training loss 0.0474 || training accuracy 98.12% || lr 1.6930630811425514e-06\n","Epoch[7/10](13380/14999) || training loss 0.0801 || training accuracy 98.75% || lr 1.691992775436484e-06\n","Epoch[7/10](13400/14999) || training loss 0.02564 || training accuracy 99.38% || lr 1.6909224697304167e-06\n","Epoch[7/10](13420/14999) || training loss 0.1071 || training accuracy 96.88% || lr 1.6898521640243493e-06\n","Epoch[7/10](13440/14999) || training loss 0.02413 || training accuracy 99.38% || lr 1.688781858318282e-06\n","Epoch[7/10](13460/14999) || training loss 0.03853 || training accuracy 96.88% || lr 1.6877115526122148e-06\n","Epoch[7/10](13480/14999) || training loss 0.04872 || training accuracy 98.75% || lr 1.6866412469061476e-06\n","Epoch[7/10](13500/14999) || training loss 0.09613 || training accuracy 97.50% || lr 1.6855709412000803e-06\n","Epoch[7/10](13520/14999) || training loss 0.02279 || training accuracy 98.75% || lr 1.684500635494013e-06\n","Epoch[7/10](13540/14999) || training loss 0.04697 || training accuracy 98.75% || lr 1.6834303297879455e-06\n","Epoch[7/10](13560/14999) || training loss 0.03216 || training accuracy 99.38% || lr 1.6823600240818782e-06\n","Epoch[7/10](13580/14999) || training loss 0.04696 || training accuracy 98.12% || lr 1.681289718375811e-06\n","Epoch[7/10](13600/14999) || training loss 0.0567 || training accuracy 98.75% || lr 1.6802194126697438e-06\n","Epoch[7/10](13620/14999) || training loss 0.05027 || training accuracy 98.12% || lr 1.6791491069636766e-06\n","Epoch[7/10](13640/14999) || training loss 0.1022 || training accuracy 96.25% || lr 1.6780788012576091e-06\n","Epoch[7/10](13660/14999) || training loss 0.057 || training accuracy 98.75% || lr 1.6770084955515417e-06\n","Epoch[7/10](13680/14999) || training loss 0.07853 || training accuracy 97.50% || lr 1.6759381898454745e-06\n","Epoch[7/10](13700/14999) || training loss 0.1426 || training accuracy 95.62% || lr 1.6748678841394072e-06\n","Epoch[7/10](13720/14999) || training loss 0.05271 || training accuracy 99.38% || lr 1.67379757843334e-06\n","Epoch[7/10](13740/14999) || training loss 0.04871 || training accuracy 97.50% || lr 1.6727272727272726e-06\n","Epoch[7/10](13760/14999) || training loss 0.03684 || training accuracy 98.12% || lr 1.6716569670212053e-06\n","Epoch[7/10](13780/14999) || training loss 0.06861 || training accuracy 98.12% || lr 1.6705866613151381e-06\n","Epoch[7/10](13800/14999) || training loss 0.04384 || training accuracy 98.12% || lr 1.6695163556090707e-06\n","Epoch[7/10](13820/14999) || training loss 0.04184 || training accuracy 99.38% || lr 1.6684460499030035e-06\n","Epoch[7/10](13840/14999) || training loss 0.03641 || training accuracy 98.75% || lr 1.6673757441969362e-06\n","Epoch[7/10](13860/14999) || training loss 0.03165 || training accuracy 99.38% || lr 1.6663054384908688e-06\n","Epoch[7/10](13880/14999) || training loss 0.0184 || training accuracy 100.00% || lr 1.6652351327848016e-06\n","Epoch[7/10](13900/14999) || training loss 0.0257 || training accuracy 99.38% || lr 1.6641648270787343e-06\n","Epoch[7/10](13920/14999) || training loss 0.04271 || training accuracy 98.12% || lr 1.6630945213726671e-06\n","Epoch[7/10](13940/14999) || training loss 0.05603 || training accuracy 96.88% || lr 1.6620242156665999e-06\n","Epoch[7/10](13960/14999) || training loss 0.06713 || training accuracy 96.25% || lr 1.6609539099605322e-06\n","Epoch[7/10](13980/14999) || training loss 0.03631 || training accuracy 98.75% || lr 1.659883604254465e-06\n","Epoch[7/10](14000/14999) || training loss 0.02856 || training accuracy 99.38% || lr 1.6588132985483978e-06\n","Epoch[7/10](14020/14999) || training loss 0.0538 || training accuracy 98.12% || lr 1.6577429928423306e-06\n","Epoch[7/10](14040/14999) || training loss 0.05551 || training accuracy 97.50% || lr 1.6566726871362633e-06\n","Epoch[7/10](14060/14999) || training loss 0.03578 || training accuracy 98.75% || lr 1.655602381430196e-06\n","Epoch[7/10](14080/14999) || training loss 0.0167 || training accuracy 99.38% || lr 1.6545320757241287e-06\n","Epoch[7/10](14100/14999) || training loss 0.04829 || training accuracy 98.75% || lr 1.6534617700180612e-06\n","Epoch[7/10](14120/14999) || training loss 0.07659 || training accuracy 96.25% || lr 1.652391464311994e-06\n","Epoch[7/10](14140/14999) || training loss 0.03191 || training accuracy 99.38% || lr 1.6513211586059268e-06\n","Epoch[7/10](14160/14999) || training loss 0.06554 || training accuracy 97.50% || lr 1.6502508528998596e-06\n","Epoch[7/10](14180/14999) || training loss 0.04565 || training accuracy 98.75% || lr 1.6491805471937921e-06\n","Epoch[7/10](14200/14999) || training loss 0.06077 || training accuracy 98.75% || lr 1.6481102414877249e-06\n","Epoch[7/10](14220/14999) || training loss 0.03477 || training accuracy 98.75% || lr 1.6470399357816577e-06\n","Epoch[7/10](14240/14999) || training loss 0.1461 || training accuracy 95.00% || lr 1.6459696300755902e-06\n","Epoch[7/10](14260/14999) || training loss 0.06362 || training accuracy 97.50% || lr 1.644899324369523e-06\n","Epoch[7/10](14280/14999) || training loss 0.01811 || training accuracy 99.38% || lr 1.6438290186634556e-06\n","Epoch[7/10](14300/14999) || training loss 0.1396 || training accuracy 96.88% || lr 1.6427587129573883e-06\n","Epoch[7/10](14320/14999) || training loss 0.05581 || training accuracy 98.12% || lr 1.641688407251321e-06\n","Epoch[7/10](14340/14999) || training loss 0.0338 || training accuracy 98.75% || lr 1.6406181015452539e-06\n","Epoch[7/10](14360/14999) || training loss 0.02904 || training accuracy 99.38% || lr 1.6395477958391867e-06\n","Epoch[7/10](14380/14999) || training loss 0.0824 || training accuracy 96.25% || lr 1.638477490133119e-06\n","Epoch[7/10](14400/14999) || training loss 0.05031 || training accuracy 98.12% || lr 1.6374071844270518e-06\n","Epoch[7/10](14420/14999) || training loss 0.09517 || training accuracy 96.88% || lr 1.6363368787209845e-06\n","Epoch[7/10](14440/14999) || training loss 0.03906 || training accuracy 98.12% || lr 1.6352665730149173e-06\n","Epoch[7/10](14460/14999) || training loss 0.05047 || training accuracy 96.25% || lr 1.63419626730885e-06\n","Epoch[7/10](14480/14999) || training loss 0.09099 || training accuracy 97.50% || lr 1.6331259616027829e-06\n","Epoch[7/10](14500/14999) || training loss 0.07352 || training accuracy 96.25% || lr 1.6320556558967154e-06\n","Epoch[7/10](14520/14999) || training loss 0.04031 || training accuracy 99.38% || lr 1.630985350190648e-06\n","Epoch[7/10](14540/14999) || training loss 0.05672 || training accuracy 97.50% || lr 1.6299150444845808e-06\n","Epoch[7/10](14560/14999) || training loss 0.05897 || training accuracy 96.88% || lr 1.6288447387785135e-06\n","Epoch[7/10](14580/14999) || training loss 0.03353 || training accuracy 99.38% || lr 1.6277744330724463e-06\n","Epoch[7/10](14600/14999) || training loss 0.05039 || training accuracy 98.75% || lr 1.6267041273663789e-06\n","Epoch[7/10](14620/14999) || training loss 0.03102 || training accuracy 99.38% || lr 1.6256338216603117e-06\n","Epoch[7/10](14640/14999) || training loss 0.03601 || training accuracy 98.75% || lr 1.6245635159542444e-06\n","Epoch[7/10](14660/14999) || training loss 0.05151 || training accuracy 98.12% || lr 1.6234932102481772e-06\n","Epoch[7/10](14680/14999) || training loss 0.05337 || training accuracy 97.50% || lr 1.6224229045421098e-06\n","Epoch[7/10](14700/14999) || training loss 0.08024 || training accuracy 97.50% || lr 1.6213525988360423e-06\n","Epoch[7/10](14720/14999) || training loss 0.03096 || training accuracy 99.38% || lr 1.620282293129975e-06\n","Epoch[7/10](14740/14999) || training loss 0.07123 || training accuracy 98.12% || lr 1.6192119874239079e-06\n","Epoch[7/10](14760/14999) || training loss 0.01705 || training accuracy 100.00% || lr 1.6181416817178406e-06\n","Epoch[7/10](14780/14999) || training loss 0.07215 || training accuracy 97.50% || lr 1.6170713760117734e-06\n","Epoch[7/10](14800/14999) || training loss 0.06437 || training accuracy 98.12% || lr 1.6160010703057062e-06\n","Epoch[7/10](14820/14999) || training loss 0.08017 || training accuracy 97.50% || lr 1.6149307645996385e-06\n","Epoch[7/10](14840/14999) || training loss 0.0449 || training accuracy 98.75% || lr 1.6138604588935713e-06\n","Epoch[7/10](14860/14999) || training loss 0.05544 || training accuracy 98.75% || lr 1.612790153187504e-06\n","Epoch[7/10](14880/14999) || training loss 0.09591 || training accuracy 97.50% || lr 1.6117198474814369e-06\n","Epoch[7/10](14900/14999) || training loss 0.02051 || training accuracy 100.00% || lr 1.6106495417753696e-06\n","Epoch[7/10](14920/14999) || training loss 0.02608 || training accuracy 98.75% || lr 1.6095792360693022e-06\n","Epoch[7/10](14940/14999) || training loss 0.07587 || training accuracy 98.12% || lr 1.608508930363235e-06\n","Epoch[7/10](14960/14999) || training loss 0.08905 || training accuracy 96.25% || lr 1.6074386246571675e-06\n","Epoch[7/10](14980/14999) || training loss 0.02534 || training accuracy 99.38% || lr 1.6063683189511003e-06\n","Calculating validation results...\n","100% 235/235 [02:22<00:00,  1.65it/s]\n","[Val] acc : 90.89%, loss: 0.378, F1 : 0.9088 || best acc : 91.14%, best loss: 0.2442\n","Time elapsed:  533.86 min\n","\n","Epoch[8/10](20/14999) || training loss 0.04869 || training accuracy 98.12% || lr 1.604281222824269e-06\n","Epoch[8/10](40/14999) || training loss 0.03316 || training accuracy 98.75% || lr 1.6032109171182018e-06\n","Epoch[8/10](60/14999) || training loss 0.01934 || training accuracy 99.38% || lr 1.6021406114121346e-06\n","Epoch[8/10](80/14999) || training loss 0.05147 || training accuracy 99.38% || lr 1.6010703057060674e-06\n","Epoch[8/10](100/14999) || training loss 0.08939 || training accuracy 96.88% || lr 1.6e-06\n","Epoch[8/10](120/14999) || training loss 0.02814 || training accuracy 99.38% || lr 1.5989296942939325e-06\n","Epoch[8/10](140/14999) || training loss 0.04928 || training accuracy 97.50% || lr 1.5978593885878653e-06\n","Epoch[8/10](160/14999) || training loss 0.02992 || training accuracy 98.75% || lr 1.596789082881798e-06\n","Epoch[8/10](180/14999) || training loss 0.01631 || training accuracy 99.38% || lr 1.5957187771757308e-06\n","Epoch[8/10](200/14999) || training loss 0.01111 || training accuracy 100.00% || lr 1.5946484714696636e-06\n","Epoch[8/10](220/14999) || training loss 0.08922 || training accuracy 98.12% || lr 1.5935781657635961e-06\n","Epoch[8/10](240/14999) || training loss 0.04318 || training accuracy 98.12% || lr 1.5925078600575287e-06\n","Epoch[8/10](260/14999) || training loss 0.07269 || training accuracy 98.12% || lr 1.5914375543514615e-06\n","Epoch[8/10](280/14999) || training loss 0.01426 || training accuracy 100.00% || lr 1.5903672486453943e-06\n","Epoch[8/10](300/14999) || training loss 0.01788 || training accuracy 100.00% || lr 1.589296942939327e-06\n","Epoch[8/10](320/14999) || training loss 0.02842 || training accuracy 98.75% || lr 1.5882266372332598e-06\n","Epoch[8/10](340/14999) || training loss 0.03553 || training accuracy 98.12% || lr 1.5871563315271924e-06\n","Epoch[8/10](360/14999) || training loss 0.01372 || training accuracy 100.00% || lr 1.5860860258211251e-06\n","Epoch[8/10](380/14999) || training loss 0.0113 || training accuracy 100.00% || lr 1.5850157201150577e-06\n","Epoch[8/10](400/14999) || training loss 0.005127 || training accuracy 100.00% || lr 1.5839454144089905e-06\n","Epoch[8/10](420/14999) || training loss 0.006506 || training accuracy 100.00% || lr 1.5828751087029232e-06\n","Epoch[8/10](440/14999) || training loss 0.01208 || training accuracy 99.38% || lr 1.5818048029968558e-06\n","Epoch[8/10](460/14999) || training loss 0.08838 || training accuracy 98.12% || lr 1.5807344972907886e-06\n","Epoch[8/10](480/14999) || training loss 0.07541 || training accuracy 96.88% || lr 1.5796641915847214e-06\n","Epoch[8/10](500/14999) || training loss 0.07713 || training accuracy 98.75% || lr 1.5785938858786541e-06\n","Epoch[8/10](520/14999) || training loss 0.04248 || training accuracy 99.38% || lr 1.577523580172587e-06\n","Epoch[8/10](540/14999) || training loss 0.03049 || training accuracy 98.12% || lr 1.5764532744665193e-06\n","Epoch[8/10](560/14999) || training loss 0.03359 || training accuracy 99.38% || lr 1.575382968760452e-06\n","Epoch[8/10](580/14999) || training loss 0.03052 || training accuracy 98.75% || lr 1.5743126630543848e-06\n","Epoch[8/10](600/14999) || training loss 0.05198 || training accuracy 98.75% || lr 1.5732423573483176e-06\n","Epoch[8/10](620/14999) || training loss 0.08092 || training accuracy 97.50% || lr 1.5721720516422503e-06\n","Epoch[8/10](640/14999) || training loss 0.01422 || training accuracy 99.38% || lr 1.5711017459361831e-06\n","Epoch[8/10](660/14999) || training loss 0.03543 || training accuracy 99.38% || lr 1.5700314402301157e-06\n","Epoch[8/10](680/14999) || training loss 0.01764 || training accuracy 98.75% || lr 1.5689611345240482e-06\n","Epoch[8/10](700/14999) || training loss 0.02409 || training accuracy 99.38% || lr 1.567890828817981e-06\n","Epoch[8/10](720/14999) || training loss 0.05653 || training accuracy 98.75% || lr 1.5668205231119138e-06\n","Epoch[8/10](740/14999) || training loss 0.131 || training accuracy 96.25% || lr 1.5657502174058466e-06\n","Epoch[8/10](760/14999) || training loss 0.009163 || training accuracy 100.00% || lr 1.5646799116997791e-06\n","Epoch[8/10](780/14999) || training loss 0.05775 || training accuracy 98.75% || lr 1.563609605993712e-06\n","Epoch[8/10](800/14999) || training loss 0.03734 || training accuracy 98.75% || lr 1.5625393002876447e-06\n","Epoch[8/10](820/14999) || training loss 0.01893 || training accuracy 99.38% || lr 1.5614689945815772e-06\n","Epoch[8/10](840/14999) || training loss 0.06609 || training accuracy 97.50% || lr 1.56039868887551e-06\n","Epoch[8/10](860/14999) || training loss 0.08217 || training accuracy 98.12% || lr 1.5593283831694426e-06\n","Epoch[8/10](880/14999) || training loss 0.05887 || training accuracy 98.12% || lr 1.5582580774633753e-06\n","Epoch[8/10](900/14999) || training loss 0.03083 || training accuracy 98.75% || lr 1.5571877717573081e-06\n","Epoch[8/10](920/14999) || training loss 0.01597 || training accuracy 99.38% || lr 1.556117466051241e-06\n","Epoch[8/10](940/14999) || training loss 0.02053 || training accuracy 98.75% || lr 1.5550471603451737e-06\n","Epoch[8/10](960/14999) || training loss 0.03349 || training accuracy 98.75% || lr 1.5539768546391062e-06\n","Epoch[8/10](980/14999) || training loss 0.05957 || training accuracy 97.50% || lr 1.5529065489330388e-06\n","Epoch[8/10](1000/14999) || training loss 0.1194 || training accuracy 96.25% || lr 1.5518362432269716e-06\n","Epoch[8/10](1020/14999) || training loss 0.007386 || training accuracy 100.00% || lr 1.5507659375209043e-06\n","Epoch[8/10](1040/14999) || training loss 0.01634 || training accuracy 100.00% || lr 1.5496956318148371e-06\n","Epoch[8/10](1060/14999) || training loss 0.02961 || training accuracy 98.75% || lr 1.5486253261087699e-06\n","Epoch[8/10](1080/14999) || training loss 0.05926 || training accuracy 98.12% || lr 1.5475550204027024e-06\n","Epoch[8/10](1100/14999) || training loss 0.07802 || training accuracy 97.50% || lr 1.546484714696635e-06\n","Epoch[8/10](1120/14999) || training loss 0.02406 || training accuracy 99.38% || lr 1.5454144089905678e-06\n","Epoch[8/10](1140/14999) || training loss 0.02817 || training accuracy 98.75% || lr 1.5443441032845006e-06\n","Epoch[8/10](1160/14999) || training loss 0.02666 || training accuracy 98.75% || lr 1.5432737975784333e-06\n","Epoch[8/10](1180/14999) || training loss 0.05277 || training accuracy 98.12% || lr 1.5422034918723659e-06\n","Epoch[8/10](1200/14999) || training loss 0.02521 || training accuracy 98.75% || lr 1.5411331861662987e-06\n","Epoch[8/10](1220/14999) || training loss 0.03865 || training accuracy 98.75% || lr 1.5400628804602314e-06\n","Epoch[8/10](1240/14999) || training loss 0.03493 || training accuracy 98.75% || lr 1.538992574754164e-06\n","Epoch[8/10](1260/14999) || training loss 0.031 || training accuracy 98.12% || lr 1.5379222690480968e-06\n","Epoch[8/10](1280/14999) || training loss 0.01767 || training accuracy 99.38% || lr 1.5368519633420295e-06\n","Epoch[8/10](1300/14999) || training loss 0.03801 || training accuracy 98.12% || lr 1.5357816576359621e-06\n","Epoch[8/10](1320/14999) || training loss 0.01944 || training accuracy 99.38% || lr 1.5347113519298949e-06\n","Epoch[8/10](1340/14999) || training loss 0.03177 || training accuracy 99.38% || lr 1.5336410462238277e-06\n","Epoch[8/10](1360/14999) || training loss 0.06311 || training accuracy 97.50% || lr 1.5325707405177604e-06\n","Epoch[8/10](1380/14999) || training loss 0.04187 || training accuracy 98.75% || lr 1.5315004348116932e-06\n","Epoch[8/10](1400/14999) || training loss 0.01785 || training accuracy 99.38% || lr 1.5304301291056256e-06\n","Epoch[8/10](1420/14999) || training loss 0.07204 || training accuracy 98.12% || lr 1.5293598233995583e-06\n","Epoch[8/10](1440/14999) || training loss 0.03667 || training accuracy 98.75% || lr 1.528289517693491e-06\n","Epoch[8/10](1460/14999) || training loss 0.08221 || training accuracy 97.50% || lr 1.5272192119874239e-06\n","Epoch[8/10](1480/14999) || training loss 0.1071 || training accuracy 97.50% || lr 1.5261489062813567e-06\n","Epoch[8/10](1500/14999) || training loss 0.054 || training accuracy 98.12% || lr 1.5250786005752892e-06\n","Epoch[8/10](1520/14999) || training loss 0.04139 || training accuracy 98.75% || lr 1.524008294869222e-06\n","Epoch[8/10](1540/14999) || training loss 0.04065 || training accuracy 98.12% || lr 1.5229379891631545e-06\n","Epoch[8/10](1560/14999) || training loss 0.04135 || training accuracy 97.50% || lr 1.5218676834570873e-06\n","Epoch[8/10](1580/14999) || training loss 0.04953 || training accuracy 98.12% || lr 1.52079737775102e-06\n","Epoch[8/10](1600/14999) || training loss 0.08702 || training accuracy 96.25% || lr 1.5197270720449529e-06\n","Epoch[8/10](1620/14999) || training loss 0.009718 || training accuracy 100.00% || lr 1.5186567663388854e-06\n","Epoch[8/10](1640/14999) || training loss 0.04639 || training accuracy 97.50% || lr 1.5175864606328182e-06\n","Epoch[8/10](1660/14999) || training loss 0.04012 || training accuracy 98.75% || lr 1.516516154926751e-06\n","Epoch[8/10](1680/14999) || training loss 0.02985 || training accuracy 97.50% || lr 1.5154458492206835e-06\n","Epoch[8/10](1700/14999) || training loss 0.09709 || training accuracy 97.50% || lr 1.5143755435146163e-06\n","Epoch[8/10](1720/14999) || training loss 0.04047 || training accuracy 98.12% || lr 1.5133052378085489e-06\n","Epoch[8/10](1740/14999) || training loss 0.02122 || training accuracy 100.00% || lr 1.5122349321024816e-06\n","Epoch[8/10](1760/14999) || training loss 0.09082 || training accuracy 98.12% || lr 1.5111646263964144e-06\n","Epoch[8/10](1780/14999) || training loss 0.1408 || training accuracy 95.62% || lr 1.5100943206903472e-06\n","Epoch[8/10](1800/14999) || training loss 0.0517 || training accuracy 98.12% || lr 1.50902401498428e-06\n","Epoch[8/10](1820/14999) || training loss 0.03729 || training accuracy 98.12% || lr 1.5079537092782123e-06\n","Epoch[8/10](1840/14999) || training loss 0.0239 || training accuracy 99.38% || lr 1.506883403572145e-06\n","Epoch[8/10](1860/14999) || training loss 0.03657 || training accuracy 98.75% || lr 1.5058130978660779e-06\n","Epoch[8/10](1880/14999) || training loss 0.01655 || training accuracy 99.38% || lr 1.5047427921600106e-06\n","Epoch[8/10](1900/14999) || training loss 0.03907 || training accuracy 98.75% || lr 1.5036724864539434e-06\n","Epoch[8/10](1920/14999) || training loss 0.0315 || training accuracy 99.38% || lr 1.5026021807478762e-06\n","Epoch[8/10](1940/14999) || training loss 0.02912 || training accuracy 99.38% || lr 1.5015318750418088e-06\n","Epoch[8/10](1960/14999) || training loss 0.03969 || training accuracy 98.75% || lr 1.5004615693357413e-06\n","Epoch[8/10](1980/14999) || training loss 0.06597 || training accuracy 96.88% || lr 1.499391263629674e-06\n","Epoch[8/10](2000/14999) || training loss 0.07922 || training accuracy 97.50% || lr 1.4983209579236069e-06\n","Epoch[8/10](2020/14999) || training loss 0.01712 || training accuracy 100.00% || lr 1.4972506522175396e-06\n","Epoch[8/10](2040/14999) || training loss 0.01402 || training accuracy 99.38% || lr 1.4961803465114722e-06\n","Epoch[8/10](2060/14999) || training loss 0.03223 || training accuracy 98.75% || lr 1.495110040805405e-06\n","Epoch[8/10](2080/14999) || training loss 0.08602 || training accuracy 95.62% || lr 1.4940397350993377e-06\n","Epoch[8/10](2100/14999) || training loss 0.03422 || training accuracy 98.12% || lr 1.4929694293932705e-06\n","Epoch[8/10](2120/14999) || training loss 0.07079 || training accuracy 98.75% || lr 1.491899123687203e-06\n","Epoch[8/10](2140/14999) || training loss 0.01338 || training accuracy 100.00% || lr 1.4908288179811356e-06\n","Epoch[8/10](2160/14999) || training loss 0.0476 || training accuracy 98.12% || lr 1.4897585122750684e-06\n","Epoch[8/10](2180/14999) || training loss 0.01351 || training accuracy 99.38% || lr 1.4886882065690012e-06\n","Epoch[8/10](2200/14999) || training loss 0.04354 || training accuracy 98.12% || lr 1.487617900862934e-06\n","Epoch[8/10](2220/14999) || training loss 0.0574 || training accuracy 97.50% || lr 1.4865475951568667e-06\n","Epoch[8/10](2240/14999) || training loss 0.03216 || training accuracy 99.38% || lr 1.4854772894507995e-06\n","Epoch[8/10](2260/14999) || training loss 0.06358 || training accuracy 99.38% || lr 1.4844069837447319e-06\n","Epoch[8/10](2280/14999) || training loss 0.01606 || training accuracy 99.38% || lr 1.4833366780386646e-06\n","Epoch[8/10](2300/14999) || training loss 0.01714 || training accuracy 100.00% || lr 1.4822663723325974e-06\n","Epoch[8/10](2320/14999) || training loss 0.1054 || training accuracy 96.88% || lr 1.4811960666265302e-06\n","Epoch[8/10](2340/14999) || training loss 0.04327 || training accuracy 98.75% || lr 1.480125760920463e-06\n","Epoch[8/10](2360/14999) || training loss 0.006519 || training accuracy 100.00% || lr 1.4790554552143955e-06\n","Epoch[8/10](2380/14999) || training loss 0.03248 || training accuracy 99.38% || lr 1.4779851495083283e-06\n","Epoch[8/10](2400/14999) || training loss 0.01334 || training accuracy 100.00% || lr 1.4769148438022609e-06\n","Epoch[8/10](2420/14999) || training loss 0.06569 || training accuracy 97.50% || lr 1.4758445380961936e-06\n","Epoch[8/10](2440/14999) || training loss 0.04222 || training accuracy 98.75% || lr 1.4747742323901264e-06\n","Epoch[8/10](2460/14999) || training loss 0.01409 || training accuracy 100.00% || lr 1.473703926684059e-06\n","Epoch[8/10](2480/14999) || training loss 0.03616 || training accuracy 98.75% || lr 1.4726336209779917e-06\n","Epoch[8/10](2500/14999) || training loss 0.0132 || training accuracy 100.00% || lr 1.4715633152719245e-06\n","Epoch[8/10](2520/14999) || training loss 0.04648 || training accuracy 97.50% || lr 1.4704930095658573e-06\n","Epoch[8/10](2540/14999) || training loss 0.07213 || training accuracy 98.75% || lr 1.4694227038597898e-06\n","Epoch[8/10](2560/14999) || training loss 0.01945 || training accuracy 99.38% || lr 1.4683523981537226e-06\n","Epoch[8/10](2580/14999) || training loss 0.02804 || training accuracy 99.38% || lr 1.4672820924476552e-06\n","Epoch[8/10](2600/14999) || training loss 0.03705 || training accuracy 98.75% || lr 1.466211786741588e-06\n","Epoch[8/10](2620/14999) || training loss 0.03554 || training accuracy 98.12% || lr 1.4651414810355207e-06\n","Epoch[8/10](2640/14999) || training loss 0.03749 || training accuracy 98.12% || lr 1.4640711753294535e-06\n","Epoch[8/10](2660/14999) || training loss 0.03613 || training accuracy 98.75% || lr 1.4630008696233863e-06\n","Epoch[8/10](2680/14999) || training loss 0.03999 || training accuracy 98.75% || lr 1.4619305639173186e-06\n","Epoch[8/10](2700/14999) || training loss 0.03923 || training accuracy 98.75% || lr 1.4608602582112514e-06\n","Epoch[8/10](2720/14999) || training loss 0.05074 || training accuracy 98.12% || lr 1.4597899525051842e-06\n","Epoch[8/10](2740/14999) || training loss 0.04358 || training accuracy 97.50% || lr 1.458719646799117e-06\n","Epoch[8/10](2760/14999) || training loss 0.03074 || training accuracy 98.12% || lr 1.4576493410930497e-06\n","Epoch[8/10](2780/14999) || training loss 0.02657 || training accuracy 99.38% || lr 1.4565790353869823e-06\n","Epoch[8/10](2800/14999) || training loss 0.01857 || training accuracy 99.38% || lr 1.455508729680915e-06\n","Epoch[8/10](2820/14999) || training loss 0.04134 || training accuracy 97.50% || lr 1.4544384239748478e-06\n","Epoch[8/10](2840/14999) || training loss 0.08547 || training accuracy 95.62% || lr 1.4533681182687804e-06\n","Epoch[8/10](2860/14999) || training loss 0.03703 || training accuracy 98.75% || lr 1.4522978125627132e-06\n","Epoch[8/10](2880/14999) || training loss 0.01456 || training accuracy 99.38% || lr 1.451227506856646e-06\n","Epoch[8/10](2900/14999) || training loss 0.02004 || training accuracy 99.38% || lr 1.4501572011505785e-06\n","Epoch[8/10](2920/14999) || training loss 0.043 || training accuracy 98.75% || lr 1.4490868954445113e-06\n","Epoch[8/10](2940/14999) || training loss 0.03778 || training accuracy 98.12% || lr 1.448016589738444e-06\n","Epoch[8/10](2960/14999) || training loss 0.01168 || training accuracy 100.00% || lr 1.4469462840323768e-06\n","Epoch[8/10](2980/14999) || training loss 0.003496 || training accuracy 100.00% || lr 1.4458759783263094e-06\n","Epoch[8/10](3000/14999) || training loss 0.05519 || training accuracy 97.50% || lr 1.444805672620242e-06\n","Epoch[8/10](3020/14999) || training loss 0.01393 || training accuracy 99.38% || lr 1.4437353669141747e-06\n","Epoch[8/10](3040/14999) || training loss 0.0291 || training accuracy 99.38% || lr 1.4426650612081075e-06\n","Epoch[8/10](3060/14999) || training loss 0.01519 || training accuracy 99.38% || lr 1.4415947555020403e-06\n","Epoch[8/10](3080/14999) || training loss 0.08868 || training accuracy 98.12% || lr 1.440524449795973e-06\n","Epoch[8/10](3100/14999) || training loss 0.0109 || training accuracy 99.38% || lr 1.4394541440899056e-06\n","Epoch[8/10](3120/14999) || training loss 0.01002 || training accuracy 99.38% || lr 1.4383838383838382e-06\n","Epoch[8/10](3140/14999) || training loss 0.07663 || training accuracy 98.12% || lr 1.437313532677771e-06\n","Epoch[8/10](3160/14999) || training loss 0.01332 || training accuracy 100.00% || lr 1.4362432269717037e-06\n","Epoch[8/10](3180/14999) || training loss 0.05122 || training accuracy 97.50% || lr 1.4351729212656365e-06\n","Epoch[8/10](3200/14999) || training loss 0.08763 || training accuracy 98.12% || lr 1.4341026155595693e-06\n","Epoch[8/10](3220/14999) || training loss 0.01458 || training accuracy 99.38% || lr 1.4330323098535018e-06\n","Epoch[8/10](3240/14999) || training loss 0.01689 || training accuracy 99.38% || lr 1.4319620041474346e-06\n","Epoch[8/10](3260/14999) || training loss 0.008848 || training accuracy 100.00% || lr 1.4308916984413672e-06\n","Epoch[8/10](3280/14999) || training loss 0.01502 || training accuracy 99.38% || lr 1.4298213927353e-06\n","Epoch[8/10](3300/14999) || training loss 0.007622 || training accuracy 100.00% || lr 1.4287510870292327e-06\n","Epoch[8/10](3320/14999) || training loss 0.03622 || training accuracy 98.75% || lr 1.4276807813231653e-06\n","Epoch[8/10](3340/14999) || training loss 0.05726 || training accuracy 98.12% || lr 1.426610475617098e-06\n","Epoch[8/10](3360/14999) || training loss 0.02236 || training accuracy 99.38% || lr 1.4255401699110308e-06\n","Epoch[8/10](3380/14999) || training loss 0.07754 || training accuracy 98.12% || lr 1.4244698642049636e-06\n","Epoch[8/10](3400/14999) || training loss 0.00663 || training accuracy 100.00% || lr 1.4233995584988964e-06\n","Epoch[8/10](3420/14999) || training loss 0.0988 || training accuracy 96.88% || lr 1.4223292527928287e-06\n","Epoch[8/10](3440/14999) || training loss 0.04888 || training accuracy 98.75% || lr 1.4212589470867615e-06\n","Epoch[8/10](3460/14999) || training loss 0.008921 || training accuracy 100.00% || lr 1.4201886413806943e-06\n","Epoch[8/10](3480/14999) || training loss 0.01289 || training accuracy 100.00% || lr 1.419118335674627e-06\n","Epoch[8/10](3500/14999) || training loss 0.04714 || training accuracy 98.75% || lr 1.4180480299685598e-06\n","Epoch[8/10](3520/14999) || training loss 0.05716 || training accuracy 98.75% || lr 1.4169777242624926e-06\n","Epoch[8/10](3540/14999) || training loss 0.05549 || training accuracy 98.75% || lr 1.4159074185564251e-06\n","Epoch[8/10](3560/14999) || training loss 0.04441 || training accuracy 98.12% || lr 1.4148371128503577e-06\n","Epoch[8/10](3580/14999) || training loss 0.03452 || training accuracy 98.75% || lr 1.4137668071442905e-06\n","Epoch[8/10](3600/14999) || training loss 0.05957 || training accuracy 96.88% || lr 1.4126965014382232e-06\n","Epoch[8/10](3620/14999) || training loss 0.02306 || training accuracy 99.38% || lr 1.411626195732156e-06\n","Epoch[8/10](3640/14999) || training loss 0.06988 || training accuracy 98.75% || lr 1.4105558900260886e-06\n","Epoch[8/10](3660/14999) || training loss 0.02527 || training accuracy 98.75% || lr 1.4094855843200214e-06\n","Epoch[8/10](3680/14999) || training loss 0.1013 || training accuracy 97.50% || lr 1.4084152786139541e-06\n","Epoch[8/10](3700/14999) || training loss 0.05817 || training accuracy 98.12% || lr 1.4073449729078867e-06\n","Epoch[8/10](3720/14999) || training loss 0.03144 || training accuracy 98.75% || lr 1.4062746672018195e-06\n","Epoch[8/10](3740/14999) || training loss 0.04847 || training accuracy 97.50% || lr 1.405204361495752e-06\n","Epoch[8/10](3760/14999) || training loss 0.05229 || training accuracy 98.12% || lr 1.4041340557896848e-06\n","Epoch[8/10](3780/14999) || training loss 0.03234 || training accuracy 99.38% || lr 1.4030637500836176e-06\n","Epoch[8/10](3800/14999) || training loss 0.06115 || training accuracy 98.12% || lr 1.4019934443775503e-06\n","Epoch[8/10](3820/14999) || training loss 0.0351 || training accuracy 98.12% || lr 1.4009231386714831e-06\n","Epoch[8/10](3840/14999) || training loss 0.04509 || training accuracy 98.12% || lr 1.3998528329654157e-06\n","Epoch[8/10](3860/14999) || training loss 0.04252 || training accuracy 98.12% || lr 1.3987825272593482e-06\n","Epoch[8/10](3880/14999) || training loss 0.0572 || training accuracy 99.38% || lr 1.397712221553281e-06\n","Epoch[8/10](3900/14999) || training loss 0.01451 || training accuracy 100.00% || lr 1.3966419158472138e-06\n","Epoch[8/10](3920/14999) || training loss 0.05841 || training accuracy 98.75% || lr 1.3955716101411466e-06\n","Epoch[8/10](3940/14999) || training loss 0.1221 || training accuracy 96.88% || lr 1.3945013044350793e-06\n","Epoch[8/10](3960/14999) || training loss 0.08249 || training accuracy 97.50% || lr 1.393430998729012e-06\n","Epoch[8/10](3980/14999) || training loss 0.01237 || training accuracy 100.00% || lr 1.3923606930229445e-06\n","Epoch[8/10](4000/14999) || training loss 0.06619 || training accuracy 96.25% || lr 1.3912903873168772e-06\n","Epoch[8/10](4020/14999) || training loss 0.02994 || training accuracy 99.38% || lr 1.39022008161081e-06\n","Epoch[8/10](4040/14999) || training loss 0.02292 || training accuracy 99.38% || lr 1.3891497759047428e-06\n","Epoch[8/10](4060/14999) || training loss 0.02533 || training accuracy 98.12% || lr 1.3880794701986753e-06\n","Epoch[8/10](4080/14999) || training loss 0.01204 || training accuracy 100.00% || lr 1.3870091644926081e-06\n","Epoch[8/10](4100/14999) || training loss 0.01501 || training accuracy 100.00% || lr 1.3859388587865409e-06\n","Epoch[8/10](4120/14999) || training loss 0.01401 || training accuracy 100.00% || lr 1.3848685530804737e-06\n","Epoch[8/10](4140/14999) || training loss 0.03653 || training accuracy 98.12% || lr 1.3837982473744062e-06\n","Epoch[8/10](4160/14999) || training loss 0.09031 || training accuracy 98.12% || lr 1.382727941668339e-06\n","Epoch[8/10](4180/14999) || training loss 0.05196 || training accuracy 98.75% || lr 1.3816576359622716e-06\n","Epoch[8/10](4200/14999) || training loss 0.03836 || training accuracy 98.12% || lr 1.3805873302562043e-06\n","Epoch[8/10](4220/14999) || training loss 0.02669 || training accuracy 98.75% || lr 1.3795170245501371e-06\n","Epoch[8/10](4240/14999) || training loss 0.07643 || training accuracy 97.50% || lr 1.3784467188440699e-06\n","Epoch[8/10](4260/14999) || training loss 0.02671 || training accuracy 98.75% || lr 1.3773764131380027e-06\n","Epoch[8/10](4280/14999) || training loss 0.01212 || training accuracy 99.38% || lr 1.376306107431935e-06\n","Epoch[8/10](4300/14999) || training loss 0.02921 || training accuracy 98.75% || lr 1.3752358017258678e-06\n","Epoch[8/10](4320/14999) || training loss 0.01227 || training accuracy 100.00% || lr 1.3741654960198006e-06\n","Epoch[8/10](4340/14999) || training loss 0.03641 || training accuracy 98.75% || lr 1.3730951903137333e-06\n","Epoch[8/10](4360/14999) || training loss 0.02298 || training accuracy 98.75% || lr 1.372024884607666e-06\n","Epoch[8/10](4380/14999) || training loss 0.06512 || training accuracy 97.50% || lr 1.3709545789015987e-06\n","Epoch[8/10](4400/14999) || training loss 0.09919 || training accuracy 97.50% || lr 1.3698842731955314e-06\n","Epoch[8/10](4420/14999) || training loss 0.008808 || training accuracy 100.00% || lr 1.368813967489464e-06\n","Epoch[8/10](4440/14999) || training loss 0.03411 || training accuracy 99.38% || lr 1.3677436617833968e-06\n","Epoch[8/10](4460/14999) || training loss 0.04116 || training accuracy 98.75% || lr 1.3666733560773295e-06\n","Epoch[8/10](4480/14999) || training loss 0.04463 || training accuracy 98.75% || lr 1.3656030503712623e-06\n","Epoch[8/10](4500/14999) || training loss 0.03902 || training accuracy 98.75% || lr 1.3645327446651949e-06\n","Epoch[8/10](4520/14999) || training loss 0.05376 || training accuracy 98.12% || lr 1.3634624389591277e-06\n","Epoch[8/10](4540/14999) || training loss 0.05596 || training accuracy 96.88% || lr 1.3623921332530604e-06\n","Epoch[8/10](4560/14999) || training loss 0.00775 || training accuracy 100.00% || lr 1.361321827546993e-06\n","Epoch[8/10](4580/14999) || training loss 0.0534 || training accuracy 98.75% || lr 1.3602515218409258e-06\n","Epoch[8/10](4600/14999) || training loss 0.00811 || training accuracy 100.00% || lr 1.3591812161348583e-06\n","Epoch[8/10](4620/14999) || training loss 0.03031 || training accuracy 99.38% || lr 1.358110910428791e-06\n","Epoch[8/10](4640/14999) || training loss 0.07309 || training accuracy 97.50% || lr 1.3570406047227239e-06\n","Epoch[8/10](4660/14999) || training loss 0.07094 || training accuracy 97.50% || lr 1.3559702990166566e-06\n","Epoch[8/10](4680/14999) || training loss 0.02929 || training accuracy 98.12% || lr 1.3548999933105894e-06\n","Epoch[8/10](4700/14999) || training loss 0.05048 || training accuracy 99.38% || lr 1.3538296876045218e-06\n","Epoch[8/10](4720/14999) || training loss 0.05529 || training accuracy 98.75% || lr 1.3527593818984545e-06\n","Epoch[8/10](4740/14999) || training loss 0.05494 || training accuracy 97.50% || lr 1.3516890761923873e-06\n","Epoch[8/10](4760/14999) || training loss 0.02694 || training accuracy 99.38% || lr 1.35061877048632e-06\n","Epoch[8/10](4780/14999) || training loss 0.05132 || training accuracy 96.88% || lr 1.3495484647802529e-06\n","Epoch[8/10](4800/14999) || training loss 0.05723 || training accuracy 99.38% || lr 1.3484781590741856e-06\n","Epoch[8/10](4820/14999) || training loss 0.07926 || training accuracy 98.12% || lr 1.3474078533681182e-06\n","Epoch[8/10](4840/14999) || training loss 0.04953 || training accuracy 98.12% || lr 1.346337547662051e-06\n","Epoch[8/10](4860/14999) || training loss 0.02813 || training accuracy 98.75% || lr 1.3452672419559835e-06\n","Epoch[8/10](4880/14999) || training loss 0.05713 || training accuracy 99.38% || lr 1.3441969362499163e-06\n","Epoch[8/10](4900/14999) || training loss 0.02468 || training accuracy 99.38% || lr 1.343126630543849e-06\n","Epoch[8/10](4920/14999) || training loss 0.03507 || training accuracy 98.75% || lr 1.3420563248377816e-06\n","Epoch[8/10](4940/14999) || training loss 0.01823 || training accuracy 99.38% || lr 1.3409860191317144e-06\n","Epoch[8/10](4960/14999) || training loss 0.02847 || training accuracy 99.38% || lr 1.3399157134256472e-06\n","Epoch[8/10](4980/14999) || training loss 0.04442 || training accuracy 98.75% || lr 1.33884540771958e-06\n","Epoch[8/10](5000/14999) || training loss 0.08449 || training accuracy 97.50% || lr 1.3377751020135125e-06\n","Epoch[8/10](5020/14999) || training loss 0.06786 || training accuracy 98.12% || lr 1.336704796307445e-06\n","Epoch[8/10](5040/14999) || training loss 0.02741 || training accuracy 99.38% || lr 1.3356344906013779e-06\n","Epoch[8/10](5060/14999) || training loss 0.09625 || training accuracy 98.12% || lr 1.3345641848953106e-06\n","Epoch[8/10](5080/14999) || training loss 0.06831 || training accuracy 98.12% || lr 1.3334938791892434e-06\n","Epoch[8/10](5100/14999) || training loss 0.08497 || training accuracy 98.12% || lr 1.3324235734831762e-06\n","Epoch[8/10](5120/14999) || training loss 0.08242 || training accuracy 97.50% || lr 1.331353267777109e-06\n","Epoch[8/10](5140/14999) || training loss 0.01322 || training accuracy 99.38% || lr 1.3302829620710413e-06\n","Epoch[8/10](5160/14999) || training loss 0.03252 || training accuracy 98.75% || lr 1.329212656364974e-06\n","Epoch[8/10](5180/14999) || training loss 0.0345 || training accuracy 98.12% || lr 1.3281423506589069e-06\n","Epoch[8/10](5200/14999) || training loss 0.04273 || training accuracy 98.12% || lr 1.3270720449528396e-06\n","Epoch[8/10](5220/14999) || training loss 0.1198 || training accuracy 95.00% || lr 1.3260017392467724e-06\n","Epoch[8/10](5240/14999) || training loss 0.02613 || training accuracy 98.75% || lr 1.324931433540705e-06\n","Epoch[8/10](5260/14999) || training loss 0.03582 || training accuracy 98.12% || lr 1.3238611278346377e-06\n","Epoch[8/10](5280/14999) || training loss 0.01495 || training accuracy 99.38% || lr 1.3227908221285703e-06\n","Epoch[8/10](5300/14999) || training loss 0.06215 || training accuracy 98.75% || lr 1.321720516422503e-06\n","Epoch[8/10](5320/14999) || training loss 0.04151 || training accuracy 99.38% || lr 1.3206502107164359e-06\n","Epoch[8/10](5340/14999) || training loss 0.04384 || training accuracy 97.50% || lr 1.3195799050103684e-06\n","Epoch[8/10](5360/14999) || training loss 0.06454 || training accuracy 96.88% || lr 1.3185095993043012e-06\n","Epoch[8/10](5380/14999) || training loss 0.06629 || training accuracy 98.75% || lr 1.317439293598234e-06\n","Epoch[8/10](5400/14999) || training loss 0.04949 || training accuracy 98.12% || lr 1.3163689878921667e-06\n","Epoch[8/10](5420/14999) || training loss 0.02173 || training accuracy 98.75% || lr 1.3152986821860995e-06\n","Epoch[8/10](5440/14999) || training loss 0.02422 || training accuracy 98.75% || lr 1.314228376480032e-06\n","Epoch[8/10](5460/14999) || training loss 0.01053 || training accuracy 99.38% || lr 1.3131580707739646e-06\n","Epoch[8/10](5480/14999) || training loss 0.03251 || training accuracy 99.38% || lr 1.3120877650678974e-06\n","Epoch[8/10](5500/14999) || training loss 0.08054 || training accuracy 97.50% || lr 1.3110174593618302e-06\n","Epoch[8/10](5520/14999) || training loss 0.05968 || training accuracy 98.12% || lr 1.309947153655763e-06\n","Epoch[8/10](5540/14999) || training loss 0.02481 || training accuracy 98.12% || lr 1.3088768479496957e-06\n","Epoch[8/10](5560/14999) || training loss 0.02032 || training accuracy 99.38% || lr 1.3078065422436283e-06\n","Epoch[8/10](5580/14999) || training loss 0.05961 || training accuracy 98.12% || lr 1.3067362365375608e-06\n","Epoch[8/10](5600/14999) || training loss 0.03005 || training accuracy 99.38% || lr 1.3056659308314936e-06\n","Epoch[8/10](5620/14999) || training loss 0.01889 || training accuracy 99.38% || lr 1.3045956251254264e-06\n","Epoch[8/10](5640/14999) || training loss 0.05399 || training accuracy 98.12% || lr 1.3035253194193592e-06\n","Epoch[8/10](5660/14999) || training loss 0.07272 || training accuracy 98.12% || lr 1.3024550137132917e-06\n","Epoch[8/10](5680/14999) || training loss 0.07581 || training accuracy 98.12% || lr 1.3013847080072245e-06\n","Epoch[8/10](5700/14999) || training loss 0.0221 || training accuracy 99.38% || lr 1.3003144023011573e-06\n","Epoch[8/10](5720/14999) || training loss 0.01824 || training accuracy 99.38% || lr 1.2992440965950898e-06\n","Epoch[8/10](5740/14999) || training loss 0.03766 || training accuracy 99.38% || lr 1.2981737908890226e-06\n","Epoch[8/10](5760/14999) || training loss 0.07502 || training accuracy 96.25% || lr 1.2971034851829554e-06\n","Epoch[8/10](5780/14999) || training loss 0.05837 || training accuracy 99.38% || lr 1.296033179476888e-06\n","Epoch[8/10](5800/14999) || training loss 0.05738 || training accuracy 97.50% || lr 1.2949628737708207e-06\n","Epoch[8/10](5820/14999) || training loss 0.0239 || training accuracy 99.38% || lr 1.2938925680647535e-06\n","Epoch[8/10](5840/14999) || training loss 0.01127 || training accuracy 100.00% || lr 1.2928222623586863e-06\n","Epoch[8/10](5860/14999) || training loss 0.01267 || training accuracy 99.38% || lr 1.2917519566526188e-06\n","Epoch[8/10](5880/14999) || training loss 0.07706 || training accuracy 98.12% || lr 1.2906816509465514e-06\n","Epoch[8/10](5900/14999) || training loss 0.04442 || training accuracy 98.12% || lr 1.2896113452404842e-06\n","Epoch[8/10](5920/14999) || training loss 0.05999 || training accuracy 98.12% || lr 1.288541039534417e-06\n","Epoch[8/10](5940/14999) || training loss 0.05285 || training accuracy 98.12% || lr 1.2874707338283497e-06\n","Epoch[8/10](5960/14999) || training loss 0.02292 || training accuracy 99.38% || lr 1.2864004281222825e-06\n","Epoch[8/10](5980/14999) || training loss 0.1057 || training accuracy 97.50% || lr 1.285330122416215e-06\n","Epoch[8/10](6000/14999) || training loss 0.1102 || training accuracy 96.88% || lr 1.2842598167101476e-06\n","Epoch[8/10](6020/14999) || training loss 0.02661 || training accuracy 99.38% || lr 1.2831895110040804e-06\n","Epoch[8/10](6040/14999) || training loss 0.01448 || training accuracy 99.38% || lr 1.2821192052980132e-06\n","Epoch[8/10](6060/14999) || training loss 0.05141 || training accuracy 99.38% || lr 1.281048899591946e-06\n","Epoch[8/10](6080/14999) || training loss 0.04874 || training accuracy 99.38% || lr 1.2799785938858787e-06\n","Epoch[8/10](6100/14999) || training loss 0.06425 || training accuracy 96.88% || lr 1.2789082881798113e-06\n","Epoch[8/10](6120/14999) || training loss 0.04063 || training accuracy 98.75% || lr 1.277837982473744e-06\n","Epoch[8/10](6140/14999) || training loss 0.09609 || training accuracy 98.12% || lr 1.2767676767676768e-06\n","Epoch[8/10](6160/14999) || training loss 0.0693 || training accuracy 98.12% || lr 1.2756973710616094e-06\n","Epoch[8/10](6180/14999) || training loss 0.01401 || training accuracy 100.00% || lr 1.2746270653555422e-06\n","Epoch[8/10](6200/14999) || training loss 0.02909 || training accuracy 99.38% || lr 1.2735567596494747e-06\n","Epoch[8/10](6220/14999) || training loss 0.08132 || training accuracy 97.50% || lr 1.2724864539434075e-06\n","Epoch[8/10](6240/14999) || training loss 0.04034 || training accuracy 98.75% || lr 1.2714161482373403e-06\n","Epoch[8/10](6260/14999) || training loss 0.04759 || training accuracy 98.75% || lr 1.270345842531273e-06\n","Epoch[8/10](6280/14999) || training loss 0.03206 || training accuracy 98.12% || lr 1.2692755368252058e-06\n","Epoch[8/10](6300/14999) || training loss 0.05605 || training accuracy 98.75% || lr 1.2682052311191382e-06\n","Epoch[8/10](6320/14999) || training loss 0.0255 || training accuracy 99.38% || lr 1.267134925413071e-06\n","Epoch[8/10](6340/14999) || training loss 0.02042 || training accuracy 98.75% || lr 1.2660646197070037e-06\n","Epoch[8/10](6360/14999) || training loss 0.03885 || training accuracy 98.12% || lr 1.2649943140009365e-06\n","Epoch[8/10](6380/14999) || training loss 0.1076 || training accuracy 96.88% || lr 1.2639240082948693e-06\n","Epoch[8/10](6400/14999) || training loss 0.01413 || training accuracy 99.38% || lr 1.262853702588802e-06\n","Epoch[8/10](6420/14999) || training loss 0.01992 || training accuracy 98.75% || lr 1.2617833968827346e-06\n","Epoch[8/10](6440/14999) || training loss 0.02619 || training accuracy 98.75% || lr 1.2607130911766672e-06\n","Epoch[8/10](6460/14999) || training loss 0.01367 || training accuracy 99.38% || lr 1.2596427854706e-06\n","Epoch[8/10](6480/14999) || training loss 0.02248 || training accuracy 99.38% || lr 1.2585724797645327e-06\n","Epoch[8/10](6500/14999) || training loss 0.01806 || training accuracy 100.00% || lr 1.2575021740584655e-06\n","Epoch[8/10](6520/14999) || training loss 0.039 || training accuracy 99.38% || lr 1.256431868352398e-06\n","Epoch[8/10](6540/14999) || training loss 0.07578 || training accuracy 96.88% || lr 1.2553615626463308e-06\n","Epoch[8/10](6560/14999) || training loss 0.02863 || training accuracy 99.38% || lr 1.2542912569402636e-06\n","Epoch[8/10](6580/14999) || training loss 0.03922 || training accuracy 98.75% || lr 1.2532209512341961e-06\n","Epoch[8/10](6600/14999) || training loss 0.0537 || training accuracy 97.50% || lr 1.252150645528129e-06\n","Epoch[8/10](6620/14999) || training loss 0.03811 || training accuracy 98.75% || lr 1.2510803398220615e-06\n","Epoch[8/10](6640/14999) || training loss 0.01758 || training accuracy 98.75% || lr 1.2500100341159943e-06\n","Epoch[8/10](6660/14999) || training loss 0.05229 || training accuracy 98.75% || lr 1.248939728409927e-06\n","Epoch[8/10](6680/14999) || training loss 0.06458 || training accuracy 98.12% || lr 1.2478694227038598e-06\n","Epoch[8/10](6700/14999) || training loss 0.05245 || training accuracy 98.75% || lr 1.2467991169977926e-06\n","Epoch[8/10](6720/14999) || training loss 0.05042 || training accuracy 99.38% || lr 1.2457288112917253e-06\n","Epoch[8/10](6740/14999) || training loss 0.07737 || training accuracy 98.12% || lr 1.2446585055856577e-06\n","Epoch[8/10](6760/14999) || training loss 0.06614 || training accuracy 99.38% || lr 1.2435881998795905e-06\n","Epoch[8/10](6780/14999) || training loss 0.06113 || training accuracy 97.50% || lr 1.2425178941735232e-06\n","Epoch[8/10](6800/14999) || training loss 0.08493 || training accuracy 96.88% || lr 1.241447588467456e-06\n","Epoch[8/10](6820/14999) || training loss 0.03309 || training accuracy 99.38% || lr 1.2403772827613888e-06\n","Epoch[8/10](6840/14999) || training loss 0.01799 || training accuracy 99.38% || lr 1.2393069770553214e-06\n","Epoch[8/10](6860/14999) || training loss 0.07402 || training accuracy 98.12% || lr 1.2382366713492541e-06\n","Epoch[8/10](6880/14999) || training loss 0.03402 || training accuracy 99.38% || lr 1.2371663656431867e-06\n","Epoch[8/10](6900/14999) || training loss 0.02731 || training accuracy 99.38% || lr 1.2360960599371195e-06\n","Epoch[8/10](6920/14999) || training loss 0.09164 || training accuracy 97.50% || lr 1.2350257542310522e-06\n","Epoch[8/10](6940/14999) || training loss 0.05484 || training accuracy 98.12% || lr 1.2339554485249848e-06\n","Epoch[8/10](6960/14999) || training loss 0.0158 || training accuracy 99.38% || lr 1.2328851428189176e-06\n","Epoch[8/10](6980/14999) || training loss 0.02164 || training accuracy 99.38% || lr 1.2318148371128503e-06\n","Epoch[8/10](7000/14999) || training loss 0.03113 || training accuracy 98.75% || lr 1.2307445314067831e-06\n","Epoch[8/10](7020/14999) || training loss 0.01384 || training accuracy 100.00% || lr 1.2296742257007157e-06\n","Epoch[8/10](7040/14999) || training loss 0.01401 || training accuracy 98.75% || lr 1.2286039199946485e-06\n","Epoch[8/10](7060/14999) || training loss 0.07382 || training accuracy 96.88% || lr 1.227533614288581e-06\n","Epoch[8/10](7080/14999) || training loss 0.0394 || training accuracy 98.12% || lr 1.2264633085825138e-06\n","Epoch[8/10](7100/14999) || training loss 0.09416 || training accuracy 97.50% || lr 1.2253930028764466e-06\n","Epoch[8/10](7120/14999) || training loss 0.04216 || training accuracy 99.38% || lr 1.2243226971703793e-06\n","Epoch[8/10](7140/14999) || training loss 0.03776 || training accuracy 98.12% || lr 1.2232523914643121e-06\n","Epoch[8/10](7160/14999) || training loss 0.06901 || training accuracy 98.75% || lr 1.2221820857582445e-06\n","Epoch[8/10](7180/14999) || training loss 0.01027 || training accuracy 99.38% || lr 1.2211117800521772e-06\n","Epoch[8/10](7200/14999) || training loss 0.0164 || training accuracy 99.38% || lr 1.22004147434611e-06\n","Epoch[8/10](7220/14999) || training loss 0.137 || training accuracy 95.62% || lr 1.2189711686400428e-06\n","Epoch[8/10](7240/14999) || training loss 0.009156 || training accuracy 100.00% || lr 1.2179008629339756e-06\n","Epoch[8/10](7260/14999) || training loss 0.113 || training accuracy 98.12% || lr 1.2168305572279081e-06\n","Epoch[8/10](7280/14999) || training loss 0.118 || training accuracy 95.00% || lr 1.2157602515218409e-06\n","Epoch[8/10](7300/14999) || training loss 0.01848 || training accuracy 99.38% || lr 1.2146899458157735e-06\n","Epoch[8/10](7320/14999) || training loss 0.007924 || training accuracy 100.00% || lr 1.2136196401097062e-06\n","Epoch[8/10](7340/14999) || training loss 0.02841 || training accuracy 98.75% || lr 1.212549334403639e-06\n","Epoch[8/10](7360/14999) || training loss 0.01018 || training accuracy 100.00% || lr 1.2114790286975718e-06\n","Epoch[8/10](7380/14999) || training loss 0.03631 || training accuracy 98.75% || lr 1.2104087229915043e-06\n","Epoch[8/10](7400/14999) || training loss 0.03539 || training accuracy 98.75% || lr 1.2093384172854371e-06\n","Epoch[8/10](7420/14999) || training loss 0.05956 || training accuracy 98.12% || lr 1.2082681115793699e-06\n","Epoch[8/10](7440/14999) || training loss 0.007419 || training accuracy 100.00% || lr 1.2071978058733027e-06\n","Epoch[8/10](7460/14999) || training loss 0.02965 || training accuracy 98.12% || lr 1.2061275001672352e-06\n","Epoch[8/10](7480/14999) || training loss 0.05314 || training accuracy 97.50% || lr 1.2050571944611678e-06\n","Epoch[8/10](7500/14999) || training loss 0.08257 || training accuracy 98.75% || lr 1.2039868887551006e-06\n","Epoch[8/10](7520/14999) || training loss 0.02983 || training accuracy 99.38% || lr 1.2029165830490333e-06\n","Epoch[8/10](7540/14999) || training loss 0.1277 || training accuracy 96.88% || lr 1.201846277342966e-06\n","Epoch[8/10](7560/14999) || training loss 0.02238 || training accuracy 99.38% || lr 1.2007759716368989e-06\n","Epoch[8/10](7580/14999) || training loss 0.009488 || training accuracy 100.00% || lr 1.1997056659308314e-06\n","Epoch[8/10](7600/14999) || training loss 0.03642 || training accuracy 98.12% || lr 1.198635360224764e-06\n","Epoch[8/10](7620/14999) || training loss 0.03438 || training accuracy 97.50% || lr 1.1975650545186968e-06\n","Epoch[8/10](7640/14999) || training loss 0.03292 || training accuracy 98.75% || lr 1.1964947488126295e-06\n","Epoch[8/10](7660/14999) || training loss 0.02007 || training accuracy 98.75% || lr 1.1954244431065623e-06\n","Epoch[8/10](7680/14999) || training loss 0.05233 || training accuracy 98.75% || lr 1.194354137400495e-06\n","Epoch[8/10](7700/14999) || training loss 0.02385 || training accuracy 99.38% || lr 1.1932838316944277e-06\n","Epoch[8/10](7720/14999) || training loss 0.03821 || training accuracy 98.75% || lr 1.1922135259883604e-06\n","Epoch[8/10](7740/14999) || training loss 0.09544 || training accuracy 96.25% || lr 1.191143220282293e-06\n","Epoch[8/10](7760/14999) || training loss 0.05973 || training accuracy 98.75% || lr 1.1900729145762258e-06\n","Epoch[8/10](7780/14999) || training loss 0.03186 || training accuracy 99.38% || lr 1.1890026088701585e-06\n","Epoch[8/10](7800/14999) || training loss 0.08177 || training accuracy 98.12% || lr 1.187932303164091e-06\n","Epoch[8/10](7820/14999) || training loss 0.03368 || training accuracy 99.38% || lr 1.1868619974580239e-06\n","Epoch[8/10](7840/14999) || training loss 0.01655 || training accuracy 99.38% || lr 1.1857916917519566e-06\n","Epoch[8/10](7860/14999) || training loss 0.03325 || training accuracy 98.12% || lr 1.1847213860458894e-06\n","Epoch[8/10](7880/14999) || training loss 0.03551 || training accuracy 98.12% || lr 1.183651080339822e-06\n","Epoch[8/10](7900/14999) || training loss 0.02072 || training accuracy 98.75% || lr 1.1825807746337545e-06\n","Epoch[8/10](7920/14999) || training loss 0.01303 || training accuracy 100.00% || lr 1.1815104689276873e-06\n","Epoch[8/10](7940/14999) || training loss 0.05264 || training accuracy 97.50% || lr 1.18044016322162e-06\n","Epoch[8/10](7960/14999) || training loss 0.01456 || training accuracy 100.00% || lr 1.1793698575155529e-06\n","Epoch[8/10](7980/14999) || training loss 0.01984 || training accuracy 98.12% || lr 1.1782995518094856e-06\n","Epoch[8/10](8000/14999) || training loss 0.01996 || training accuracy 99.38% || lr 1.1772292461034184e-06\n","Epoch[8/10](8020/14999) || training loss 0.03746 || training accuracy 98.75% || lr 1.1761589403973508e-06\n","Epoch[8/10](8040/14999) || training loss 0.01972 || training accuracy 98.75% || lr 1.1750886346912835e-06\n","Epoch[8/10](8060/14999) || training loss 0.08501 || training accuracy 96.25% || lr 1.1740183289852163e-06\n","Epoch[8/10](8080/14999) || training loss 0.03569 || training accuracy 98.12% || lr 1.172948023279149e-06\n","Epoch[8/10](8100/14999) || training loss 0.05137 || training accuracy 96.88% || lr 1.1718777175730819e-06\n","Epoch[8/10](8120/14999) || training loss 0.04309 || training accuracy 98.75% || lr 1.1708074118670144e-06\n","Epoch[8/10](8140/14999) || training loss 0.0165 || training accuracy 100.00% || lr 1.1697371061609472e-06\n","Epoch[8/10](8160/14999) || training loss 0.0543 || training accuracy 98.75% || lr 1.16866680045488e-06\n","Epoch[8/10](8180/14999) || training loss 0.03735 || training accuracy 99.38% || lr 1.1675964947488125e-06\n","Epoch[8/10](8200/14999) || training loss 0.05486 || training accuracy 98.75% || lr 1.1665261890427453e-06\n","Epoch[8/10](8220/14999) || training loss 0.03261 || training accuracy 98.75% || lr 1.1654558833366779e-06\n","Epoch[8/10](8240/14999) || training loss 0.01113 || training accuracy 100.00% || lr 1.1643855776306106e-06\n","Epoch[8/10](8260/14999) || training loss 0.04813 || training accuracy 98.75% || lr 1.1633152719245434e-06\n","Epoch[8/10](8280/14999) || training loss 0.08101 || training accuracy 97.50% || lr 1.1622449662184762e-06\n","Epoch[8/10](8300/14999) || training loss 0.02712 || training accuracy 99.38% || lr 1.161174660512409e-06\n","Epoch[8/10](8320/14999) || training loss 0.1044 || training accuracy 96.88% || lr 1.1601043548063415e-06\n","Epoch[8/10](8340/14999) || training loss 0.06666 || training accuracy 98.12% || lr 1.159034049100274e-06\n","Epoch[8/10](8360/14999) || training loss 0.02155 || training accuracy 99.38% || lr 1.1579637433942069e-06\n","Epoch[8/10](8380/14999) || training loss 0.02118 || training accuracy 99.38% || lr 1.1568934376881396e-06\n","Epoch[8/10](8400/14999) || training loss 0.01559 || training accuracy 100.00% || lr 1.1558231319820724e-06\n","Epoch[8/10](8420/14999) || training loss 0.02561 || training accuracy 99.38% || lr 1.1547528262760052e-06\n","Epoch[8/10](8440/14999) || training loss 0.01545 || training accuracy 99.38% || lr 1.1536825205699377e-06\n","Epoch[8/10](8460/14999) || training loss 0.03248 || training accuracy 98.75% || lr 1.1526122148638703e-06\n","Epoch[8/10](8480/14999) || training loss 0.0363 || training accuracy 98.75% || lr 1.151541909157803e-06\n","Epoch[8/10](8500/14999) || training loss 0.05349 || training accuracy 97.50% || lr 1.1504716034517358e-06\n","Epoch[8/10](8520/14999) || training loss 0.02111 || training accuracy 98.75% || lr 1.1494012977456686e-06\n","Epoch[8/10](8540/14999) || training loss 0.06821 || training accuracy 98.75% || lr 1.1483309920396012e-06\n","Epoch[8/10](8560/14999) || training loss 0.03459 || training accuracy 98.75% || lr 1.147260686333534e-06\n","Epoch[8/10](8580/14999) || training loss 0.08675 || training accuracy 96.88% || lr 1.1461903806274667e-06\n","Epoch[8/10](8600/14999) || training loss 0.07589 || training accuracy 98.75% || lr 1.1451200749213993e-06\n","Epoch[8/10](8620/14999) || training loss 0.08632 || training accuracy 96.88% || lr 1.144049769215332e-06\n","Epoch[8/10](8640/14999) || training loss 0.06182 || training accuracy 98.12% || lr 1.1429794635092648e-06\n","Epoch[8/10](8660/14999) || training loss 0.07046 || training accuracy 99.38% || lr 1.1419091578031974e-06\n","Epoch[8/10](8680/14999) || training loss 0.05519 || training accuracy 98.12% || lr 1.1408388520971302e-06\n","Epoch[8/10](8700/14999) || training loss 0.06328 || training accuracy 96.88% || lr 1.139768546391063e-06\n","Epoch[8/10](8720/14999) || training loss 0.0849 || training accuracy 96.88% || lr 1.1386982406849957e-06\n","Epoch[8/10](8740/14999) || training loss 0.07862 || training accuracy 97.50% || lr 1.1376279349789285e-06\n","Epoch[8/10](8760/14999) || training loss 0.02085 || training accuracy 99.38% || lr 1.1365576292728608e-06\n","Epoch[8/10](8780/14999) || training loss 0.04652 || training accuracy 98.12% || lr 1.1354873235667936e-06\n","Epoch[8/10](8800/14999) || training loss 0.01153 || training accuracy 99.38% || lr 1.1344170178607264e-06\n","Epoch[8/10](8820/14999) || training loss 0.02182 || training accuracy 99.38% || lr 1.1333467121546592e-06\n","Epoch[8/10](8840/14999) || training loss 0.0611 || training accuracy 97.50% || lr 1.132276406448592e-06\n","Epoch[8/10](8860/14999) || training loss 0.1047 || training accuracy 95.62% || lr 1.1312061007425245e-06\n","Epoch[8/10](8880/14999) || training loss 0.01626 || training accuracy 100.00% || lr 1.1301357950364573e-06\n","Epoch[8/10](8900/14999) || training loss 0.02137 || training accuracy 99.38% || lr 1.1290654893303898e-06\n","Epoch[8/10](8920/14999) || training loss 0.05806 || training accuracy 98.12% || lr 1.1279951836243226e-06\n","Epoch[8/10](8940/14999) || training loss 0.01859 || training accuracy 99.38% || lr 1.1269248779182554e-06\n","Epoch[8/10](8960/14999) || training loss 0.04043 || training accuracy 98.12% || lr 1.1258545722121882e-06\n","Epoch[8/10](8980/14999) || training loss 0.01117 || training accuracy 100.00% || lr 1.1247842665061207e-06\n","Epoch[8/10](9000/14999) || training loss 0.06653 || training accuracy 98.12% || lr 1.1237139608000535e-06\n","Epoch[8/10](9020/14999) || training loss 0.01567 || training accuracy 100.00% || lr 1.1226436550939863e-06\n","Epoch[8/10](9040/14999) || training loss 0.1138 || training accuracy 97.50% || lr 1.1215733493879188e-06\n","Epoch[8/10](9060/14999) || training loss 0.02236 || training accuracy 99.38% || lr 1.1205030436818516e-06\n","Epoch[8/10](9080/14999) || training loss 0.02293 || training accuracy 99.38% || lr 1.1194327379757842e-06\n","Epoch[8/10](9100/14999) || training loss 0.06293 || training accuracy 98.12% || lr 1.118362432269717e-06\n","Epoch[8/10](9120/14999) || training loss 0.05964 || training accuracy 98.75% || lr 1.1172921265636497e-06\n","Epoch[8/10](9140/14999) || training loss 0.03589 || training accuracy 99.38% || lr 1.1162218208575825e-06\n","Epoch[8/10](9160/14999) || training loss 0.03205 || training accuracy 98.12% || lr 1.1151515151515153e-06\n","Epoch[8/10](9180/14999) || training loss 0.1135 || training accuracy 96.88% || lr 1.1140812094454476e-06\n","Epoch[8/10](9200/14999) || training loss 0.03746 || training accuracy 98.75% || lr 1.1130109037393804e-06\n","Epoch[8/10](9220/14999) || training loss 0.09337 || training accuracy 97.50% || lr 1.1119405980333132e-06\n","Epoch[8/10](9240/14999) || training loss 0.07673 || training accuracy 98.75% || lr 1.110870292327246e-06\n","Epoch[8/10](9260/14999) || training loss 0.04423 || training accuracy 97.50% || lr 1.1097999866211787e-06\n","Epoch[8/10](9280/14999) || training loss 0.01288 || training accuracy 99.38% || lr 1.1087296809151115e-06\n","Epoch[8/10](9300/14999) || training loss 0.01889 || training accuracy 98.75% || lr 1.107659375209044e-06\n","Epoch[8/10](9320/14999) || training loss 0.0651 || training accuracy 98.75% || lr 1.1065890695029766e-06\n","Epoch[8/10](9340/14999) || training loss 0.07545 || training accuracy 98.75% || lr 1.1055187637969094e-06\n","Epoch[8/10](9360/14999) || training loss 0.06126 || training accuracy 97.50% || lr 1.1044484580908422e-06\n","Epoch[8/10](9380/14999) || training loss 0.06132 || training accuracy 98.12% || lr 1.103378152384775e-06\n","Epoch[8/10](9400/14999) || training loss 0.01974 || training accuracy 99.38% || lr 1.1023078466787075e-06\n","Epoch[8/10](9420/14999) || training loss 0.03575 || training accuracy 98.12% || lr 1.1012375409726403e-06\n","Epoch[8/10](9440/14999) || training loss 0.02284 || training accuracy 98.75% || lr 1.100167235266573e-06\n","Epoch[8/10](9460/14999) || training loss 0.0492 || training accuracy 98.75% || lr 1.0990969295605058e-06\n","Epoch[8/10](9480/14999) || training loss 0.02443 || training accuracy 99.38% || lr 1.0980266238544384e-06\n","Epoch[8/10](9500/14999) || training loss 0.02572 || training accuracy 99.38% || lr 1.096956318148371e-06\n","Epoch[8/10](9520/14999) || training loss 0.02256 || training accuracy 99.38% || lr 1.0958860124423037e-06\n","Epoch[8/10](9540/14999) || training loss 0.05724 || training accuracy 98.75% || lr 1.0948157067362365e-06\n","Epoch[8/10](9560/14999) || training loss 0.05142 || training accuracy 98.75% || lr 1.0937454010301693e-06\n","Epoch[8/10](9580/14999) || training loss 0.01484 || training accuracy 99.38% || lr 1.092675095324102e-06\n","Epoch[8/10](9600/14999) || training loss 0.07267 || training accuracy 96.25% || lr 1.0916047896180348e-06\n","Epoch[8/10](9620/14999) || training loss 0.04652 || training accuracy 97.50% || lr 1.0905344839119671e-06\n","Epoch[8/10](9640/14999) || training loss 0.074 || training accuracy 98.12% || lr 1.0894641782059e-06\n","Epoch[8/10](9660/14999) || training loss 0.02706 || training accuracy 99.38% || lr 1.0883938724998327e-06\n","Epoch[8/10](9680/14999) || training loss 0.1084 || training accuracy 96.88% || lr 1.0873235667937655e-06\n","Epoch[8/10](9700/14999) || training loss 0.06028 || training accuracy 98.75% || lr 1.0862532610876982e-06\n","Epoch[8/10](9720/14999) || training loss 0.05255 || training accuracy 98.75% || lr 1.0851829553816308e-06\n","Epoch[8/10](9740/14999) || training loss 0.07308 || training accuracy 96.88% || lr 1.0841126496755636e-06\n","Epoch[8/10](9760/14999) || training loss 0.0656 || training accuracy 98.75% || lr 1.0830423439694961e-06\n","Epoch[8/10](9780/14999) || training loss 0.05481 || training accuracy 98.12% || lr 1.081972038263429e-06\n","Epoch[8/10](9800/14999) || training loss 0.05593 || training accuracy 99.38% || lr 1.0809017325573617e-06\n","Epoch[8/10](9820/14999) || training loss 0.03227 || training accuracy 99.38% || lr 1.0798314268512943e-06\n","Epoch[8/10](9840/14999) || training loss 0.04695 || training accuracy 99.38% || lr 1.078761121145227e-06\n","Epoch[8/10](9860/14999) || training loss 0.03709 || training accuracy 98.75% || lr 1.0776908154391598e-06\n","Epoch[8/10](9880/14999) || training loss 0.01752 || training accuracy 99.38% || lr 1.0766205097330926e-06\n","Epoch[8/10](9900/14999) || training loss 0.01869 || training accuracy 99.38% || lr 1.0755502040270251e-06\n","Epoch[8/10](9920/14999) || training loss 0.02157 || training accuracy 98.75% || lr 1.074479898320958e-06\n","Epoch[8/10](9940/14999) || training loss 0.01822 || training accuracy 98.75% || lr 1.0734095926148905e-06\n","Epoch[8/10](9960/14999) || training loss 0.01655 || training accuracy 99.38% || lr 1.0723392869088232e-06\n","Epoch[8/10](9980/14999) || training loss 0.02946 || training accuracy 99.38% || lr 1.071268981202756e-06\n","Epoch[8/10](10000/14999) || training loss 0.05502 || training accuracy 97.50% || lr 1.0701986754966888e-06\n","Epoch[8/10](10020/14999) || training loss 0.03082 || training accuracy 98.12% || lr 1.0691283697906216e-06\n","Epoch[8/10](10040/14999) || training loss 0.03211 || training accuracy 98.75% || lr 1.068058064084554e-06\n","Epoch[8/10](10060/14999) || training loss 0.03001 || training accuracy 98.12% || lr 1.0669877583784867e-06\n","Epoch[8/10](10080/14999) || training loss 0.02062 || training accuracy 99.38% || lr 1.0659174526724195e-06\n","Epoch[8/10](10100/14999) || training loss 0.04754 || training accuracy 97.50% || lr 1.0648471469663522e-06\n","Epoch[8/10](10120/14999) || training loss 0.02198 || training accuracy 99.38% || lr 1.063776841260285e-06\n","Epoch[8/10](10140/14999) || training loss 0.06274 || training accuracy 98.12% || lr 1.0627065355542176e-06\n","Epoch[8/10](10160/14999) || training loss 0.04576 || training accuracy 98.12% || lr 1.0616362298481503e-06\n","Epoch[8/10](10180/14999) || training loss 0.02014 || training accuracy 99.38% || lr 1.0605659241420831e-06\n","Epoch[8/10](10200/14999) || training loss 0.02486 || training accuracy 99.38% || lr 1.0594956184360157e-06\n","Epoch[8/10](10220/14999) || training loss 0.03975 || training accuracy 99.38% || lr 1.0584253127299485e-06\n","Epoch[8/10](10240/14999) || training loss 0.0101 || training accuracy 100.00% || lr 1.0573550070238812e-06\n","Epoch[8/10](10260/14999) || training loss 0.0404 || training accuracy 98.75% || lr 1.0562847013178138e-06\n","Epoch[8/10](10280/14999) || training loss 0.0225 || training accuracy 99.38% || lr 1.0552143956117466e-06\n","Epoch[8/10](10300/14999) || training loss 0.02654 || training accuracy 98.12% || lr 1.0541440899056793e-06\n","Epoch[8/10](10320/14999) || training loss 0.06059 || training accuracy 98.75% || lr 1.0530737841996121e-06\n","Epoch[8/10](10340/14999) || training loss 0.03186 || training accuracy 98.75% || lr 1.0520034784935447e-06\n","Epoch[8/10](10360/14999) || training loss 0.09803 || training accuracy 96.25% || lr 1.0509331727874772e-06\n","Epoch[8/10](10380/14999) || training loss 0.01587 || training accuracy 100.00% || lr 1.04986286708141e-06\n","Epoch[8/10](10400/14999) || training loss 0.04357 || training accuracy 98.12% || lr 1.0487925613753428e-06\n","Epoch[8/10](10420/14999) || training loss 0.08139 || training accuracy 96.25% || lr 1.0477222556692756e-06\n","Epoch[8/10](10440/14999) || training loss 0.007627 || training accuracy 100.00% || lr 1.0466519499632083e-06\n","Epoch[8/10](10460/14999) || training loss 0.03385 || training accuracy 98.12% || lr 1.0455816442571409e-06\n","Epoch[8/10](10480/14999) || training loss 0.03584 || training accuracy 98.12% || lr 1.0445113385510735e-06\n","Epoch[8/10](10500/14999) || training loss 0.02291 || training accuracy 99.38% || lr 1.0434410328450062e-06\n","Epoch[8/10](10520/14999) || training loss 0.0357 || training accuracy 98.12% || lr 1.042370727138939e-06\n","Epoch[8/10](10540/14999) || training loss 0.0572 || training accuracy 98.12% || lr 1.0413004214328718e-06\n","Epoch[8/10](10560/14999) || training loss 0.08519 || training accuracy 97.50% || lr 1.0402301157268045e-06\n","Epoch[8/10](10580/14999) || training loss 0.007205 || training accuracy 100.00% || lr 1.0391598100207371e-06\n","Epoch[8/10](10600/14999) || training loss 0.04432 || training accuracy 98.75% || lr 1.0380895043146699e-06\n","Epoch[8/10](10620/14999) || training loss 0.02741 || training accuracy 98.75% || lr 1.0370191986086024e-06\n","Epoch[8/10](10640/14999) || training loss 0.03471 || training accuracy 98.75% || lr 1.0359488929025352e-06\n","Epoch[8/10](10660/14999) || training loss 0.03495 || training accuracy 98.75% || lr 1.034878587196468e-06\n","Epoch[8/10](10680/14999) || training loss 0.03587 || training accuracy 98.12% || lr 1.0338082814904006e-06\n","Epoch[8/10](10700/14999) || training loss 0.02154 || training accuracy 99.38% || lr 1.0327379757843333e-06\n","Epoch[8/10](10720/14999) || training loss 0.05217 || training accuracy 98.12% || lr 1.031667670078266e-06\n","Epoch[8/10](10740/14999) || training loss 0.05185 || training accuracy 98.12% || lr 1.0305973643721989e-06\n","Epoch[8/10](10760/14999) || training loss 0.06769 || training accuracy 98.12% || lr 1.0295270586661316e-06\n","Epoch[8/10](10780/14999) || training loss 0.02717 || training accuracy 99.38% || lr 1.028456752960064e-06\n","Epoch[8/10](10800/14999) || training loss 0.0271 || training accuracy 98.75% || lr 1.0273864472539968e-06\n","Epoch[8/10](10820/14999) || training loss 0.01839 || training accuracy 99.38% || lr 1.0263161415479295e-06\n","Epoch[8/10](10840/14999) || training loss 0.01474 || training accuracy 100.00% || lr 1.0252458358418623e-06\n","Epoch[8/10](10860/14999) || training loss 0.03575 || training accuracy 98.12% || lr 1.024175530135795e-06\n","Epoch[8/10](10880/14999) || training loss 0.04771 || training accuracy 98.75% || lr 1.0231052244297279e-06\n","Epoch[8/10](10900/14999) || training loss 0.02577 || training accuracy 98.12% || lr 1.0220349187236604e-06\n","Epoch[8/10](10920/14999) || training loss 0.07825 || training accuracy 98.12% || lr 1.020964613017593e-06\n","Epoch[8/10](10940/14999) || training loss 0.0706 || training accuracy 97.50% || lr 1.0198943073115258e-06\n","Epoch[8/10](10960/14999) || training loss 0.04327 || training accuracy 98.75% || lr 1.0188240016054585e-06\n","Epoch[8/10](10980/14999) || training loss 0.04198 || training accuracy 98.12% || lr 1.0177536958993913e-06\n","Epoch[8/10](11000/14999) || training loss 0.08176 || training accuracy 96.88% || lr 1.0166833901933239e-06\n","Epoch[8/10](11020/14999) || training loss 0.04383 || training accuracy 98.12% || lr 1.0156130844872566e-06\n","Epoch[8/10](11040/14999) || training loss 0.06604 || training accuracy 97.50% || lr 1.0145427787811894e-06\n","Epoch[8/10](11060/14999) || training loss 0.03937 || training accuracy 98.75% || lr 1.013472473075122e-06\n","Epoch[8/10](11080/14999) || training loss 0.02932 || training accuracy 98.75% || lr 1.0124021673690548e-06\n","Epoch[8/10](11100/14999) || training loss 0.05226 || training accuracy 97.50% || lr 1.0113318616629873e-06\n","Epoch[8/10](11120/14999) || training loss 0.04915 || training accuracy 99.38% || lr 1.01026155595692e-06\n","Epoch[8/10](11140/14999) || training loss 0.0672 || training accuracy 98.75% || lr 1.0091912502508529e-06\n","Epoch[8/10](11160/14999) || training loss 0.04036 || training accuracy 97.50% || lr 1.0081209445447856e-06\n","Epoch[8/10](11180/14999) || training loss 0.02888 || training accuracy 99.38% || lr 1.0070506388387184e-06\n","Epoch[8/10](11200/14999) || training loss 0.02833 || training accuracy 99.38% || lr 1.005980333132651e-06\n","Epoch[8/10](11220/14999) || training loss 0.02011 || training accuracy 98.75% || lr 1.0049100274265835e-06\n","Epoch[8/10](11240/14999) || training loss 0.05312 || training accuracy 98.75% || lr 1.0038397217205163e-06\n","Epoch[8/10](11260/14999) || training loss 0.0525 || training accuracy 99.38% || lr 1.002769416014449e-06\n","Epoch[8/10](11280/14999) || training loss 0.005031 || training accuracy 100.00% || lr 1.0016991103083819e-06\n","Epoch[8/10](11300/14999) || training loss 0.03889 || training accuracy 99.38% || lr 1.0006288046023146e-06\n","Epoch[8/10](11320/14999) || training loss 0.03535 || training accuracy 97.50% || lr 9.995584988962472e-07\n","Epoch[8/10](11340/14999) || training loss 0.03496 || training accuracy 98.75% || lr 9.984881931901798e-07\n","Epoch[8/10](11360/14999) || training loss 0.008078 || training accuracy 100.00% || lr 9.974178874841125e-07\n","Epoch[8/10](11380/14999) || training loss 0.04329 || training accuracy 98.12% || lr 9.963475817780453e-07\n","Epoch[8/10](11400/14999) || training loss 0.02893 || training accuracy 98.75% || lr 9.95277276071978e-07\n","Epoch[8/10](11420/14999) || training loss 0.1091 || training accuracy 95.62% || lr 9.942069703659106e-07\n","Epoch[8/10](11440/14999) || training loss 0.02998 || training accuracy 98.75% || lr 9.931366646598434e-07\n","Epoch[8/10](11460/14999) || training loss 0.05215 || training accuracy 98.12% || lr 9.920663589537762e-07\n","Epoch[8/10](11480/14999) || training loss 0.08521 || training accuracy 98.12% || lr 9.90996053247709e-07\n","Epoch[8/10](11500/14999) || training loss 0.03376 || training accuracy 98.75% || lr 9.899257475416415e-07\n","Epoch[8/10](11520/14999) || training loss 0.008112 || training accuracy 100.00% || lr 9.888554418355743e-07\n","Epoch[8/10](11540/14999) || training loss 0.02743 || training accuracy 98.75% || lr 9.87785136129507e-07\n","Epoch[8/10](11560/14999) || training loss 0.07744 || training accuracy 98.75% || lr 9.867148304234396e-07\n","Epoch[8/10](11580/14999) || training loss 0.05236 || training accuracy 98.75% || lr 9.856445247173724e-07\n","Epoch[8/10](11600/14999) || training loss 0.09635 || training accuracy 97.50% || lr 9.845742190113052e-07\n","Epoch[8/10](11620/14999) || training loss 0.01431 || training accuracy 99.38% || lr 9.835039133052377e-07\n","Epoch[8/10](11640/14999) || training loss 0.06632 || training accuracy 98.12% || lr 9.824336075991705e-07\n","Epoch[8/10](11660/14999) || training loss 0.009976 || training accuracy 99.38% || lr 9.81363301893103e-07\n","Epoch[8/10](11680/14999) || training loss 0.03364 || training accuracy 98.12% || lr 9.802929961870358e-07\n","Epoch[8/10](11700/14999) || training loss 0.007308 || training accuracy 100.00% || lr 9.792226904809686e-07\n","Epoch[8/10](11720/14999) || training loss 0.01434 || training accuracy 99.38% || lr 9.781523847749012e-07\n","Epoch[8/10](11740/14999) || training loss 0.06167 || training accuracy 98.12% || lr 9.77082079068834e-07\n","Epoch[8/10](11760/14999) || training loss 0.02904 || training accuracy 99.38% || lr 9.760117733627667e-07\n","Epoch[8/10](11780/14999) || training loss 0.05971 || training accuracy 97.50% || lr 9.749414676566993e-07\n","Epoch[8/10](11800/14999) || training loss 0.02399 || training accuracy 98.75% || lr 9.73871161950632e-07\n","Epoch[8/10](11820/14999) || training loss 0.009146 || training accuracy 100.00% || lr 9.728008562445648e-07\n","Epoch[8/10](11840/14999) || training loss 0.07659 || training accuracy 97.50% || lr 9.717305505384974e-07\n","Epoch[8/10](11860/14999) || training loss 0.01792 || training accuracy 99.38% || lr 9.706602448324302e-07\n","Epoch[8/10](11880/14999) || training loss 0.02904 || training accuracy 99.38% || lr 9.69589939126363e-07\n","Epoch[8/10](11900/14999) || training loss 0.04491 || training accuracy 98.75% || lr 9.685196334202957e-07\n","Epoch[8/10](11920/14999) || training loss 0.01605 || training accuracy 99.38% || lr 9.674493277142283e-07\n","Epoch[8/10](11940/14999) || training loss 0.1094 || training accuracy 96.88% || lr 9.66379022008161e-07\n","Epoch[8/10](11960/14999) || training loss 0.09772 || training accuracy 96.88% || lr 9.653087163020938e-07\n","Epoch[8/10](11980/14999) || training loss 0.03258 || training accuracy 99.38% || lr 9.642384105960264e-07\n","Epoch[8/10](12000/14999) || training loss 0.06539 || training accuracy 98.75% || lr 9.631681048899592e-07\n","Epoch[8/10](12020/14999) || training loss 0.0541 || training accuracy 99.38% || lr 9.62097799183892e-07\n","Epoch[8/10](12040/14999) || training loss 0.1081 || training accuracy 97.50% || lr 9.610274934778245e-07\n","Epoch[8/10](12060/14999) || training loss 0.02111 || training accuracy 99.38% || lr 9.599571877717573e-07\n","Epoch[8/10](12080/14999) || training loss 0.03158 || training accuracy 98.75% || lr 9.5888688206569e-07\n","Epoch[8/10](12100/14999) || training loss 0.04012 || training accuracy 98.75% || lr 9.578165763596226e-07\n","Epoch[8/10](12120/14999) || training loss 0.008009 || training accuracy 100.00% || lr 9.567462706535554e-07\n","Epoch[8/10](12140/14999) || training loss 0.0404 || training accuracy 98.75% || lr 9.55675964947488e-07\n","Epoch[8/10](12160/14999) || training loss 0.09404 || training accuracy 98.12% || lr 9.546056592414207e-07\n","Epoch[8/10](12180/14999) || training loss 0.0128 || training accuracy 100.00% || lr 9.535353535353535e-07\n","Epoch[8/10](12200/14999) || training loss 0.0654 || training accuracy 98.12% || lr 9.524650478292862e-07\n","Epoch[8/10](12220/14999) || training loss 0.04253 || training accuracy 98.12% || lr 9.513947421232189e-07\n","Epoch[8/10](12240/14999) || training loss 0.006869 || training accuracy 100.00% || lr 9.503244364171516e-07\n","Epoch[8/10](12260/14999) || training loss 0.03738 || training accuracy 98.75% || lr 9.492541307110843e-07\n","Epoch[8/10](12280/14999) || training loss 0.07116 || training accuracy 96.88% || lr 9.48183825005017e-07\n","Epoch[8/10](12300/14999) || training loss 0.02635 || training accuracy 99.38% || lr 9.471135192989497e-07\n","Epoch[8/10](12320/14999) || training loss 0.02281 || training accuracy 98.75% || lr 9.460432135928824e-07\n","Epoch[8/10](12340/14999) || training loss 0.04598 || training accuracy 97.50% || lr 9.449729078868152e-07\n","Epoch[8/10](12360/14999) || training loss 0.05628 || training accuracy 98.12% || lr 9.439026021807478e-07\n","Epoch[8/10](12380/14999) || training loss 0.03444 || training accuracy 98.12% || lr 9.428322964746806e-07\n","Epoch[8/10](12400/14999) || training loss 0.06737 || training accuracy 97.50% || lr 9.417619907686133e-07\n","Epoch[8/10](12420/14999) || training loss 0.06041 || training accuracy 98.75% || lr 9.406916850625459e-07\n","Epoch[8/10](12440/14999) || training loss 0.05538 || training accuracy 98.75% || lr 9.396213793564787e-07\n","Epoch[8/10](12460/14999) || training loss 0.04921 || training accuracy 97.50% || lr 9.385510736504113e-07\n","Epoch[8/10](12480/14999) || training loss 0.02848 || training accuracy 99.38% || lr 9.37480767944344e-07\n","Epoch[8/10](12500/14999) || training loss 0.07922 || training accuracy 96.88% || lr 9.364104622382768e-07\n","Epoch[8/10](12520/14999) || training loss 0.04209 || training accuracy 98.12% || lr 9.353401565322095e-07\n","Epoch[8/10](12540/14999) || training loss 0.06787 || training accuracy 98.75% || lr 9.342698508261421e-07\n","Epoch[8/10](12560/14999) || training loss 0.02116 || training accuracy 98.12% || lr 9.331995451200749e-07\n","Epoch[8/10](12580/14999) || training loss 0.06902 || training accuracy 97.50% || lr 9.321292394140076e-07\n","Epoch[8/10](12600/14999) || training loss 0.02735 || training accuracy 99.38% || lr 9.310589337079403e-07\n","Epoch[8/10](12620/14999) || training loss 0.08972 || training accuracy 96.88% || lr 9.299886280018729e-07\n","Epoch[8/10](12640/14999) || training loss 0.03259 || training accuracy 98.75% || lr 9.289183222958057e-07\n","Epoch[8/10](12660/14999) || training loss 0.02855 || training accuracy 99.38% || lr 9.278480165897385e-07\n","Epoch[8/10](12680/14999) || training loss 0.03235 || training accuracy 98.75% || lr 9.26777710883671e-07\n","Epoch[8/10](12700/14999) || training loss 0.02261 || training accuracy 99.38% || lr 9.257074051776038e-07\n","Epoch[8/10](12720/14999) || training loss 0.05603 || training accuracy 98.12% || lr 9.246370994715366e-07\n","Epoch[8/10](12740/14999) || training loss 0.01293 || training accuracy 100.00% || lr 9.235667937654693e-07\n","Epoch[8/10](12760/14999) || training loss 0.04631 || training accuracy 98.12% || lr 9.224964880594019e-07\n","Epoch[8/10](12780/14999) || training loss 0.01136 || training accuracy 100.00% || lr 9.214261823533346e-07\n","Epoch[8/10](12800/14999) || training loss 0.0174 || training accuracy 99.38% || lr 9.203558766472674e-07\n","Epoch[8/10](12820/14999) || training loss 0.02495 || training accuracy 98.12% || lr 9.192855709412e-07\n","Epoch[8/10](12840/14999) || training loss 0.0239 || training accuracy 99.38% || lr 9.182152652351327e-07\n","Epoch[8/10](12860/14999) || training loss 0.09577 || training accuracy 95.62% || lr 9.171449595290655e-07\n","Epoch[8/10](12880/14999) || training loss 0.03285 || training accuracy 99.38% || lr 9.160746538229982e-07\n","Epoch[8/10](12900/14999) || training loss 0.02124 || training accuracy 100.00% || lr 9.150043481169308e-07\n","Epoch[8/10](12920/14999) || training loss 0.0327 || training accuracy 99.38% || lr 9.139340424108636e-07\n","Epoch[8/10](12940/14999) || training loss 0.05649 || training accuracy 96.88% || lr 9.128637367047962e-07\n","Epoch[8/10](12960/14999) || training loss 0.06791 || training accuracy 97.50% || lr 9.117934309987289e-07\n","Epoch[8/10](12980/14999) || training loss 0.03764 || training accuracy 98.12% || lr 9.107231252926617e-07\n","Epoch[8/10](13000/14999) || training loss 0.04313 || training accuracy 98.75% || lr 9.096528195865944e-07\n","Epoch[8/10](13020/14999) || training loss 0.04184 || training accuracy 98.75% || lr 9.085825138805271e-07\n","Epoch[8/10](13040/14999) || training loss 0.05902 || training accuracy 98.12% || lr 9.075122081744598e-07\n","Epoch[8/10](13060/14999) || training loss 0.02336 || training accuracy 99.38% || lr 9.064419024683925e-07\n","Epoch[8/10](13080/14999) || training loss 0.04926 || training accuracy 96.88% || lr 9.053715967623252e-07\n","Epoch[8/10](13100/14999) || training loss 0.05326 || training accuracy 98.12% || lr 9.043012910562579e-07\n","Epoch[8/10](13120/14999) || training loss 0.03203 || training accuracy 98.75% || lr 9.032309853501906e-07\n","Epoch[8/10](13140/14999) || training loss 0.02304 || training accuracy 99.38% || lr 9.021606796441233e-07\n","Epoch[8/10](13160/14999) || training loss 0.05246 || training accuracy 98.12% || lr 9.01090373938056e-07\n","Epoch[8/10](13180/14999) || training loss 0.09878 || training accuracy 96.25% || lr 9.000200682319887e-07\n","Epoch[8/10](13200/14999) || training loss 0.03129 || training accuracy 99.38% || lr 8.989497625259215e-07\n","Epoch[8/10](13220/14999) || training loss 0.04211 || training accuracy 98.75% || lr 8.978794568198541e-07\n","Epoch[8/10](13240/14999) || training loss 0.0388 || training accuracy 98.12% || lr 8.968091511137869e-07\n","Epoch[8/10](13260/14999) || training loss 0.06156 || training accuracy 96.88% || lr 8.957388454077195e-07\n","Epoch[8/10](13280/14999) || training loss 0.04975 || training accuracy 98.75% || lr 8.946685397016522e-07\n","Epoch[8/10](13300/14999) || training loss 0.07426 || training accuracy 97.50% || lr 8.93598233995585e-07\n","Epoch[8/10](13320/14999) || training loss 0.02542 || training accuracy 98.75% || lr 8.925279282895176e-07\n","Epoch[8/10](13340/14999) || training loss 0.08868 || training accuracy 97.50% || lr 8.914576225834503e-07\n","Epoch[8/10](13360/14999) || training loss 0.096 || training accuracy 97.50% || lr 8.903873168773831e-07\n","Epoch[8/10](13380/14999) || training loss 0.06061 || training accuracy 98.12% || lr 8.893170111713158e-07\n","Epoch[8/10](13400/14999) || training loss 0.01743 || training accuracy 100.00% || lr 8.882467054652485e-07\n","Epoch[8/10](13420/14999) || training loss 0.02566 || training accuracy 98.75% || lr 8.871763997591811e-07\n","Epoch[8/10](13440/14999) || training loss 0.0462 || training accuracy 97.50% || lr 8.861060940531139e-07\n","Epoch[8/10](13460/14999) || training loss 0.01052 || training accuracy 100.00% || lr 8.850357883470467e-07\n","Epoch[8/10](13480/14999) || training loss 0.04474 || training accuracy 98.12% || lr 8.839654826409792e-07\n","Epoch[8/10](13500/14999) || training loss 0.0441 || training accuracy 98.75% || lr 8.82895176934912e-07\n","Epoch[8/10](13520/14999) || training loss 0.06253 || training accuracy 98.12% || lr 8.818248712288448e-07\n","Epoch[8/10](13540/14999) || training loss 0.04199 || training accuracy 99.38% || lr 8.807545655227773e-07\n","Epoch[8/10](13560/14999) || training loss 0.009499 || training accuracy 100.00% || lr 8.796842598167101e-07\n","Epoch[8/10](13580/14999) || training loss 0.0416 || training accuracy 97.50% || lr 8.786139541106428e-07\n","Epoch[8/10](13600/14999) || training loss 0.06556 || training accuracy 98.75% || lr 8.775436484045756e-07\n","Epoch[8/10](13620/14999) || training loss 0.02801 || training accuracy 99.38% || lr 8.764733426985082e-07\n","Epoch[8/10](13640/14999) || training loss 0.02141 || training accuracy 99.38% || lr 8.754030369924409e-07\n","Epoch[8/10](13660/14999) || training loss 0.025 || training accuracy 98.75% || lr 8.743327312863737e-07\n","Epoch[8/10](13680/14999) || training loss 0.01839 || training accuracy 99.38% || lr 8.732624255803064e-07\n","Epoch[8/10](13700/14999) || training loss 0.0386 || training accuracy 97.50% || lr 8.72192119874239e-07\n","Epoch[8/10](13720/14999) || training loss 0.03738 || training accuracy 98.75% || lr 8.711218141681718e-07\n","Epoch[8/10](13740/14999) || training loss 0.05911 || training accuracy 98.12% || lr 8.700515084621044e-07\n","Epoch[8/10](13760/14999) || training loss 0.01888 || training accuracy 99.38% || lr 8.689812027560371e-07\n","Epoch[8/10](13780/14999) || training loss 0.03402 || training accuracy 98.75% || lr 8.679108970499699e-07\n","Epoch[8/10](13800/14999) || training loss 0.04225 || training accuracy 98.75% || lr 8.668405913439025e-07\n","Epoch[8/10](13820/14999) || training loss 0.01144 || training accuracy 100.00% || lr 8.657702856378353e-07\n","Epoch[8/10](13840/14999) || training loss 0.04161 || training accuracy 98.75% || lr 8.64699979931768e-07\n","Epoch[8/10](13860/14999) || training loss 0.01672 || training accuracy 100.00% || lr 8.636296742257007e-07\n","Epoch[8/10](13880/14999) || training loss 0.07268 || training accuracy 98.12% || lr 8.625593685196334e-07\n","Epoch[8/10](13900/14999) || training loss 0.005509 || training accuracy 100.00% || lr 8.61489062813566e-07\n","Epoch[8/10](13920/14999) || training loss 0.01447 || training accuracy 100.00% || lr 8.604187571074988e-07\n","Epoch[8/10](13940/14999) || training loss 0.05859 || training accuracy 98.75% || lr 8.593484514014315e-07\n","Epoch[8/10](13960/14999) || training loss 0.06663 || training accuracy 97.50% || lr 8.582781456953642e-07\n","Epoch[8/10](13980/14999) || training loss 0.02708 || training accuracy 98.75% || lr 8.572078399892969e-07\n","Epoch[8/10](14000/14999) || training loss 0.0103 || training accuracy 100.00% || lr 8.561375342832296e-07\n","Epoch[8/10](14020/14999) || training loss 0.03532 || training accuracy 99.38% || lr 8.550672285771623e-07\n","Epoch[8/10](14040/14999) || training loss 0.01511 || training accuracy 99.38% || lr 8.539969228710951e-07\n","Epoch[8/10](14060/14999) || training loss 0.06063 || training accuracy 97.50% || lr 8.529266171650277e-07\n","Epoch[8/10](14080/14999) || training loss 0.05562 || training accuracy 98.12% || lr 8.518563114589604e-07\n","Epoch[8/10](14100/14999) || training loss 0.03336 || training accuracy 98.75% || lr 8.507860057528932e-07\n","Epoch[8/10](14120/14999) || training loss 0.0279 || training accuracy 98.75% || lr 8.497157000468258e-07\n","Epoch[8/10](14140/14999) || training loss 0.06347 || training accuracy 98.12% || lr 8.486453943407585e-07\n","Epoch[8/10](14160/14999) || training loss 0.03035 || training accuracy 98.12% || lr 8.475750886346913e-07\n","Epoch[8/10](14180/14999) || training loss 0.02423 || training accuracy 98.75% || lr 8.46504782928624e-07\n","Epoch[8/10](14200/14999) || training loss 0.06265 || training accuracy 96.88% || lr 8.454344772225566e-07\n","Epoch[8/10](14220/14999) || training loss 0.05273 || training accuracy 98.12% || lr 8.443641715164893e-07\n","Epoch[8/10](14240/14999) || training loss 0.01748 || training accuracy 98.75% || lr 8.432938658104221e-07\n","Epoch[8/10](14260/14999) || training loss 0.0299 || training accuracy 97.50% || lr 8.422235601043548e-07\n","Epoch[8/10](14280/14999) || training loss 0.05302 || training accuracy 99.38% || lr 8.411532543982874e-07\n","Epoch[8/10](14300/14999) || training loss 0.0199 || training accuracy 98.75% || lr 8.400829486922202e-07\n","Epoch[8/10](14320/14999) || training loss 0.01915 || training accuracy 99.38% || lr 8.39012642986153e-07\n","Epoch[8/10](14340/14999) || training loss 0.05981 || training accuracy 98.12% || lr 8.379423372800855e-07\n","Epoch[8/10](14360/14999) || training loss 0.0146 || training accuracy 99.38% || lr 8.368720315740183e-07\n","Epoch[8/10](14380/14999) || training loss 0.07544 || training accuracy 97.50% || lr 8.35801725867951e-07\n","Epoch[8/10](14400/14999) || training loss 0.0637 || training accuracy 97.50% || lr 8.347314201618836e-07\n","Epoch[8/10](14420/14999) || training loss 0.04545 || training accuracy 98.12% || lr 8.336611144558164e-07\n","Epoch[8/10](14440/14999) || training loss 0.03719 || training accuracy 98.12% || lr 8.325908087497491e-07\n","Epoch[8/10](14460/14999) || training loss 0.04184 || training accuracy 98.75% || lr 8.315205030436819e-07\n","Epoch[8/10](14480/14999) || training loss 0.0454 || training accuracy 99.38% || lr 8.304501973376145e-07\n","Epoch[8/10](14500/14999) || training loss 0.04484 || training accuracy 99.38% || lr 8.293798916315472e-07\n","Epoch[8/10](14520/14999) || training loss 0.01261 || training accuracy 99.38% || lr 8.2830958592548e-07\n","Epoch[8/10](14540/14999) || training loss 0.02447 || training accuracy 98.75% || lr 8.272392802194126e-07\n","Epoch[8/10](14560/14999) || training loss 0.04679 || training accuracy 98.12% || lr 8.261689745133453e-07\n","Epoch[8/10](14580/14999) || training loss 0.1271 || training accuracy 96.25% || lr 8.250986688072781e-07\n","Epoch[8/10](14600/14999) || training loss 0.04896 || training accuracy 98.12% || lr 8.240283631012107e-07\n","Epoch[8/10](14620/14999) || training loss 0.05911 || training accuracy 98.75% || lr 8.229580573951434e-07\n","Epoch[8/10](14640/14999) || training loss 0.01204 || training accuracy 99.38% || lr 8.218877516890762e-07\n","Epoch[8/10](14660/14999) || training loss 0.04821 || training accuracy 98.12% || lr 8.208174459830089e-07\n","Epoch[8/10](14680/14999) || training loss 0.02559 || training accuracy 99.38% || lr 8.197471402769416e-07\n","Epoch[8/10](14700/14999) || training loss 0.02157 || training accuracy 99.38% || lr 8.186768345708742e-07\n","Epoch[8/10](14720/14999) || training loss 0.02919 || training accuracy 98.75% || lr 8.17606528864807e-07\n","Epoch[8/10](14740/14999) || training loss 0.02386 || training accuracy 99.38% || lr 8.165362231587397e-07\n","Epoch[8/10](14760/14999) || training loss 0.07852 || training accuracy 98.12% || lr 8.154659174526723e-07\n","Epoch[8/10](14780/14999) || training loss 0.0509 || training accuracy 96.88% || lr 8.143956117466051e-07\n","Epoch[8/10](14800/14999) || training loss 0.02994 || training accuracy 99.38% || lr 8.133253060405378e-07\n","Epoch[8/10](14820/14999) || training loss 0.04058 || training accuracy 99.38% || lr 8.122550003344705e-07\n","Epoch[8/10](14840/14999) || training loss 0.02934 || training accuracy 99.38% || lr 8.111846946284032e-07\n","Epoch[8/10](14860/14999) || training loss 0.03214 || training accuracy 99.38% || lr 8.101143889223358e-07\n","Epoch[8/10](14880/14999) || training loss 0.05128 || training accuracy 97.50% || lr 8.090440832162686e-07\n","Epoch[8/10](14900/14999) || training loss 0.05452 || training accuracy 98.75% || lr 8.079737775102014e-07\n","Epoch[8/10](14920/14999) || training loss 0.02304 || training accuracy 98.12% || lr 8.06903471804134e-07\n","Epoch[8/10](14940/14999) || training loss 0.04508 || training accuracy 99.38% || lr 8.058331660980667e-07\n","Epoch[8/10](14960/14999) || training loss 0.01676 || training accuracy 99.38% || lr 8.047628603919995e-07\n","Epoch[8/10](14980/14999) || training loss 0.03361 || training accuracy 98.12% || lr 8.036925546859321e-07\n","Calculating validation results...\n","100% 235/235 [02:22<00:00,  1.65it/s]\n","[Val] acc : 91.00%, loss: 0.4258, F1 : 0.91 || best acc : 91.14%, best loss: 0.2442\n","Time elapsed:  600.27 min\n","\n","Epoch[9/10](20/14999) || training loss 0.02426 || training accuracy 99.38% || lr 8.016054585591009e-07\n","Epoch[9/10](40/14999) || training loss 0.01647 || training accuracy 99.38% || lr 8.005351528530337e-07\n","Epoch[9/10](60/14999) || training loss 0.01048 || training accuracy 100.00% || lr 7.994648471469662e-07\n","Epoch[9/10](80/14999) || training loss 0.05411 || training accuracy 98.12% || lr 7.98394541440899e-07\n","Epoch[9/10](100/14999) || training loss 0.02346 || training accuracy 98.75% || lr 7.973242357348318e-07\n","Epoch[9/10](120/14999) || training loss 0.009797 || training accuracy 99.38% || lr 7.962539300287644e-07\n","Epoch[9/10](140/14999) || training loss 0.04781 || training accuracy 98.75% || lr 7.951836243226971e-07\n","Epoch[9/10](160/14999) || training loss 0.02027 || training accuracy 99.38% || lr 7.941133186166299e-07\n","Epoch[9/10](180/14999) || training loss 0.04365 || training accuracy 98.75% || lr 7.930430129105626e-07\n","Epoch[9/10](200/14999) || training loss 0.06964 || training accuracy 98.75% || lr 7.919727072044952e-07\n","Epoch[9/10](220/14999) || training loss 0.00992 || training accuracy 100.00% || lr 7.909024014984279e-07\n","Epoch[9/10](240/14999) || training loss 0.02197 || training accuracy 99.38% || lr 7.898320957923607e-07\n","Epoch[9/10](260/14999) || training loss 0.02417 || training accuracy 99.38% || lr 7.887617900862935e-07\n","Epoch[9/10](280/14999) || training loss 0.02827 || training accuracy 98.12% || lr 7.87691484380226e-07\n","Epoch[9/10](300/14999) || training loss 0.02485 || training accuracy 99.38% || lr 7.866211786741588e-07\n","Epoch[9/10](320/14999) || training loss 0.0475 || training accuracy 98.75% || lr 7.855508729680916e-07\n","Epoch[9/10](340/14999) || training loss 0.01334 || training accuracy 100.00% || lr 7.844805672620241e-07\n","Epoch[9/10](360/14999) || training loss 0.01462 || training accuracy 99.38% || lr 7.834102615559569e-07\n","Epoch[9/10](380/14999) || training loss 0.06254 || training accuracy 98.12% || lr 7.823399558498896e-07\n","Epoch[9/10](400/14999) || training loss 0.02421 || training accuracy 99.38% || lr 7.812696501438223e-07\n","Epoch[9/10](420/14999) || training loss 0.026 || training accuracy 98.75% || lr 7.80199344437755e-07\n","Epoch[9/10](440/14999) || training loss 0.02167 || training accuracy 99.38% || lr 7.791290387316877e-07\n","Epoch[9/10](460/14999) || training loss 0.01493 || training accuracy 100.00% || lr 7.780587330256204e-07\n","Epoch[9/10](480/14999) || training loss 0.01993 || training accuracy 99.38% || lr 7.769884273195531e-07\n","Epoch[9/10](500/14999) || training loss 0.08064 || training accuracy 97.50% || lr 7.759181216134858e-07\n","Epoch[9/10](520/14999) || training loss 0.01887 || training accuracy 99.38% || lr 7.748478159074186e-07\n","Epoch[9/10](540/14999) || training loss 0.02988 || training accuracy 98.12% || lr 7.737775102013512e-07\n","Epoch[9/10](560/14999) || training loss 0.02988 || training accuracy 99.38% || lr 7.727072044952839e-07\n","Epoch[9/10](580/14999) || training loss 0.05794 || training accuracy 99.38% || lr 7.716368987892167e-07\n","Epoch[9/10](600/14999) || training loss 0.02171 || training accuracy 99.38% || lr 7.705665930831493e-07\n","Epoch[9/10](620/14999) || training loss 0.008073 || training accuracy 100.00% || lr 7.69496287377082e-07\n","Epoch[9/10](640/14999) || training loss 0.05112 || training accuracy 99.38% || lr 7.684259816710148e-07\n","Epoch[9/10](660/14999) || training loss 0.01086 || training accuracy 99.38% || lr 7.673556759649474e-07\n","Epoch[9/10](680/14999) || training loss 0.008536 || training accuracy 99.38% || lr 7.662853702588802e-07\n","Epoch[9/10](700/14999) || training loss 0.09598 || training accuracy 97.50% || lr 7.652150645528128e-07\n","Epoch[9/10](720/14999) || training loss 0.02042 || training accuracy 99.38% || lr 7.641447588467456e-07\n","Epoch[9/10](740/14999) || training loss 0.02382 || training accuracy 98.75% || lr 7.630744531406783e-07\n","Epoch[9/10](760/14999) || training loss 0.04279 || training accuracy 99.38% || lr 7.62004147434611e-07\n","Epoch[9/10](780/14999) || training loss 0.06273 || training accuracy 98.12% || lr 7.609338417285437e-07\n","Epoch[9/10](800/14999) || training loss 0.08107 || training accuracy 97.50% || lr 7.598635360224764e-07\n","Epoch[9/10](820/14999) || training loss 0.03522 || training accuracy 99.38% || lr 7.587932303164091e-07\n","Epoch[9/10](840/14999) || training loss 0.04502 || training accuracy 98.12% || lr 7.577229246103418e-07\n","Epoch[9/10](860/14999) || training loss 0.03313 || training accuracy 99.38% || lr 7.566526189042744e-07\n","Epoch[9/10](880/14999) || training loss 0.02415 || training accuracy 99.38% || lr 7.555823131982072e-07\n","Epoch[9/10](900/14999) || training loss 0.09722 || training accuracy 98.12% || lr 7.5451200749214e-07\n","Epoch[9/10](920/14999) || training loss 0.01683 || training accuracy 99.38% || lr 7.534417017860725e-07\n","Epoch[9/10](940/14999) || training loss 0.02992 || training accuracy 99.38% || lr 7.523713960800053e-07\n","Epoch[9/10](960/14999) || training loss 0.09858 || training accuracy 96.25% || lr 7.513010903739381e-07\n","Epoch[9/10](980/14999) || training loss 0.01488 || training accuracy 100.00% || lr 7.502307846678707e-07\n","Epoch[9/10](1000/14999) || training loss 0.0192 || training accuracy 99.38% || lr 7.491604789618034e-07\n","Epoch[9/10](1020/14999) || training loss 0.04608 || training accuracy 98.75% || lr 7.480901732557361e-07\n","Epoch[9/10](1040/14999) || training loss 0.0163 || training accuracy 99.38% || lr 7.470198675496689e-07\n","Epoch[9/10](1060/14999) || training loss 0.01312 || training accuracy 99.38% || lr 7.459495618436015e-07\n","Epoch[9/10](1080/14999) || training loss 0.01223 || training accuracy 99.38% || lr 7.448792561375342e-07\n","Epoch[9/10](1100/14999) || training loss 0.02563 || training accuracy 99.38% || lr 7.43808950431467e-07\n","Epoch[9/10](1120/14999) || training loss 0.006919 || training accuracy 100.00% || lr 7.427386447253998e-07\n","Epoch[9/10](1140/14999) || training loss 0.02076 || training accuracy 99.38% || lr 7.416683390193323e-07\n","Epoch[9/10](1160/14999) || training loss 0.00543 || training accuracy 100.00% || lr 7.405980333132651e-07\n","Epoch[9/10](1180/14999) || training loss 0.06758 || training accuracy 96.88% || lr 7.395277276071978e-07\n","Epoch[9/10](1200/14999) || training loss 0.03091 || training accuracy 98.75% || lr 7.384574219011304e-07\n","Epoch[9/10](1220/14999) || training loss 0.01454 || training accuracy 99.38% || lr 7.373871161950632e-07\n","Epoch[9/10](1240/14999) || training loss 0.02673 || training accuracy 98.75% || lr 7.363168104889959e-07\n","Epoch[9/10](1260/14999) || training loss 0.05578 || training accuracy 97.50% || lr 7.352465047829286e-07\n","Epoch[9/10](1280/14999) || training loss 0.02705 || training accuracy 98.75% || lr 7.341761990768613e-07\n","Epoch[9/10](1300/14999) || training loss 0.0318 || training accuracy 98.75% || lr 7.33105893370794e-07\n","Epoch[9/10](1320/14999) || training loss 0.03337 || training accuracy 98.12% || lr 7.320355876647267e-07\n","Epoch[9/10](1340/14999) || training loss 0.04448 || training accuracy 97.50% || lr 7.309652819586593e-07\n","Epoch[9/10](1360/14999) || training loss 0.02097 || training accuracy 99.38% || lr 7.298949762525921e-07\n","Epoch[9/10](1380/14999) || training loss 0.01818 || training accuracy 98.75% || lr 7.288246705465249e-07\n","Epoch[9/10](1400/14999) || training loss 0.02257 || training accuracy 98.12% || lr 7.277543648404575e-07\n","Epoch[9/10](1420/14999) || training loss 0.02247 || training accuracy 98.12% || lr 7.266840591343902e-07\n","Epoch[9/10](1440/14999) || training loss 0.004747 || training accuracy 100.00% || lr 7.25613753428323e-07\n","Epoch[9/10](1460/14999) || training loss 0.04512 || training accuracy 98.12% || lr 7.245434477222556e-07\n","Epoch[9/10](1480/14999) || training loss 0.05388 || training accuracy 99.38% || lr 7.234731420161884e-07\n","Epoch[9/10](1500/14999) || training loss 0.07366 || training accuracy 96.25% || lr 7.22402836310121e-07\n","Epoch[9/10](1520/14999) || training loss 0.03203 || training accuracy 98.75% || lr 7.213325306040537e-07\n","Epoch[9/10](1540/14999) || training loss 0.01909 || training accuracy 99.38% || lr 7.202622248979865e-07\n","Epoch[9/10](1560/14999) || training loss 0.01255 || training accuracy 100.00% || lr 7.191919191919191e-07\n","Epoch[9/10](1580/14999) || training loss 0.01352 || training accuracy 100.00% || lr 7.181216134858519e-07\n","Epoch[9/10](1600/14999) || training loss 0.02857 || training accuracy 98.75% || lr 7.170513077797846e-07\n","Epoch[9/10](1620/14999) || training loss 0.01473 || training accuracy 99.38% || lr 7.159810020737173e-07\n","Epoch[9/10](1640/14999) || training loss 0.04858 || training accuracy 98.75% || lr 7.1491069636765e-07\n","Epoch[9/10](1660/14999) || training loss 0.005114 || training accuracy 100.00% || lr 7.138403906615826e-07\n","Epoch[9/10](1680/14999) || training loss 0.05889 || training accuracy 98.75% || lr 7.127700849555154e-07\n","Epoch[9/10](1700/14999) || training loss 0.02626 || training accuracy 98.12% || lr 7.116997792494482e-07\n","Epoch[9/10](1720/14999) || training loss 0.09159 || training accuracy 98.75% || lr 7.106294735433807e-07\n","Epoch[9/10](1740/14999) || training loss 0.02891 || training accuracy 98.12% || lr 7.095591678373135e-07\n","Epoch[9/10](1760/14999) || training loss 0.01803 || training accuracy 98.75% || lr 7.084888621312463e-07\n","Epoch[9/10](1780/14999) || training loss 0.02301 || training accuracy 99.38% || lr 7.074185564251788e-07\n","Epoch[9/10](1800/14999) || training loss 0.01922 || training accuracy 98.75% || lr 7.063482507191116e-07\n","Epoch[9/10](1820/14999) || training loss 0.003163 || training accuracy 100.00% || lr 7.052779450130443e-07\n","Epoch[9/10](1840/14999) || training loss 0.009666 || training accuracy 100.00% || lr 7.042076393069771e-07\n","Epoch[9/10](1860/14999) || training loss 0.065 || training accuracy 98.75% || lr 7.031373336009097e-07\n","Epoch[9/10](1880/14999) || training loss 0.07264 || training accuracy 96.25% || lr 7.020670278948424e-07\n","Epoch[9/10](1900/14999) || training loss 0.05399 || training accuracy 98.75% || lr 7.009967221887752e-07\n","Epoch[9/10](1920/14999) || training loss 0.005389 || training accuracy 100.00% || lr 6.999264164827078e-07\n","Epoch[9/10](1940/14999) || training loss 0.1252 || training accuracy 98.12% || lr 6.988561107766405e-07\n","Epoch[9/10](1960/14999) || training loss 0.005817 || training accuracy 100.00% || lr 6.977858050705733e-07\n","Epoch[9/10](1980/14999) || training loss 0.03842 || training accuracy 97.50% || lr 6.96715499364506e-07\n","Epoch[9/10](2000/14999) || training loss 0.03024 || training accuracy 98.12% || lr 6.956451936584386e-07\n","Epoch[9/10](2020/14999) || training loss 0.07105 || training accuracy 98.75% || lr 6.945748879523714e-07\n","Epoch[9/10](2040/14999) || training loss 0.01625 || training accuracy 99.38% || lr 6.935045822463041e-07\n","Epoch[9/10](2060/14999) || training loss 0.006544 || training accuracy 100.00% || lr 6.924342765402368e-07\n","Epoch[9/10](2080/14999) || training loss 0.02179 || training accuracy 99.38% || lr 6.913639708341695e-07\n","Epoch[9/10](2100/14999) || training loss 0.05499 || training accuracy 98.12% || lr 6.902936651281022e-07\n","Epoch[9/10](2120/14999) || training loss 0.03543 || training accuracy 98.75% || lr 6.892233594220349e-07\n","Epoch[9/10](2140/14999) || training loss 0.02297 || training accuracy 99.38% || lr 6.881530537159675e-07\n","Epoch[9/10](2160/14999) || training loss 0.07201 || training accuracy 98.12% || lr 6.870827480099003e-07\n","Epoch[9/10](2180/14999) || training loss 0.03441 || training accuracy 98.75% || lr 6.86012442303833e-07\n","Epoch[9/10](2200/14999) || training loss 0.002822 || training accuracy 100.00% || lr 6.849421365977657e-07\n","Epoch[9/10](2220/14999) || training loss 0.0637 || training accuracy 97.50% || lr 6.838718308916984e-07\n","Epoch[9/10](2240/14999) || training loss 0.01481 || training accuracy 99.38% || lr 6.828015251856312e-07\n","Epoch[9/10](2260/14999) || training loss 0.07035 || training accuracy 98.12% || lr 6.817312194795638e-07\n","Epoch[9/10](2280/14999) || training loss 0.05241 || training accuracy 98.75% || lr 6.806609137734965e-07\n","Epoch[9/10](2300/14999) || training loss 0.03325 || training accuracy 98.75% || lr 6.795906080674292e-07\n","Epoch[9/10](2320/14999) || training loss 0.03414 || training accuracy 98.75% || lr 6.785203023613619e-07\n","Epoch[9/10](2340/14999) || training loss 0.02511 || training accuracy 99.38% || lr 6.774499966552947e-07\n","Epoch[9/10](2360/14999) || training loss 0.07389 || training accuracy 98.75% || lr 6.763796909492273e-07\n","Epoch[9/10](2380/14999) || training loss 0.06308 || training accuracy 98.75% || lr 6.7530938524316e-07\n","Epoch[9/10](2400/14999) || training loss 0.05714 || training accuracy 98.75% || lr 6.742390795370928e-07\n","Epoch[9/10](2420/14999) || training loss 0.03595 || training accuracy 97.50% || lr 6.731687738310255e-07\n","Epoch[9/10](2440/14999) || training loss 0.008521 || training accuracy 100.00% || lr 6.720984681249582e-07\n","Epoch[9/10](2460/14999) || training loss 0.0413 || training accuracy 99.38% || lr 6.710281624188908e-07\n","Epoch[9/10](2480/14999) || training loss 0.07301 || training accuracy 98.75% || lr 6.699578567128236e-07\n","Epoch[9/10](2500/14999) || training loss 0.04603 || training accuracy 97.50% || lr 6.688875510067563e-07\n","Epoch[9/10](2520/14999) || training loss 0.02474 || training accuracy 98.75% || lr 6.678172453006889e-07\n","Epoch[9/10](2540/14999) || training loss 0.0176 || training accuracy 99.38% || lr 6.667469395946217e-07\n","Epoch[9/10](2560/14999) || training loss 0.01016 || training accuracy 99.38% || lr 6.656766338885545e-07\n","Epoch[9/10](2580/14999) || training loss 0.04295 || training accuracy 97.50% || lr 6.64606328182487e-07\n","Epoch[9/10](2600/14999) || training loss 0.04046 || training accuracy 98.75% || lr 6.635360224764198e-07\n","Epoch[9/10](2620/14999) || training loss 0.01944 || training accuracy 99.38% || lr 6.624657167703525e-07\n","Epoch[9/10](2640/14999) || training loss 0.0138 || training accuracy 100.00% || lr 6.613954110642852e-07\n","Epoch[9/10](2660/14999) || training loss 0.05067 || training accuracy 98.12% || lr 6.603251053582179e-07\n","Epoch[9/10](2680/14999) || training loss 0.02705 || training accuracy 98.75% || lr 6.592547996521506e-07\n","Epoch[9/10](2700/14999) || training loss 0.003549 || training accuracy 100.00% || lr 6.581844939460834e-07\n","Epoch[9/10](2720/14999) || training loss 0.01362 || training accuracy 99.38% || lr 6.57114188240016e-07\n","Epoch[9/10](2740/14999) || training loss 0.03586 || training accuracy 99.38% || lr 6.560438825339487e-07\n","Epoch[9/10](2760/14999) || training loss 0.0636 || training accuracy 98.12% || lr 6.549735768278815e-07\n","Epoch[9/10](2780/14999) || training loss 0.08747 || training accuracy 96.25% || lr 6.539032711218141e-07\n","Epoch[9/10](2800/14999) || training loss 0.04604 || training accuracy 98.12% || lr 6.528329654157468e-07\n","Epoch[9/10](2820/14999) || training loss 0.02772 || training accuracy 98.75% || lr 6.517626597096796e-07\n","Epoch[9/10](2840/14999) || training loss 0.003082 || training accuracy 100.00% || lr 6.506923540036123e-07\n","Epoch[9/10](2860/14999) || training loss 0.05144 || training accuracy 97.50% || lr 6.496220482975449e-07\n","Epoch[9/10](2880/14999) || training loss 0.03568 || training accuracy 98.75% || lr 6.485517425914777e-07\n","Epoch[9/10](2900/14999) || training loss 0.02915 || training accuracy 98.75% || lr 6.474814368854104e-07\n","Epoch[9/10](2920/14999) || training loss 0.02813 || training accuracy 99.38% || lr 6.464111311793431e-07\n","Epoch[9/10](2940/14999) || training loss 0.04565 || training accuracy 98.12% || lr 6.453408254732757e-07\n","Epoch[9/10](2960/14999) || training loss 0.02218 || training accuracy 99.38% || lr 6.442705197672085e-07\n","Epoch[9/10](2980/14999) || training loss 0.006703 || training accuracy 100.00% || lr 6.432002140611412e-07\n","Epoch[9/10](3000/14999) || training loss 0.02515 || training accuracy 99.38% || lr 6.421299083550738e-07\n","Epoch[9/10](3020/14999) || training loss 0.01926 || training accuracy 99.38% || lr 6.410596026490066e-07\n","Epoch[9/10](3040/14999) || training loss 0.0136 || training accuracy 99.38% || lr 6.399892969429394e-07\n","Epoch[9/10](3060/14999) || training loss 0.02411 || training accuracy 99.38% || lr 6.38918991236872e-07\n","Epoch[9/10](3080/14999) || training loss 0.03595 || training accuracy 98.75% || lr 6.378486855308047e-07\n","Epoch[9/10](3100/14999) || training loss 0.06916 || training accuracy 97.50% || lr 6.367783798247374e-07\n","Epoch[9/10](3120/14999) || training loss 0.01001 || training accuracy 100.00% || lr 6.357080741186701e-07\n","Epoch[9/10](3140/14999) || training loss 0.0653 || training accuracy 98.12% || lr 6.346377684126029e-07\n","Epoch[9/10](3160/14999) || training loss 0.03623 || training accuracy 98.75% || lr 6.335674627065355e-07\n","Epoch[9/10](3180/14999) || training loss 0.01467 || training accuracy 100.00% || lr 6.324971570004682e-07\n","Epoch[9/10](3200/14999) || training loss 0.03507 || training accuracy 99.38% || lr 6.31426851294401e-07\n","Epoch[9/10](3220/14999) || training loss 0.01363 || training accuracy 100.00% || lr 6.303565455883336e-07\n","Epoch[9/10](3240/14999) || training loss 0.01683 || training accuracy 99.38% || lr 6.292862398822663e-07\n","Epoch[9/10](3260/14999) || training loss 0.02665 || training accuracy 98.75% || lr 6.28215934176199e-07\n","Epoch[9/10](3280/14999) || training loss 0.01701 || training accuracy 98.75% || lr 6.271456284701318e-07\n","Epoch[9/10](3300/14999) || training loss 0.01128 || training accuracy 100.00% || lr 6.260753227640645e-07\n","Epoch[9/10](3320/14999) || training loss 0.01109 || training accuracy 99.38% || lr 6.250050170579971e-07\n","Epoch[9/10](3340/14999) || training loss 0.02333 || training accuracy 99.38% || lr 6.239347113519299e-07\n","Epoch[9/10](3360/14999) || training loss 0.0113 || training accuracy 100.00% || lr 6.228644056458627e-07\n","Epoch[9/10](3380/14999) || training loss 0.01393 || training accuracy 99.38% || lr 6.217940999397952e-07\n","Epoch[9/10](3400/14999) || training loss 0.0484 || training accuracy 98.12% || lr 6.20723794233728e-07\n","Epoch[9/10](3420/14999) || training loss 0.07133 || training accuracy 98.75% || lr 6.196534885276607e-07\n","Epoch[9/10](3440/14999) || training loss 0.02361 || training accuracy 98.75% || lr 6.185831828215933e-07\n","Epoch[9/10](3460/14999) || training loss 0.01795 || training accuracy 99.38% || lr 6.175128771155261e-07\n","Epoch[9/10](3480/14999) || training loss 0.05265 || training accuracy 98.75% || lr 6.164425714094588e-07\n","Epoch[9/10](3500/14999) || training loss 0.02023 || training accuracy 98.75% || lr 6.153722657033916e-07\n","Epoch[9/10](3520/14999) || training loss 0.03118 || training accuracy 98.75% || lr 6.143019599973242e-07\n","Epoch[9/10](3540/14999) || training loss 0.00692 || training accuracy 100.00% || lr 6.132316542912569e-07\n","Epoch[9/10](3560/14999) || training loss 0.01824 || training accuracy 99.38% || lr 6.121613485851897e-07\n","Epoch[9/10](3580/14999) || training loss 0.01521 || training accuracy 99.38% || lr 6.110910428791222e-07\n","Epoch[9/10](3600/14999) || training loss 0.02456 || training accuracy 98.75% || lr 6.10020737173055e-07\n","Epoch[9/10](3620/14999) || training loss 0.05124 || training accuracy 98.12% || lr 6.089504314669878e-07\n","Epoch[9/10](3640/14999) || training loss 0.02201 || training accuracy 99.38% || lr 6.078801257609204e-07\n","Epoch[9/10](3660/14999) || training loss 0.0651 || training accuracy 97.50% || lr 6.068098200548531e-07\n","Epoch[9/10](3680/14999) || training loss 0.03613 || training accuracy 99.38% || lr 6.057395143487859e-07\n","Epoch[9/10](3700/14999) || training loss 0.039 || training accuracy 98.12% || lr 6.046692086427186e-07\n","Epoch[9/10](3720/14999) || training loss 0.005996 || training accuracy 100.00% || lr 6.035989029366513e-07\n","Epoch[9/10](3740/14999) || training loss 0.06559 || training accuracy 98.75% || lr 6.025285972305839e-07\n","Epoch[9/10](3760/14999) || training loss 0.01505 || training accuracy 99.38% || lr 6.014582915245167e-07\n","Epoch[9/10](3780/14999) || training loss 0.06834 || training accuracy 98.12% || lr 6.003879858184494e-07\n","Epoch[9/10](3800/14999) || training loss 0.04864 || training accuracy 99.38% || lr 5.99317680112382e-07\n","Epoch[9/10](3820/14999) || training loss 0.04297 || training accuracy 98.12% || lr 5.982473744063148e-07\n","Epoch[9/10](3840/14999) || training loss 0.004524 || training accuracy 100.00% || lr 5.971770687002475e-07\n","Epoch[9/10](3860/14999) || training loss 0.02278 || training accuracy 99.38% || lr 5.961067629941802e-07\n","Epoch[9/10](3880/14999) || training loss 0.05997 || training accuracy 99.38% || lr 5.950364572881129e-07\n","Epoch[9/10](3900/14999) || training loss 0.08779 || training accuracy 96.88% || lr 5.939661515820456e-07\n","Epoch[9/10](3920/14999) || training loss 0.007581 || training accuracy 100.00% || lr 5.928958458759783e-07\n","Epoch[9/10](3940/14999) || training loss 0.0531 || training accuracy 98.12% || lr 5.91825540169911e-07\n","Epoch[9/10](3960/14999) || training loss 0.06535 || training accuracy 98.12% || lr 5.907552344638437e-07\n","Epoch[9/10](3980/14999) || training loss 0.05226 || training accuracy 98.12% || lr 5.896849287577764e-07\n","Epoch[9/10](4000/14999) || training loss 0.03279 || training accuracy 98.75% || lr 5.886146230517092e-07\n","Epoch[9/10](4020/14999) || training loss 0.04213 || training accuracy 98.75% || lr 5.875443173456418e-07\n","Epoch[9/10](4040/14999) || training loss 0.01506 || training accuracy 99.38% || lr 5.864740116395745e-07\n","Epoch[9/10](4060/14999) || training loss 0.02152 || training accuracy 98.75% || lr 5.854037059335072e-07\n","Epoch[9/10](4080/14999) || training loss 0.01198 || training accuracy 100.00% || lr 5.8433340022744e-07\n","Epoch[9/10](4100/14999) || training loss 0.01352 || training accuracy 100.00% || lr 5.832630945213727e-07\n","Epoch[9/10](4120/14999) || training loss 0.04331 || training accuracy 98.75% || lr 5.821927888153053e-07\n","Epoch[9/10](4140/14999) || training loss 0.02978 || training accuracy 99.38% || lr 5.811224831092381e-07\n","Epoch[9/10](4160/14999) || training loss 0.006747 || training accuracy 100.00% || lr 5.800521774031708e-07\n","Epoch[9/10](4180/14999) || training loss 0.03487 || training accuracy 98.75% || lr 5.789818716971034e-07\n","Epoch[9/10](4200/14999) || training loss 0.009479 || training accuracy 100.00% || lr 5.779115659910362e-07\n","Epoch[9/10](4220/14999) || training loss 0.03106 || training accuracy 98.75% || lr 5.768412602849689e-07\n","Epoch[9/10](4240/14999) || training loss 0.04108 || training accuracy 98.75% || lr 5.757709545789015e-07\n","Epoch[9/10](4260/14999) || training loss 0.009947 || training accuracy 100.00% || lr 5.747006488728343e-07\n","Epoch[9/10](4280/14999) || training loss 0.06138 || training accuracy 98.75% || lr 5.73630343166767e-07\n","Epoch[9/10](4300/14999) || training loss 0.04412 || training accuracy 98.12% || lr 5.725600374606996e-07\n","Epoch[9/10](4320/14999) || training loss 0.09018 || training accuracy 96.88% || lr 5.714897317546324e-07\n","Epoch[9/10](4340/14999) || training loss 0.04281 || training accuracy 98.75% || lr 5.704194260485651e-07\n","Epoch[9/10](4360/14999) || training loss 0.04472 || training accuracy 97.50% || lr 5.693491203424979e-07\n","Epoch[9/10](4380/14999) || training loss 0.06094 || training accuracy 98.12% || lr 5.682788146364304e-07\n","Epoch[9/10](4400/14999) || training loss 0.01655 || training accuracy 99.38% || lr 5.672085089303632e-07\n","Epoch[9/10](4420/14999) || training loss 0.025 || training accuracy 99.38% || lr 5.66138203224296e-07\n","Epoch[9/10](4440/14999) || training loss 0.04265 || training accuracy 98.75% || lr 5.650678975182286e-07\n","Epoch[9/10](4460/14999) || training loss 0.03678 || training accuracy 98.12% || lr 5.639975918121613e-07\n","Epoch[9/10](4480/14999) || training loss 0.06242 || training accuracy 98.12% || lr 5.629272861060941e-07\n","Epoch[9/10](4500/14999) || training loss 0.04179 || training accuracy 98.75% || lr 5.618569804000267e-07\n","Epoch[9/10](4520/14999) || training loss 0.04042 || training accuracy 98.75% || lr 5.607866746939594e-07\n","Epoch[9/10](4540/14999) || training loss 0.0718 || training accuracy 98.12% || lr 5.597163689878921e-07\n","Epoch[9/10](4560/14999) || training loss 0.06694 || training accuracy 98.75% || lr 5.586460632818249e-07\n","Epoch[9/10](4580/14999) || training loss 0.0794 || training accuracy 98.12% || lr 5.575757575757576e-07\n","Epoch[9/10](4600/14999) || training loss 0.04088 || training accuracy 98.12% || lr 5.565054518696902e-07\n","Epoch[9/10](4620/14999) || training loss 0.03496 || training accuracy 99.38% || lr 5.55435146163623e-07\n","Epoch[9/10](4640/14999) || training loss 0.02672 || training accuracy 98.12% || lr 5.543648404575557e-07\n","Epoch[9/10](4660/14999) || training loss 0.02404 || training accuracy 99.38% || lr 5.532945347514883e-07\n","Epoch[9/10](4680/14999) || training loss 0.04257 || training accuracy 98.75% || lr 5.522242290454211e-07\n","Epoch[9/10](4700/14999) || training loss 0.01126 || training accuracy 99.38% || lr 5.511539233393537e-07\n","Epoch[9/10](4720/14999) || training loss 0.008997 || training accuracy 99.38% || lr 5.500836176332865e-07\n","Epoch[9/10](4740/14999) || training loss 0.06042 || training accuracy 98.75% || lr 5.490133119272192e-07\n","Epoch[9/10](4760/14999) || training loss 0.07428 || training accuracy 98.75% || lr 5.479430062211519e-07\n","Epoch[9/10](4780/14999) || training loss 0.02611 || training accuracy 98.75% || lr 5.468727005150846e-07\n","Epoch[9/10](4800/14999) || training loss 0.02764 || training accuracy 98.75% || lr 5.458023948090174e-07\n","Epoch[9/10](4820/14999) || training loss 0.02424 || training accuracy 99.38% || lr 5.4473208910295e-07\n","Epoch[9/10](4840/14999) || training loss 0.07399 || training accuracy 96.88% || lr 5.436617833968827e-07\n","Epoch[9/10](4860/14999) || training loss 0.02528 || training accuracy 99.38% || lr 5.425914776908154e-07\n","Epoch[9/10](4880/14999) || training loss 0.0183 || training accuracy 99.38% || lr 5.415211719847481e-07\n","Epoch[9/10](4900/14999) || training loss 0.02119 || training accuracy 99.38% || lr 5.404508662786808e-07\n","Epoch[9/10](4920/14999) || training loss 0.01037 || training accuracy 100.00% || lr 5.393805605726135e-07\n","Epoch[9/10](4940/14999) || training loss 0.008358 || training accuracy 100.00% || lr 5.383102548665463e-07\n","Epoch[9/10](4960/14999) || training loss 0.0177 || training accuracy 99.38% || lr 5.37239949160479e-07\n","Epoch[9/10](4980/14999) || training loss 0.02332 || training accuracy 98.75% || lr 5.361696434544116e-07\n","Epoch[9/10](5000/14999) || training loss 0.0531 || training accuracy 97.50% || lr 5.350993377483444e-07\n","Epoch[9/10](5020/14999) || training loss 0.03239 || training accuracy 98.75% || lr 5.34029032042277e-07\n","Epoch[9/10](5040/14999) || training loss 0.03051 || training accuracy 99.38% || lr 5.329587263362097e-07\n","Epoch[9/10](5060/14999) || training loss 0.03015 || training accuracy 98.75% || lr 5.318884206301425e-07\n","Epoch[9/10](5080/14999) || training loss 0.03056 || training accuracy 99.38% || lr 5.308181149240752e-07\n","Epoch[9/10](5100/14999) || training loss 0.008025 || training accuracy 100.00% || lr 5.297478092180078e-07\n","Epoch[9/10](5120/14999) || training loss 0.06796 || training accuracy 96.88% || lr 5.286775035119406e-07\n","Epoch[9/10](5140/14999) || training loss 0.0427 || training accuracy 98.75% || lr 5.276071978058733e-07\n","Epoch[9/10](5160/14999) || training loss 0.02002 || training accuracy 99.38% || lr 5.265368920998061e-07\n","Epoch[9/10](5180/14999) || training loss 0.041 || training accuracy 98.12% || lr 5.254665863937386e-07\n","Epoch[9/10](5200/14999) || training loss 0.04721 || training accuracy 98.12% || lr 5.243962806876714e-07\n","Epoch[9/10](5220/14999) || training loss 0.05662 || training accuracy 98.75% || lr 5.233259749816042e-07\n","Epoch[9/10](5240/14999) || training loss 0.02112 || training accuracy 99.38% || lr 5.222556692755367e-07\n","Epoch[9/10](5260/14999) || training loss 0.01079 || training accuracy 100.00% || lr 5.211853635694695e-07\n","Epoch[9/10](5280/14999) || training loss 0.06085 || training accuracy 98.12% || lr 5.201150578634023e-07\n","Epoch[9/10](5300/14999) || training loss 0.01768 || training accuracy 99.38% || lr 5.190447521573349e-07\n","Epoch[9/10](5320/14999) || training loss 0.02004 || training accuracy 98.75% || lr 5.179744464512676e-07\n","Epoch[9/10](5340/14999) || training loss 0.0624 || training accuracy 97.50% || lr 5.169041407452003e-07\n","Epoch[9/10](5360/14999) || training loss 0.02917 || training accuracy 98.75% || lr 5.15833835039133e-07\n","Epoch[9/10](5380/14999) || training loss 0.04464 || training accuracy 97.50% || lr 5.147635293330658e-07\n","Epoch[9/10](5400/14999) || training loss 0.02574 || training accuracy 99.38% || lr 5.136932236269984e-07\n","Epoch[9/10](5420/14999) || training loss 0.04196 || training accuracy 98.12% || lr 5.126229179209312e-07\n","Epoch[9/10](5440/14999) || training loss 0.0319 || training accuracy 98.75% || lr 5.115526122148639e-07\n","Epoch[9/10](5460/14999) || training loss 0.05883 || training accuracy 98.75% || lr 5.104823065087965e-07\n","Epoch[9/10](5480/14999) || training loss 0.03166 || training accuracy 99.38% || lr 5.094120008027293e-07\n","Epoch[9/10](5500/14999) || training loss 0.06221 || training accuracy 98.12% || lr 5.083416950966619e-07\n","Epoch[9/10](5520/14999) || training loss 0.06641 || training accuracy 97.50% || lr 5.072713893905947e-07\n","Epoch[9/10](5540/14999) || training loss 0.01653 || training accuracy 99.38% || lr 5.062010836845274e-07\n","Epoch[9/10](5560/14999) || training loss 0.02234 || training accuracy 98.75% || lr 5.0513077797846e-07\n","Epoch[9/10](5580/14999) || training loss 0.009743 || training accuracy 100.00% || lr 5.040604722723928e-07\n","Epoch[9/10](5600/14999) || training loss 0.06071 || training accuracy 97.50% || lr 5.029901665663255e-07\n","Epoch[9/10](5620/14999) || training loss 0.02512 || training accuracy 98.75% || lr 5.019198608602582e-07\n","Epoch[9/10](5640/14999) || training loss 0.02843 || training accuracy 99.38% || lr 5.008495551541909e-07\n","Epoch[9/10](5660/14999) || training loss 0.03285 || training accuracy 98.75% || lr 4.997792494481236e-07\n","Epoch[9/10](5680/14999) || training loss 0.04849 || training accuracy 99.38% || lr 4.987089437420563e-07\n","Epoch[9/10](5700/14999) || training loss 0.0127 || training accuracy 99.38% || lr 4.97638638035989e-07\n","Epoch[9/10](5720/14999) || training loss 0.0229 || training accuracy 99.38% || lr 4.965683323299217e-07\n","Epoch[9/10](5740/14999) || training loss 0.03421 || training accuracy 99.38% || lr 4.954980266238545e-07\n","Epoch[9/10](5760/14999) || training loss 0.03338 || training accuracy 99.38% || lr 4.944277209177871e-07\n","Epoch[9/10](5780/14999) || training loss 0.00781 || training accuracy 100.00% || lr 4.933574152117198e-07\n","Epoch[9/10](5800/14999) || training loss 0.01062 || training accuracy 100.00% || lr 4.922871095056526e-07\n","Epoch[9/10](5820/14999) || training loss 0.06673 || training accuracy 98.75% || lr 4.912168037995853e-07\n","Epoch[9/10](5840/14999) || training loss 0.01668 || training accuracy 99.38% || lr 4.901464980935179e-07\n","Epoch[9/10](5860/14999) || training loss 0.03232 || training accuracy 98.75% || lr 4.890761923874506e-07\n","Epoch[9/10](5880/14999) || training loss 0.01912 || training accuracy 98.75% || lr 4.880058866813834e-07\n","Epoch[9/10](5900/14999) || training loss 0.03304 || training accuracy 98.12% || lr 4.86935580975316e-07\n","Epoch[9/10](5920/14999) || training loss 0.04943 || training accuracy 97.50% || lr 4.858652752692487e-07\n","Epoch[9/10](5940/14999) || training loss 0.05574 || training accuracy 98.75% || lr 4.847949695631815e-07\n","Epoch[9/10](5960/14999) || training loss 0.05189 || training accuracy 98.12% || lr 4.837246638571141e-07\n","Epoch[9/10](5980/14999) || training loss 0.04687 || training accuracy 99.38% || lr 4.826543581510469e-07\n","Epoch[9/10](6000/14999) || training loss 0.01609 || training accuracy 99.38% || lr 4.815840524449796e-07\n","Epoch[9/10](6020/14999) || training loss 0.1053 || training accuracy 97.50% || lr 4.805137467389123e-07\n","Epoch[9/10](6040/14999) || training loss 0.01777 || training accuracy 99.38% || lr 4.79443441032845e-07\n","Epoch[9/10](6060/14999) || training loss 0.0278 || training accuracy 98.12% || lr 4.783731353267777e-07\n","Epoch[9/10](6080/14999) || training loss 0.04722 || training accuracy 97.50% || lr 4.773028296207104e-07\n","Epoch[9/10](6100/14999) || training loss 0.005515 || training accuracy 100.00% || lr 4.762325239146431e-07\n","Epoch[9/10](6120/14999) || training loss 0.08374 || training accuracy 96.88% || lr 4.751622182085758e-07\n","Epoch[9/10](6140/14999) || training loss 0.01974 || training accuracy 98.75% || lr 4.740919125025085e-07\n","Epoch[9/10](6160/14999) || training loss 0.01622 || training accuracy 100.00% || lr 4.730216067964412e-07\n","Epoch[9/10](6180/14999) || training loss 0.01437 || training accuracy 99.38% || lr 4.719513010903739e-07\n","Epoch[9/10](6200/14999) || training loss 0.01813 || training accuracy 98.75% || lr 4.7088099538430663e-07\n","Epoch[9/10](6220/14999) || training loss 0.01143 || training accuracy 100.00% || lr 4.6981068967823935e-07\n","Epoch[9/10](6240/14999) || training loss 0.02785 || training accuracy 98.75% || lr 4.68740383972172e-07\n","Epoch[9/10](6260/14999) || training loss 0.02607 || training accuracy 98.12% || lr 4.6767007826610474e-07\n","Epoch[9/10](6280/14999) || training loss 0.008286 || training accuracy 99.38% || lr 4.6659977256003746e-07\n","Epoch[9/10](6300/14999) || training loss 0.07119 || training accuracy 98.75% || lr 4.6552946685397013e-07\n","Epoch[9/10](6320/14999) || training loss 0.02463 || training accuracy 99.38% || lr 4.6445916114790285e-07\n","Epoch[9/10](6340/14999) || training loss 0.02196 || training accuracy 98.75% || lr 4.633888554418355e-07\n","Epoch[9/10](6360/14999) || training loss 0.07583 || training accuracy 96.25% || lr 4.623185497357683e-07\n","Epoch[9/10](6380/14999) || training loss 0.04613 || training accuracy 98.75% || lr 4.6124824402970096e-07\n","Epoch[9/10](6400/14999) || training loss 0.0472 || training accuracy 98.12% || lr 4.601779383236337e-07\n","Epoch[9/10](6420/14999) || training loss 0.02054 || training accuracy 99.38% || lr 4.5910763261756635e-07\n","Epoch[9/10](6440/14999) || training loss 0.02625 || training accuracy 99.38% || lr 4.580373269114991e-07\n","Epoch[9/10](6460/14999) || training loss 0.0136 || training accuracy 99.38% || lr 4.569670212054318e-07\n","Epoch[9/10](6480/14999) || training loss 0.03493 || training accuracy 98.75% || lr 4.5589671549936446e-07\n","Epoch[9/10](6500/14999) || training loss 0.02416 || training accuracy 98.75% || lr 4.548264097932972e-07\n","Epoch[9/10](6520/14999) || training loss 0.03188 || training accuracy 98.75% || lr 4.537561040872299e-07\n","Epoch[9/10](6540/14999) || training loss 0.0864 || training accuracy 98.12% || lr 4.526857983811626e-07\n","Epoch[9/10](6560/14999) || training loss 0.03217 || training accuracy 98.75% || lr 4.516154926750953e-07\n","Epoch[9/10](6580/14999) || training loss 0.06643 || training accuracy 98.75% || lr 4.50545186969028e-07\n","Epoch[9/10](6600/14999) || training loss 0.01622 || training accuracy 99.38% || lr 4.4947488126296073e-07\n","Epoch[9/10](6620/14999) || training loss 0.007655 || training accuracy 100.00% || lr 4.4840457555689345e-07\n","Epoch[9/10](6640/14999) || training loss 0.06111 || training accuracy 97.50% || lr 4.473342698508261e-07\n","Epoch[9/10](6660/14999) || training loss 0.004983 || training accuracy 100.00% || lr 4.462639641447588e-07\n","Epoch[9/10](6680/14999) || training loss 0.01004 || training accuracy 100.00% || lr 4.4519365843869156e-07\n","Epoch[9/10](6700/14999) || training loss 0.05597 || training accuracy 96.88% || lr 4.441233527326242e-07\n","Epoch[9/10](6720/14999) || training loss 0.03889 || training accuracy 98.12% || lr 4.4305304702655695e-07\n","Epoch[9/10](6740/14999) || training loss 0.05468 || training accuracy 99.38% || lr 4.419827413204896e-07\n","Epoch[9/10](6760/14999) || training loss 0.03402 || training accuracy 99.38% || lr 4.409124356144224e-07\n","Epoch[9/10](6780/14999) || training loss 0.01119 || training accuracy 99.38% || lr 4.3984212990835506e-07\n","Epoch[9/10](6800/14999) || training loss 0.05037 || training accuracy 98.75% || lr 4.387718242022878e-07\n","Epoch[9/10](6820/14999) || training loss 0.05399 || training accuracy 96.88% || lr 4.3770151849622044e-07\n","Epoch[9/10](6840/14999) || training loss 0.0291 || training accuracy 99.38% || lr 4.366312127901532e-07\n","Epoch[9/10](6860/14999) || training loss 0.01814 || training accuracy 99.38% || lr 4.355609070840859e-07\n","Epoch[9/10](6880/14999) || training loss 0.06936 || training accuracy 98.75% || lr 4.3449060137801855e-07\n","Epoch[9/10](6900/14999) || training loss 0.06841 || training accuracy 98.75% || lr 4.334202956719513e-07\n","Epoch[9/10](6920/14999) || training loss 0.02845 || training accuracy 99.38% || lr 4.32349989965884e-07\n","Epoch[9/10](6940/14999) || training loss 0.007287 || training accuracy 100.00% || lr 4.312796842598167e-07\n","Epoch[9/10](6960/14999) || training loss 0.005476 || training accuracy 100.00% || lr 4.302093785537494e-07\n","Epoch[9/10](6980/14999) || training loss 0.04893 || training accuracy 98.12% || lr 4.291390728476821e-07\n","Epoch[9/10](7000/14999) || training loss 0.01919 || training accuracy 98.75% || lr 4.280687671416148e-07\n","Epoch[9/10](7020/14999) || training loss 0.03217 || training accuracy 98.12% || lr 4.2699846143554755e-07\n","Epoch[9/10](7040/14999) || training loss 0.01594 || training accuracy 99.38% || lr 4.259281557294802e-07\n","Epoch[9/10](7060/14999) || training loss 0.0297 || training accuracy 98.75% || lr 4.248578500234129e-07\n","Epoch[9/10](7080/14999) || training loss 0.006392 || training accuracy 100.00% || lr 4.2378754431734565e-07\n","Epoch[9/10](7100/14999) || training loss 0.02692 || training accuracy 98.75% || lr 4.227172386112783e-07\n","Epoch[9/10](7120/14999) || training loss 0.05967 || training accuracy 98.75% || lr 4.2164693290521104e-07\n","Epoch[9/10](7140/14999) || training loss 0.02818 || training accuracy 98.75% || lr 4.205766271991437e-07\n","Epoch[9/10](7160/14999) || training loss 0.0157 || training accuracy 100.00% || lr 4.195063214930765e-07\n","Epoch[9/10](7180/14999) || training loss 0.07202 || training accuracy 98.12% || lr 4.1843601578700915e-07\n","Epoch[9/10](7200/14999) || training loss 0.01165 || training accuracy 99.38% || lr 4.173657100809418e-07\n","Epoch[9/10](7220/14999) || training loss 0.02283 || training accuracy 98.75% || lr 4.1629540437487454e-07\n","Epoch[9/10](7240/14999) || training loss 0.01865 || training accuracy 99.38% || lr 4.1522509866880726e-07\n","Epoch[9/10](7260/14999) || training loss 0.009403 || training accuracy 100.00% || lr 4.1415479296274e-07\n","Epoch[9/10](7280/14999) || training loss 0.02876 || training accuracy 99.38% || lr 4.1308448725667265e-07\n","Epoch[9/10](7300/14999) || training loss 0.05004 || training accuracy 97.50% || lr 4.1201418155060537e-07\n","Epoch[9/10](7320/14999) || training loss 0.06675 || training accuracy 98.75% || lr 4.109438758445381e-07\n","Epoch[9/10](7340/14999) || training loss 0.02652 || training accuracy 98.75% || lr 4.098735701384708e-07\n","Epoch[9/10](7360/14999) || training loss 0.05654 || training accuracy 98.12% || lr 4.088032644324035e-07\n","Epoch[9/10](7380/14999) || training loss 0.1154 || training accuracy 96.88% || lr 4.0773295872633615e-07\n","Epoch[9/10](7400/14999) || training loss 0.003672 || training accuracy 100.00% || lr 4.066626530202689e-07\n","Epoch[9/10](7420/14999) || training loss 0.04682 || training accuracy 98.12% || lr 4.055923473142016e-07\n","Epoch[9/10](7440/14999) || training loss 0.006214 || training accuracy 100.00% || lr 4.045220416081343e-07\n","Epoch[9/10](7460/14999) || training loss 0.0185 || training accuracy 98.75% || lr 4.03451735902067e-07\n","Epoch[9/10](7480/14999) || training loss 0.05173 || training accuracy 98.12% || lr 4.0238143019599975e-07\n","Epoch[9/10](7500/14999) || training loss 0.007903 || training accuracy 100.00% || lr 4.013111244899324e-07\n","Epoch[9/10](7520/14999) || training loss 0.0398 || training accuracy 98.12% || lr 4.0024081878386514e-07\n","Epoch[9/10](7540/14999) || training loss 0.03177 || training accuracy 98.75% || lr 3.991705130777978e-07\n","Epoch[9/10](7560/14999) || training loss 0.01295 || training accuracy 100.00% || lr 3.981002073717306e-07\n","Epoch[9/10](7580/14999) || training loss 0.0272 || training accuracy 98.75% || lr 3.9702990166566325e-07\n","Epoch[9/10](7600/14999) || training loss 0.03375 || training accuracy 98.12% || lr 3.959595959595959e-07\n","Epoch[9/10](7620/14999) || training loss 0.06325 || training accuracy 98.75% || lr 3.9488929025352864e-07\n","Epoch[9/10](7640/14999) || training loss 0.1347 || training accuracy 96.25% || lr 3.9381898454746136e-07\n","Epoch[9/10](7660/14999) || training loss 0.01702 || training accuracy 99.38% || lr 3.927486788413941e-07\n","Epoch[9/10](7680/14999) || training loss 0.0103 || training accuracy 100.00% || lr 3.9167837313532675e-07\n","Epoch[9/10](7700/14999) || training loss 0.009654 || training accuracy 100.00% || lr 3.9060806742925947e-07\n","Epoch[9/10](7720/14999) || training loss 0.04137 || training accuracy 98.12% || lr 3.895377617231922e-07\n","Epoch[9/10](7740/14999) || training loss 0.03056 || training accuracy 98.75% || lr 3.884674560171249e-07\n","Epoch[9/10](7760/14999) || training loss 0.02334 || training accuracy 98.12% || lr 3.873971503110576e-07\n","Epoch[9/10](7780/14999) || training loss 0.02268 || training accuracy 99.38% || lr 3.8632684460499024e-07\n","Epoch[9/10](7800/14999) || training loss 0.008603 || training accuracy 100.00% || lr 3.85256538898923e-07\n","Epoch[9/10](7820/14999) || training loss 0.008475 || training accuracy 100.00% || lr 3.841862331928557e-07\n","Epoch[9/10](7840/14999) || training loss 0.03685 || training accuracy 98.12% || lr 3.831159274867884e-07\n","Epoch[9/10](7860/14999) || training loss 0.01583 || training accuracy 98.75% || lr 3.820456217807211e-07\n","Epoch[9/10](7880/14999) || training loss 0.0387 || training accuracy 98.75% || lr 3.8097531607465385e-07\n","Epoch[9/10](7900/14999) || training loss 0.02527 || training accuracy 99.38% || lr 3.799050103685865e-07\n","Epoch[9/10](7920/14999) || training loss 0.01652 || training accuracy 99.38% || lr 3.7883470466251924e-07\n","Epoch[9/10](7940/14999) || training loss 0.02353 || training accuracy 99.38% || lr 3.777643989564519e-07\n","Epoch[9/10](7960/14999) || training loss 0.03525 || training accuracy 98.75% || lr 3.766940932503846e-07\n","Epoch[9/10](7980/14999) || training loss 0.06109 || training accuracy 97.50% || lr 3.7562378754431735e-07\n","Epoch[9/10](8000/14999) || training loss 0.03515 || training accuracy 98.75% || lr 3.7455348183825e-07\n","Epoch[9/10](8020/14999) || training loss 0.008925 || training accuracy 100.00% || lr 3.7348317613218273e-07\n","Epoch[9/10](8040/14999) || training loss 0.03111 || training accuracy 98.75% || lr 3.7241287042611545e-07\n","Epoch[9/10](8060/14999) || training loss 0.02185 || training accuracy 98.75% || lr 3.713425647200482e-07\n","Epoch[9/10](8080/14999) || training loss 0.02728 || training accuracy 98.75% || lr 3.7027225901398084e-07\n","Epoch[9/10](8100/14999) || training loss 0.03066 || training accuracy 97.50% || lr 3.6920195330791356e-07\n","Epoch[9/10](8120/14999) || training loss 0.0192 || training accuracy 99.38% || lr 3.681316476018463e-07\n","Epoch[9/10](8140/14999) || training loss 0.09574 || training accuracy 97.50% || lr 3.6706134189577895e-07\n","Epoch[9/10](8160/14999) || training loss 0.01936 || training accuracy 98.75% || lr 3.6599103618971167e-07\n","Epoch[9/10](8180/14999) || training loss 0.05197 || training accuracy 99.38% || lr 3.6492073048364434e-07\n","Epoch[9/10](8200/14999) || training loss 0.007979 || training accuracy 100.00% || lr 3.638504247775771e-07\n","Epoch[9/10](8220/14999) || training loss 0.02607 || training accuracy 98.75% || lr 3.627801190715098e-07\n","Epoch[9/10](8240/14999) || training loss 0.02953 || training accuracy 98.75% || lr 3.617098133654425e-07\n","Epoch[9/10](8260/14999) || training loss 0.01642 || training accuracy 99.38% || lr 3.6063950765937517e-07\n","Epoch[9/10](8280/14999) || training loss 0.09372 || training accuracy 96.88% || lr 3.5956920195330794e-07\n","Epoch[9/10](8300/14999) || training loss 0.04391 || training accuracy 98.12% || lr 3.584988962472406e-07\n","Epoch[9/10](8320/14999) || training loss 0.02249 || training accuracy 98.75% || lr 3.574285905411733e-07\n","Epoch[9/10](8340/14999) || training loss 0.01566 || training accuracy 98.75% || lr 3.56358284835106e-07\n","Epoch[9/10](8360/14999) || training loss 0.05918 || training accuracy 99.38% || lr 3.552879791290387e-07\n","Epoch[9/10](8380/14999) || training loss 0.05904 || training accuracy 97.50% || lr 3.5421767342297144e-07\n","Epoch[9/10](8400/14999) || training loss 0.03845 || training accuracy 98.75% || lr 3.531473677169041e-07\n","Epoch[9/10](8420/14999) || training loss 0.07856 || training accuracy 98.75% || lr 3.5207706201083683e-07\n","Epoch[9/10](8440/14999) || training loss 0.01808 || training accuracy 99.38% || lr 3.5100675630476955e-07\n","Epoch[9/10](8460/14999) || training loss 0.04849 || training accuracy 99.38% || lr 3.4993645059870227e-07\n","Epoch[9/10](8480/14999) || training loss 0.02601 || training accuracy 99.38% || lr 3.4886614489263494e-07\n","Epoch[9/10](8500/14999) || training loss 0.01028 || training accuracy 99.38% || lr 3.477958391865676e-07\n","Epoch[9/10](8520/14999) || training loss 0.02956 || training accuracy 99.38% || lr 3.467255334805004e-07\n","Epoch[9/10](8540/14999) || training loss 0.02173 || training accuracy 98.75% || lr 3.4565522777443305e-07\n","Epoch[9/10](8560/14999) || training loss 0.04442 || training accuracy 98.12% || lr 3.4458492206836577e-07\n","Epoch[9/10](8580/14999) || training loss 0.03104 || training accuracy 98.75% || lr 3.4351461636229844e-07\n","Epoch[9/10](8600/14999) || training loss 0.03141 || training accuracy 98.12% || lr 3.424443106562312e-07\n","Epoch[9/10](8620/14999) || training loss 0.0879 || training accuracy 96.25% || lr 3.413740049501639e-07\n","Epoch[9/10](8640/14999) || training loss 0.01338 || training accuracy 100.00% || lr 3.403036992440966e-07\n","Epoch[9/10](8660/14999) || training loss 0.01255 || training accuracy 100.00% || lr 3.3923339353802927e-07\n","Epoch[9/10](8680/14999) || training loss 0.02206 || training accuracy 98.75% || lr 3.38163087831962e-07\n","Epoch[9/10](8700/14999) || training loss 0.05095 || training accuracy 98.75% || lr 3.370927821258947e-07\n","Epoch[9/10](8720/14999) || training loss 0.02131 || training accuracy 98.75% || lr 3.360224764198274e-07\n","Epoch[9/10](8740/14999) || training loss 0.01287 || training accuracy 100.00% || lr 3.349521707137601e-07\n","Epoch[9/10](8760/14999) || training loss 0.08878 || training accuracy 98.12% || lr 3.338818650076928e-07\n","Epoch[9/10](8780/14999) || training loss 0.0151 || training accuracy 99.38% || lr 3.3281155930162554e-07\n","Epoch[9/10](8800/14999) || training loss 0.07204 || training accuracy 97.50% || lr 3.317412535955582e-07\n","Epoch[9/10](8820/14999) || training loss 0.003393 || training accuracy 100.00% || lr 3.3067094788949093e-07\n","Epoch[9/10](8840/14999) || training loss 0.02969 || training accuracy 98.75% || lr 3.2960064218342365e-07\n","Epoch[9/10](8860/14999) || training loss 0.0185 || training accuracy 98.75% || lr 3.285303364773563e-07\n","Epoch[9/10](8880/14999) || training loss 0.07211 || training accuracy 97.50% || lr 3.2746003077128904e-07\n","Epoch[9/10](8900/14999) || training loss 0.05441 || training accuracy 98.75% || lr 3.263897250652217e-07\n","Epoch[9/10](8920/14999) || training loss 0.05378 || training accuracy 99.38% || lr 3.253194193591545e-07\n","Epoch[9/10](8940/14999) || training loss 0.0077 || training accuracy 100.00% || lr 3.2424911365308715e-07\n","Epoch[9/10](8960/14999) || training loss 0.003866 || training accuracy 100.00% || lr 3.2317880794701987e-07\n","Epoch[9/10](8980/14999) || training loss 0.01309 || training accuracy 100.00% || lr 3.2210850224095253e-07\n","Epoch[9/10](9000/14999) || training loss 0.01157 || training accuracy 100.00% || lr 3.210381965348853e-07\n","Epoch[9/10](9020/14999) || training loss 0.05595 || training accuracy 96.25% || lr 3.19967890828818e-07\n","Epoch[9/10](9040/14999) || training loss 0.00543 || training accuracy 100.00% || lr 3.1889758512275064e-07\n","Epoch[9/10](9060/14999) || training loss 0.009976 || training accuracy 100.00% || lr 3.1782727941668336e-07\n","Epoch[9/10](9080/14999) || training loss 0.01506 || training accuracy 99.38% || lr 3.167569737106161e-07\n","Epoch[9/10](9100/14999) || training loss 0.05089 || training accuracy 98.12% || lr 3.156866680045488e-07\n","Epoch[9/10](9120/14999) || training loss 0.1397 || training accuracy 96.25% || lr 3.1461636229848147e-07\n","Epoch[9/10](9140/14999) || training loss 0.05216 || training accuracy 98.75% || lr 3.135460565924142e-07\n","Epoch[9/10](9160/14999) || training loss 0.08076 || training accuracy 98.12% || lr 3.124757508863469e-07\n","Epoch[9/10](9180/14999) || training loss 0.004527 || training accuracy 100.00% || lr 3.1140544518027963e-07\n","Epoch[9/10](9200/14999) || training loss 0.05638 || training accuracy 99.38% || lr 3.103351394742123e-07\n","Epoch[9/10](9220/14999) || training loss 0.01246 || training accuracy 100.00% || lr 3.0926483376814497e-07\n","Epoch[9/10](9240/14999) || training loss 0.01074 || training accuracy 99.38% || lr 3.0819452806207774e-07\n","Epoch[9/10](9260/14999) || training loss 0.02842 || training accuracy 99.38% || lr 3.071242223560104e-07\n","Epoch[9/10](9280/14999) || training loss 0.04124 || training accuracy 98.12% || lr 3.0605391664994313e-07\n","Epoch[9/10](9300/14999) || training loss 0.02766 || training accuracy 98.75% || lr 3.049836109438758e-07\n","Epoch[9/10](9320/14999) || training loss 0.01272 || training accuracy 99.38% || lr 3.039133052378086e-07\n","Epoch[9/10](9340/14999) || training loss 0.06004 || training accuracy 98.12% || lr 3.0284299953174124e-07\n","Epoch[9/10](9360/14999) || training loss 0.02369 || training accuracy 98.75% || lr 3.0177269382567396e-07\n","Epoch[9/10](9380/14999) || training loss 0.04239 || training accuracy 98.75% || lr 3.0070238811960663e-07\n","Epoch[9/10](9400/14999) || training loss 0.05241 || training accuracy 97.50% || lr 2.996320824135394e-07\n","Epoch[9/10](9420/14999) || training loss 0.006825 || training accuracy 100.00% || lr 2.9856177670747207e-07\n","Epoch[9/10](9440/14999) || training loss 0.02865 || training accuracy 98.12% || lr 2.9749147100140474e-07\n","Epoch[9/10](9460/14999) || training loss 0.06101 || training accuracy 97.50% || lr 2.9642116529533746e-07\n","Epoch[9/10](9480/14999) || training loss 0.0157 || training accuracy 99.38% || lr 2.953508595892702e-07\n","Epoch[9/10](9500/14999) || training loss 0.03751 || training accuracy 98.75% || lr 2.942805538832029e-07\n","Epoch[9/10](9520/14999) || training loss 0.07564 || training accuracy 98.12% || lr 2.9321024817713557e-07\n","Epoch[9/10](9540/14999) || training loss 0.05456 || training accuracy 98.75% || lr 2.921399424710683e-07\n","Epoch[9/10](9560/14999) || training loss 0.02831 || training accuracy 98.12% || lr 2.91069636765001e-07\n","Epoch[9/10](9580/14999) || training loss 0.02567 || training accuracy 99.38% || lr 2.8999933105893373e-07\n","Epoch[9/10](9600/14999) || training loss 0.03432 || training accuracy 98.75% || lr 2.889290253528664e-07\n","Epoch[9/10](9620/14999) || training loss 0.03789 || training accuracy 98.12% || lr 2.8785871964679907e-07\n","Epoch[9/10](9640/14999) || training loss 0.06431 || training accuracy 98.75% || lr 2.8678841394073184e-07\n","Epoch[9/10](9660/14999) || training loss 0.01895 || training accuracy 99.38% || lr 2.857181082346645e-07\n","Epoch[9/10](9680/14999) || training loss 0.06441 || training accuracy 99.38% || lr 2.8464780252859723e-07\n","Epoch[9/10](9700/14999) || training loss 0.0107 || training accuracy 100.00% || lr 2.835774968225299e-07\n","Epoch[9/10](9720/14999) || training loss 0.06368 || training accuracy 97.50% || lr 2.8250719111646267e-07\n","Epoch[9/10](9740/14999) || training loss 0.004149 || training accuracy 100.00% || lr 2.8143688541039534e-07\n","Epoch[9/10](9760/14999) || training loss 0.02952 || training accuracy 99.38% || lr 2.80366579704328e-07\n","Epoch[9/10](9780/14999) || training loss 0.02082 || training accuracy 99.38% || lr 2.7929627399826073e-07\n","Epoch[9/10](9800/14999) || training loss 0.05151 || training accuracy 97.50% || lr 2.7822596829219345e-07\n","Epoch[9/10](9820/14999) || training loss 0.04272 || training accuracy 98.12% || lr 2.7715566258612617e-07\n","Epoch[9/10](9840/14999) || training loss 0.04515 || training accuracy 98.12% || lr 2.7608535688005884e-07\n","Epoch[9/10](9860/14999) || training loss 0.047 || training accuracy 97.50% || lr 2.7501505117399156e-07\n","Epoch[9/10](9880/14999) || training loss 0.02739 || training accuracy 98.75% || lr 2.739447454679243e-07\n","Epoch[9/10](9900/14999) || training loss 0.01415 || training accuracy 99.38% || lr 2.72874439761857e-07\n","Epoch[9/10](9920/14999) || training loss 0.03426 || training accuracy 98.75% || lr 2.7180413405578967e-07\n","Epoch[9/10](9940/14999) || training loss 0.04234 || training accuracy 98.75% || lr 2.7073382834972233e-07\n","Epoch[9/10](9960/14999) || training loss 0.0329 || training accuracy 98.75% || lr 2.696635226436551e-07\n","Epoch[9/10](9980/14999) || training loss 0.02422 || training accuracy 99.38% || lr 2.685932169375878e-07\n","Epoch[9/10](10000/14999) || training loss 0.0578 || training accuracy 99.38% || lr 2.675229112315205e-07\n","Epoch[9/10](10020/14999) || training loss 0.08954 || training accuracy 97.50% || lr 2.6645260552545316e-07\n","Epoch[9/10](10040/14999) || training loss 0.03731 || training accuracy 97.50% || lr 2.6538229981938594e-07\n","Epoch[9/10](10060/14999) || training loss 0.005105 || training accuracy 100.00% || lr 2.643119941133186e-07\n","Epoch[9/10](10080/14999) || training loss 0.008654 || training accuracy 99.38% || lr 2.632416884072513e-07\n","Epoch[9/10](10100/14999) || training loss 0.05427 || training accuracy 98.12% || lr 2.62171382701184e-07\n","Epoch[9/10](10120/14999) || training loss 0.02528 || training accuracy 98.75% || lr 2.6110107699511677e-07\n","Epoch[9/10](10140/14999) || training loss 0.01918 || training accuracy 100.00% || lr 2.6003077128904943e-07\n","Epoch[9/10](10160/14999) || training loss 0.009561 || training accuracy 100.00% || lr 2.589604655829821e-07\n","Epoch[9/10](10180/14999) || training loss 0.002806 || training accuracy 100.00% || lr 2.578901598769148e-07\n","Epoch[9/10](10200/14999) || training loss 0.005724 || training accuracy 100.00% || lr 2.5681985417084754e-07\n","Epoch[9/10](10220/14999) || training loss 0.06808 || training accuracy 97.50% || lr 2.5574954846478026e-07\n","Epoch[9/10](10240/14999) || training loss 0.007988 || training accuracy 100.00% || lr 2.5467924275871293e-07\n","Epoch[9/10](10260/14999) || training loss 0.08561 || training accuracy 98.12% || lr 2.5360893705264565e-07\n","Epoch[9/10](10280/14999) || training loss 0.02006 || training accuracy 99.38% || lr 2.525386313465784e-07\n","Epoch[9/10](10300/14999) || training loss 0.008973 || training accuracy 100.00% || lr 2.514683256405111e-07\n","Epoch[9/10](10320/14999) || training loss 0.06184 || training accuracy 98.75% || lr 2.5039801993444376e-07\n","Epoch[9/10](10340/14999) || training loss 0.01482 || training accuracy 99.38% || lr 2.493277142283765e-07\n","Epoch[9/10](10360/14999) || training loss 0.01469 || training accuracy 99.38% || lr 2.4825740852230915e-07\n","Epoch[9/10](10380/14999) || training loss 0.04777 || training accuracy 99.38% || lr 2.4718710281624187e-07\n","Epoch[9/10](10400/14999) || training loss 0.03861 || training accuracy 98.75% || lr 2.461167971101746e-07\n","Epoch[9/10](10420/14999) || training loss 0.05688 || training accuracy 97.50% || lr 2.450464914041073e-07\n","Epoch[9/10](10440/14999) || training loss 0.03557 || training accuracy 98.12% || lr 2.4397618569804e-07\n","Epoch[9/10](10460/14999) || training loss 0.07211 || training accuracy 98.12% || lr 2.429058799919727e-07\n","Epoch[9/10](10480/14999) || training loss 0.07143 || training accuracy 98.75% || lr 2.418355742859054e-07\n","Epoch[9/10](10500/14999) || training loss 0.02844 || training accuracy 98.75% || lr 2.407652685798381e-07\n","Epoch[9/10](10520/14999) || training loss 0.0312 || training accuracy 99.38% || lr 2.396949628737708e-07\n","Epoch[9/10](10540/14999) || training loss 0.01126 || training accuracy 99.38% || lr 2.3862465716770353e-07\n","Epoch[9/10](10560/14999) || training loss 0.007198 || training accuracy 100.00% || lr 2.3755435146163623e-07\n","Epoch[9/10](10580/14999) || training loss 0.02059 || training accuracy 98.75% || lr 2.3648404575556895e-07\n","Epoch[9/10](10600/14999) || training loss 0.01102 || training accuracy 99.38% || lr 2.3541374004950161e-07\n","Epoch[9/10](10620/14999) || training loss 0.02398 || training accuracy 99.38% || lr 2.3434343434343433e-07\n","Epoch[9/10](10640/14999) || training loss 0.0587 || training accuracy 98.75% || lr 2.3327312863736703e-07\n","Epoch[9/10](10660/14999) || training loss 0.00579 || training accuracy 100.00% || lr 2.3220282293129975e-07\n","Epoch[9/10](10680/14999) || training loss 0.01541 || training accuracy 99.38% || lr 2.3113251722523244e-07\n","Epoch[9/10](10700/14999) || training loss 0.01846 || training accuracy 99.38% || lr 2.3006221151916516e-07\n","Epoch[9/10](10720/14999) || training loss 0.03899 || training accuracy 98.12% || lr 2.2899190581309786e-07\n","Epoch[9/10](10740/14999) || training loss 0.0223 || training accuracy 99.38% || lr 2.2792160010703058e-07\n","Epoch[9/10](10760/14999) || training loss 0.01109 || training accuracy 99.38% || lr 2.2685129440096327e-07\n","Epoch[9/10](10780/14999) || training loss 0.01454 || training accuracy 100.00% || lr 2.2578098869489597e-07\n","Epoch[9/10](10800/14999) || training loss 0.0269 || training accuracy 98.75% || lr 2.2471068298882866e-07\n","Epoch[9/10](10820/14999) || training loss 0.008034 || training accuracy 99.38% || lr 2.2364037728276138e-07\n","Epoch[9/10](10840/14999) || training loss 0.006728 || training accuracy 100.00% || lr 2.2257007157669408e-07\n","Epoch[9/10](10860/14999) || training loss 0.02234 || training accuracy 99.38% || lr 2.214997658706268e-07\n","Epoch[9/10](10880/14999) || training loss 0.04197 || training accuracy 98.75% || lr 2.204294601645595e-07\n","Epoch[9/10](10900/14999) || training loss 0.0298 || training accuracy 98.75% || lr 2.193591544584922e-07\n","Epoch[9/10](10920/14999) || training loss 0.0533 || training accuracy 97.50% || lr 2.182888487524249e-07\n","Epoch[9/10](10940/14999) || training loss 0.07714 || training accuracy 98.12% || lr 2.1721854304635763e-07\n","Epoch[9/10](10960/14999) || training loss 0.0245 || training accuracy 98.75% || lr 2.161482373402903e-07\n","Epoch[9/10](10980/14999) || training loss 0.02678 || training accuracy 98.12% || lr 2.1507793163422302e-07\n","Epoch[9/10](11000/14999) || training loss 0.04454 || training accuracy 98.75% || lr 2.140076259281557e-07\n","Epoch[9/10](11020/14999) || training loss 0.008414 || training accuracy 100.00% || lr 2.1293732022208843e-07\n","Epoch[9/10](11040/14999) || training loss 0.05212 || training accuracy 99.38% || lr 2.1186701451602113e-07\n","Epoch[9/10](11060/14999) || training loss 0.04778 || training accuracy 98.12% || lr 2.1079670880995385e-07\n","Epoch[9/10](11080/14999) || training loss 0.02211 || training accuracy 98.75% || lr 2.0972640310388654e-07\n","Epoch[9/10](11100/14999) || training loss 0.05439 || training accuracy 99.38% || lr 2.0865609739781926e-07\n","Epoch[9/10](11120/14999) || training loss 0.05327 || training accuracy 99.38% || lr 2.0758579169175196e-07\n","Epoch[9/10](11140/14999) || training loss 0.01082 || training accuracy 99.38% || lr 2.0651548598568465e-07\n","Epoch[9/10](11160/14999) || training loss 0.04121 || training accuracy 96.25% || lr 2.0544518027961734e-07\n","Epoch[9/10](11180/14999) || training loss 0.07309 || training accuracy 98.12% || lr 2.0437487457355006e-07\n","Epoch[9/10](11200/14999) || training loss 0.04387 || training accuracy 97.50% || lr 2.0330456886748276e-07\n","Epoch[9/10](11220/14999) || training loss 0.06677 || training accuracy 96.88% || lr 2.0223426316141548e-07\n","Epoch[9/10](11240/14999) || training loss 0.01654 || training accuracy 99.38% || lr 2.0116395745534817e-07\n","Epoch[9/10](11260/14999) || training loss 0.009106 || training accuracy 100.00% || lr 2.000936517492809e-07\n","Epoch[9/10](11280/14999) || training loss 0.07292 || training accuracy 98.12% || lr 1.990233460432136e-07\n","Epoch[9/10](11300/14999) || training loss 0.03707 || training accuracy 97.50% || lr 1.979530403371463e-07\n","Epoch[9/10](11320/14999) || training loss 0.01213 || training accuracy 100.00% || lr 1.9688273463107898e-07\n","Epoch[9/10](11340/14999) || training loss 0.02905 || training accuracy 99.38% || lr 1.958124289250117e-07\n","Epoch[9/10](11360/14999) || training loss 0.04726 || training accuracy 99.38% || lr 1.947421232189444e-07\n","Epoch[9/10](11380/14999) || training loss 0.02152 || training accuracy 98.75% || lr 1.936718175128771e-07\n","Epoch[9/10](11400/14999) || training loss 0.008105 || training accuracy 100.00% || lr 1.926015118068098e-07\n","Epoch[9/10](11420/14999) || training loss 0.01002 || training accuracy 100.00% || lr 1.9153120610074253e-07\n","Epoch[9/10](11440/14999) || training loss 0.06827 || training accuracy 98.75% || lr 1.9046090039467522e-07\n","Epoch[9/10](11460/14999) || training loss 0.1205 || training accuracy 96.88% || lr 1.8939059468860794e-07\n","Epoch[9/10](11480/14999) || training loss 0.04523 || training accuracy 97.50% || lr 1.8832028898254064e-07\n","Epoch[9/10](11500/14999) || training loss 0.04984 || training accuracy 96.88% || lr 1.8724998327647336e-07\n","Epoch[9/10](11520/14999) || training loss 0.01321 || training accuracy 99.38% || lr 1.8617967757040603e-07\n","Epoch[9/10](11540/14999) || training loss 0.0306 || training accuracy 98.75% || lr 1.8510937186433875e-07\n","Epoch[9/10](11560/14999) || training loss 0.05813 || training accuracy 98.12% || lr 1.8403906615827144e-07\n","Epoch[9/10](11580/14999) || training loss 0.0218 || training accuracy 98.75% || lr 1.8296876045220416e-07\n","Epoch[9/10](11600/14999) || training loss 0.08502 || training accuracy 97.50% || lr 1.8189845474613686e-07\n","Epoch[9/10](11620/14999) || training loss 0.02488 || training accuracy 98.75% || lr 1.8082814904006958e-07\n","Epoch[9/10](11640/14999) || training loss 0.01518 || training accuracy 99.38% || lr 1.7975784333400227e-07\n","Epoch[9/10](11660/14999) || training loss 0.01152 || training accuracy 100.00% || lr 1.78687537627935e-07\n","Epoch[9/10](11680/14999) || training loss 0.03771 || training accuracy 98.75% || lr 1.7761723192186766e-07\n","Epoch[9/10](11700/14999) || training loss 0.04055 || training accuracy 98.75% || lr 1.7654692621580038e-07\n","Epoch[9/10](11720/14999) || training loss 0.07737 || training accuracy 98.12% || lr 1.7547662050973307e-07\n","Epoch[9/10](11740/14999) || training loss 0.06735 || training accuracy 98.75% || lr 1.744063148036658e-07\n","Epoch[9/10](11760/14999) || training loss 0.02166 || training accuracy 99.38% || lr 1.733360090975985e-07\n","Epoch[9/10](11780/14999) || training loss 0.007897 || training accuracy 99.38% || lr 1.722657033915312e-07\n","Epoch[9/10](11800/14999) || training loss 0.05642 || training accuracy 98.12% || lr 1.711953976854639e-07\n","Epoch[9/10](11820/14999) || training loss 0.004963 || training accuracy 100.00% || lr 1.7012509197939662e-07\n","Epoch[9/10](11840/14999) || training loss 0.03101 || training accuracy 99.38% || lr 1.6905478627332932e-07\n","Epoch[9/10](11860/14999) || training loss 0.03553 || training accuracy 99.38% || lr 1.6798448056726204e-07\n","Epoch[9/10](11880/14999) || training loss 0.03097 || training accuracy 99.38% || lr 1.669141748611947e-07\n","Epoch[9/10](11900/14999) || training loss 0.06248 || training accuracy 97.50% || lr 1.6584386915512743e-07\n","Epoch[9/10](11920/14999) || training loss 0.007363 || training accuracy 100.00% || lr 1.6477356344906012e-07\n","Epoch[9/10](11940/14999) || training loss 0.04304 || training accuracy 98.75% || lr 1.6370325774299284e-07\n","Epoch[9/10](11960/14999) || training loss 0.03608 || training accuracy 98.75% || lr 1.6263295203692554e-07\n","Epoch[9/10](11980/14999) || training loss 0.04426 || training accuracy 98.12% || lr 1.6156264633085826e-07\n","Epoch[9/10](12000/14999) || training loss 0.0774 || training accuracy 98.12% || lr 1.6049234062479095e-07\n","Epoch[9/10](12020/14999) || training loss 0.05255 || training accuracy 98.75% || lr 1.5942203491872367e-07\n","Epoch[9/10](12040/14999) || training loss 0.08806 || training accuracy 98.75% || lr 1.5835172921265637e-07\n","Epoch[9/10](12060/14999) || training loss 0.07424 || training accuracy 98.12% || lr 1.5728142350658906e-07\n","Epoch[9/10](12080/14999) || training loss 0.01105 || training accuracy 99.38% || lr 1.5621111780052176e-07\n","Epoch[9/10](12100/14999) || training loss 0.03034 || training accuracy 98.75% || lr 1.5514081209445448e-07\n","Epoch[9/10](12120/14999) || training loss 0.02035 || training accuracy 100.00% || lr 1.5407050638838717e-07\n","Epoch[9/10](12140/14999) || training loss 0.03133 || training accuracy 98.75% || lr 1.530002006823199e-07\n","Epoch[9/10](12160/14999) || training loss 0.005948 || training accuracy 100.00% || lr 1.5192989497625259e-07\n","Epoch[9/10](12180/14999) || training loss 0.02464 || training accuracy 99.38% || lr 1.508595892701853e-07\n","Epoch[9/10](12200/14999) || training loss 0.005887 || training accuracy 100.00% || lr 1.49789283564118e-07\n","Epoch[9/10](12220/14999) || training loss 0.06839 || training accuracy 98.12% || lr 1.4871897785805072e-07\n","Epoch[9/10](12240/14999) || training loss 0.03159 || training accuracy 98.75% || lr 1.476486721519834e-07\n","Epoch[9/10](12260/14999) || training loss 0.06539 || training accuracy 97.50% || lr 1.465783664459161e-07\n","Epoch[9/10](12280/14999) || training loss 0.01389 || training accuracy 100.00% || lr 1.455080607398488e-07\n","Epoch[9/10](12300/14999) || training loss 0.005091 || training accuracy 100.00% || lr 1.4443775503378152e-07\n","Epoch[9/10](12320/14999) || training loss 0.003674 || training accuracy 100.00% || lr 1.4336744932771422e-07\n","Epoch[9/10](12340/14999) || training loss 0.09313 || training accuracy 96.25% || lr 1.4229714362164694e-07\n","Epoch[9/10](12360/14999) || training loss 0.05506 || training accuracy 97.50% || lr 1.4122683791557963e-07\n","Epoch[9/10](12380/14999) || training loss 0.004509 || training accuracy 100.00% || lr 1.4015653220951235e-07\n","Epoch[9/10](12400/14999) || training loss 0.03284 || training accuracy 99.38% || lr 1.3908622650344505e-07\n","Epoch[9/10](12420/14999) || training loss 0.06605 || training accuracy 98.75% || lr 1.3801592079737774e-07\n","Epoch[9/10](12440/14999) || training loss 0.0127 || training accuracy 99.38% || lr 1.3694561509131044e-07\n","Epoch[9/10](12460/14999) || training loss 0.005548 || training accuracy 100.00% || lr 1.3587530938524316e-07\n","Epoch[9/10](12480/14999) || training loss 0.03229 || training accuracy 98.12% || lr 1.3480500367917585e-07\n","Epoch[9/10](12500/14999) || training loss 0.04843 || training accuracy 98.75% || lr 1.3373469797310857e-07\n","Epoch[9/10](12520/14999) || training loss 0.05323 || training accuracy 99.38% || lr 1.3266439226704127e-07\n","Epoch[9/10](12540/14999) || training loss 0.03097 || training accuracy 98.12% || lr 1.31594086560974e-07\n","Epoch[9/10](12560/14999) || training loss 0.01888 || training accuracy 99.38% || lr 1.3052378085490668e-07\n","Epoch[9/10](12580/14999) || training loss 0.02239 || training accuracy 98.75% || lr 1.294534751488394e-07\n","Epoch[9/10](12600/14999) || training loss 0.02418 || training accuracy 99.38% || lr 1.2838316944277207e-07\n","Epoch[9/10](12620/14999) || training loss 0.009646 || training accuracy 100.00% || lr 1.273128637367048e-07\n","Epoch[9/10](12640/14999) || training loss 0.116 || training accuracy 96.88% || lr 1.2624255803063749e-07\n","Epoch[9/10](12660/14999) || training loss 0.02489 || training accuracy 99.38% || lr 1.251722523245702e-07\n","Epoch[9/10](12680/14999) || training loss 0.02698 || training accuracy 98.75% || lr 1.241019466185029e-07\n","Epoch[9/10](12700/14999) || training loss 0.007086 || training accuracy 100.00% || lr 1.230316409124356e-07\n","Epoch[9/10](12720/14999) || training loss 0.005154 || training accuracy 100.00% || lr 1.2196133520636831e-07\n","Epoch[9/10](12740/14999) || training loss 0.01618 || training accuracy 99.38% || lr 1.20891029500301e-07\n","Epoch[9/10](12760/14999) || training loss 0.03033 || training accuracy 98.12% || lr 1.1982072379423373e-07\n","Epoch[9/10](12780/14999) || training loss 0.007833 || training accuracy 99.38% || lr 1.1875041808816642e-07\n","Epoch[9/10](12800/14999) || training loss 0.03612 || training accuracy 98.75% || lr 1.1768011238209913e-07\n","Epoch[9/10](12820/14999) || training loss 0.0382 || training accuracy 99.38% || lr 1.1660980667603184e-07\n","Epoch[9/10](12840/14999) || training loss 0.01519 || training accuracy 100.00% || lr 1.1553950096996453e-07\n","Epoch[9/10](12860/14999) || training loss 0.1354 || training accuracy 97.50% || lr 1.1446919526389724e-07\n","Epoch[9/10](12880/14999) || training loss 0.0158 || training accuracy 100.00% || lr 1.1339888955782995e-07\n","Epoch[9/10](12900/14999) || training loss 0.02409 || training accuracy 98.75% || lr 1.1232858385176266e-07\n","Epoch[9/10](12920/14999) || training loss 0.0141 || training accuracy 99.38% || lr 1.1125827814569535e-07\n","Epoch[9/10](12940/14999) || training loss 0.0204 || training accuracy 99.38% || lr 1.1018797243962806e-07\n","Epoch[9/10](12960/14999) || training loss 0.02642 || training accuracy 99.38% || lr 1.0911766673356076e-07\n","Epoch[9/10](12980/14999) || training loss 0.03021 || training accuracy 98.75% || lr 1.0804736102749347e-07\n","Epoch[9/10](13000/14999) || training loss 0.0206 || training accuracy 99.38% || lr 1.0697705532142618e-07\n","Epoch[9/10](13020/14999) || training loss 0.00873 || training accuracy 100.00% || lr 1.0590674961535887e-07\n","Epoch[9/10](13040/14999) || training loss 0.01624 || training accuracy 100.00% || lr 1.0483644390929158e-07\n","Epoch[9/10](13060/14999) || training loss 0.01653 || training accuracy 99.38% || lr 1.0376613820322429e-07\n","Epoch[9/10](13080/14999) || training loss 0.1079 || training accuracy 98.12% || lr 1.02695832497157e-07\n","Epoch[9/10](13100/14999) || training loss 0.01475 || training accuracy 99.38% || lr 1.0162552679108969e-07\n","Epoch[9/10](13120/14999) || training loss 0.04957 || training accuracy 99.38% || lr 1.005552210850224e-07\n","Epoch[9/10](13140/14999) || training loss 0.008846 || training accuracy 99.38% || lr 9.94849153789551e-08\n","Epoch[9/10](13160/14999) || training loss 0.09433 || training accuracy 97.50% || lr 9.841460967288781e-08\n","Epoch[9/10](13180/14999) || training loss 0.04817 || training accuracy 98.12% || lr 9.734430396682052e-08\n","Epoch[9/10](13200/14999) || training loss 0.009383 || training accuracy 100.00% || lr 9.627399826075321e-08\n","Epoch[9/10](13220/14999) || training loss 0.02383 || training accuracy 98.75% || lr 9.520369255468592e-08\n","Epoch[9/10](13240/14999) || training loss 0.02422 || training accuracy 99.38% || lr 9.413338684861863e-08\n","Epoch[9/10](13260/14999) || training loss 0.01038 || training accuracy 99.38% || lr 9.306308114255134e-08\n","Epoch[9/10](13280/14999) || training loss 0.0042 || training accuracy 100.00% || lr 9.199277543648403e-08\n","Epoch[9/10](13300/14999) || training loss 0.06078 || training accuracy 98.12% || lr 9.092246973041674e-08\n","Epoch[9/10](13320/14999) || training loss 0.02104 || training accuracy 99.38% || lr 8.985216402434945e-08\n","Epoch[9/10](13340/14999) || training loss 0.01231 || training accuracy 100.00% || lr 8.878185831828215e-08\n","Epoch[9/10](13360/14999) || training loss 0.08367 || training accuracy 97.50% || lr 8.771155261221486e-08\n","Epoch[9/10](13380/14999) || training loss 0.02868 || training accuracy 99.38% || lr 8.664124690614756e-08\n","Epoch[9/10](13400/14999) || training loss 0.03452 || training accuracy 99.38% || lr 8.557094120008026e-08\n","Epoch[9/10](13420/14999) || training loss 0.08216 || training accuracy 98.12% || lr 8.450063549401297e-08\n","Epoch[9/10](13440/14999) || training loss 0.03004 || training accuracy 98.75% || lr 8.343032978794568e-08\n","Epoch[9/10](13460/14999) || training loss 0.0302 || training accuracy 99.38% || lr 8.236002408187839e-08\n","Epoch[9/10](13480/14999) || training loss 0.04224 || training accuracy 98.12% || lr 8.128971837581108e-08\n","Epoch[9/10](13500/14999) || training loss 0.03708 || training accuracy 98.12% || lr 8.021941266974379e-08\n","Epoch[9/10](13520/14999) || training loss 0.02439 || training accuracy 99.38% || lr 7.91491069636765e-08\n","Epoch[9/10](13540/14999) || training loss 0.01438 || training accuracy 99.38% || lr 7.80788012576092e-08\n","Epoch[9/10](13560/14999) || training loss 0.04626 || training accuracy 99.38% || lr 7.70084955515419e-08\n","Epoch[9/10](13580/14999) || training loss 0.03196 || training accuracy 98.75% || lr 7.59381898454746e-08\n","Epoch[9/10](13600/14999) || training loss 0.01996 || training accuracy 99.38% || lr 7.486788413940731e-08\n","Epoch[9/10](13620/14999) || training loss 0.05892 || training accuracy 98.75% || lr 7.379757843334002e-08\n","Epoch[9/10](13640/14999) || training loss 0.06535 || training accuracy 98.75% || lr 7.272727272727273e-08\n","Epoch[9/10](13660/14999) || training loss 0.01138 || training accuracy 100.00% || lr 7.165696702120542e-08\n","Epoch[9/10](13680/14999) || training loss 0.1389 || training accuracy 98.12% || lr 7.058666131513813e-08\n","Epoch[9/10](13700/14999) || training loss 0.009927 || training accuracy 100.00% || lr 6.951635560907084e-08\n","Epoch[9/10](13720/14999) || training loss 0.0422 || training accuracy 98.75% || lr 6.844604990300354e-08\n","Epoch[9/10](13740/14999) || training loss 0.0173 || training accuracy 99.38% || lr 6.737574419693624e-08\n","Epoch[9/10](13760/14999) || training loss 0.01552 || training accuracy 100.00% || lr 6.630543849086894e-08\n","Epoch[9/10](13780/14999) || training loss 0.03361 || training accuracy 98.75% || lr 6.523513278480165e-08\n","Epoch[9/10](13800/14999) || training loss 0.01498 || training accuracy 99.38% || lr 6.416482707873436e-08\n","Epoch[9/10](13820/14999) || training loss 0.03251 || training accuracy 98.75% || lr 6.309452137266707e-08\n","Epoch[9/10](13840/14999) || training loss 0.05371 || training accuracy 98.12% || lr 6.202421566659977e-08\n","Epoch[9/10](13860/14999) || training loss 0.02276 || training accuracy 99.38% || lr 6.095390996053248e-08\n","Epoch[9/10](13880/14999) || training loss 0.01853 || training accuracy 100.00% || lr 5.988360425446518e-08\n","Epoch[9/10](13900/14999) || training loss 0.01773 || training accuracy 99.38% || lr 5.8813298548397884e-08\n","Epoch[9/10](13920/14999) || training loss 0.0117 || training accuracy 100.00% || lr 5.774299284233059e-08\n","Epoch[9/10](13940/14999) || training loss 0.01502 || training accuracy 100.00% || lr 5.66726871362633e-08\n","Epoch[9/10](13960/14999) || training loss 0.006552 || training accuracy 100.00% || lr 5.5602381430196e-08\n","Epoch[9/10](13980/14999) || training loss 0.01221 || training accuracy 100.00% || lr 5.453207572412871e-08\n","Epoch[9/10](14000/14999) || training loss 0.05996 || training accuracy 98.12% || lr 5.346177001806141e-08\n","Epoch[9/10](14020/14999) || training loss 0.05827 || training accuracy 98.75% || lr 5.2391464311994115e-08\n","Epoch[9/10](14040/14999) || training loss 0.01552 || training accuracy 99.38% || lr 5.1321158605926816e-08\n","Epoch[9/10](14060/14999) || training loss 0.03092 || training accuracy 99.38% || lr 5.0250852899859524e-08\n","Epoch[9/10](14080/14999) || training loss 0.01167 || training accuracy 99.38% || lr 4.918054719379223e-08\n","Epoch[9/10](14100/14999) || training loss 0.1436 || training accuracy 96.88% || lr 4.811024148772493e-08\n","Epoch[9/10](14120/14999) || training loss 0.01573 || training accuracy 100.00% || lr 4.703993578165764e-08\n","Epoch[9/10](14140/14999) || training loss 0.03936 || training accuracy 98.12% || lr 4.596963007559034e-08\n","Epoch[9/10](14160/14999) || training loss 0.04303 || training accuracy 98.75% || lr 4.489932436952305e-08\n","Epoch[9/10](14180/14999) || training loss 0.08701 || training accuracy 98.75% || lr 4.382901866345575e-08\n","Epoch[9/10](14200/14999) || training loss 0.03798 || training accuracy 98.12% || lr 4.2758712957388456e-08\n","Epoch[9/10](14220/14999) || training loss 0.04652 || training accuracy 98.12% || lr 4.168840725132116e-08\n","Epoch[9/10](14240/14999) || training loss 0.01426 || training accuracy 98.75% || lr 4.0618101545253865e-08\n","Epoch[9/10](14260/14999) || training loss 0.004876 || training accuracy 100.00% || lr 3.954779583918657e-08\n","Epoch[9/10](14280/14999) || training loss 0.04883 || training accuracy 98.75% || lr 3.847749013311927e-08\n","Epoch[9/10](14300/14999) || training loss 0.01361 || training accuracy 99.38% || lr 3.740718442705198e-08\n","Epoch[9/10](14320/14999) || training loss 0.03254 || training accuracy 98.75% || lr 3.633687872098468e-08\n","Epoch[9/10](14340/14999) || training loss 0.05206 || training accuracy 98.12% || lr 3.526657301491739e-08\n","Epoch[9/10](14360/14999) || training loss 0.02028 || training accuracy 99.38% || lr 3.419626730885009e-08\n","Epoch[9/10](14380/14999) || training loss 0.006329 || training accuracy 100.00% || lr 3.31259616027828e-08\n","Epoch[9/10](14400/14999) || training loss 0.03062 || training accuracy 98.12% || lr 3.2055655896715504e-08\n","Epoch[9/10](14420/14999) || training loss 0.01073 || training accuracy 100.00% || lr 3.0985350190648205e-08\n","Epoch[9/10](14440/14999) || training loss 0.04622 || training accuracy 99.38% || lr 2.9915044484580906e-08\n","Epoch[9/10](14460/14999) || training loss 0.07079 || training accuracy 97.50% || lr 2.884473877851361e-08\n","Epoch[9/10](14480/14999) || training loss 0.09563 || training accuracy 97.50% || lr 2.7774433072446318e-08\n","Epoch[9/10](14500/14999) || training loss 0.05064 || training accuracy 98.75% || lr 2.6704127366379022e-08\n","Epoch[9/10](14520/14999) || training loss 0.04948 || training accuracy 98.75% || lr 2.5633821660311726e-08\n","Epoch[9/10](14540/14999) || training loss 0.03352 || training accuracy 97.50% || lr 2.456351595424443e-08\n","Epoch[9/10](14560/14999) || training loss 0.07242 || training accuracy 96.88% || lr 2.3493210248177134e-08\n","Epoch[9/10](14580/14999) || training loss 0.04022 || training accuracy 99.38% || lr 2.242290454210984e-08\n","Epoch[9/10](14600/14999) || training loss 0.01524 || training accuracy 99.38% || lr 2.1352598836042543e-08\n","Epoch[9/10](14620/14999) || training loss 0.006104 || training accuracy 100.00% || lr 2.0282293129975247e-08\n","Epoch[9/10](14640/14999) || training loss 0.04384 || training accuracy 98.12% || lr 1.9211987423907954e-08\n","Epoch[9/10](14660/14999) || training loss 0.08234 || training accuracy 97.50% || lr 1.814168171784066e-08\n","Epoch[9/10](14680/14999) || training loss 0.0875 || training accuracy 98.12% || lr 1.7071376011773363e-08\n","Epoch[9/10](14700/14999) || training loss 0.03164 || training accuracy 99.38% || lr 1.6001070305706067e-08\n","Epoch[9/10](14720/14999) || training loss 0.03739 || training accuracy 98.75% || lr 1.493076459963877e-08\n","Epoch[9/10](14740/14999) || training loss 0.008309 || training accuracy 100.00% || lr 1.3860458893571475e-08\n","Epoch[9/10](14760/14999) || training loss 0.008503 || training accuracy 100.00% || lr 1.2790153187504181e-08\n","Epoch[9/10](14780/14999) || training loss 0.01912 || training accuracy 99.38% || lr 1.1719847481436885e-08\n","Epoch[9/10](14800/14999) || training loss 0.01062 || training accuracy 100.00% || lr 1.064954177536959e-08\n","Epoch[9/10](14820/14999) || training loss 0.01976 || training accuracy 99.38% || lr 9.579236069302294e-09\n","Epoch[9/10](14840/14999) || training loss 0.01741 || training accuracy 99.38% || lr 8.508930363235e-09\n","Epoch[9/10](14860/14999) || training loss 0.006234 || training accuracy 100.00% || lr 7.4386246571677036e-09\n","Epoch[9/10](14880/14999) || training loss 0.01837 || training accuracy 98.75% || lr 6.368318951100408e-09\n","Epoch[9/10](14900/14999) || training loss 0.04035 || training accuracy 99.38% || lr 5.298013245033113e-09\n","Epoch[9/10](14920/14999) || training loss 0.05244 || training accuracy 98.75% || lr 4.227707538965817e-09\n","Epoch[9/10](14940/14999) || training loss 0.04689 || training accuracy 99.38% || lr 3.1574018328985215e-09\n","Epoch[9/10](14960/14999) || training loss 0.009386 || training accuracy 100.00% || lr 2.087096126831226e-09\n","Epoch[9/10](14980/14999) || training loss 0.01391 || training accuracy 100.00% || lr 1.0167904207639306e-09\n","Calculating validation results...\n","100% 235/235 [02:22<00:00,  1.65it/s]\n","[Val] acc : 90.96%, loss: 0.4431, F1 : 0.9096 || best acc : 91.14%, best loss: 0.2442\n","Time elapsed:  666.65 min\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"i9UT6AQYxYKE"},"source":["import torch\n","import torch.nn as nn\n","from transformers import BertModel, BertPreTrainedModel, ElectraModel, ElectraPreTrainedModel, XLMRobertaModel, RobertaModel, BartModel, BartPretrainedModel, PreTrainedModel\n","import transformers\n","from transformers import AutoTokenizer\n","\n","class FCLayer(nn.Module):\n","    def __init__(self, input_dim, output_dim, dropout_rate=0.0, use_activation=True):\n","        super(FCLayer, self).__init__()\n","        self.use_activation = use_activation\n","        self.dropout = nn.Dropout(dropout_rate)\n","        self.linear = nn.Linear(input_dim, output_dim)\n","        self.tanh = nn.Tanh()\n","\n","    def forward(self, x):\n","        x = self.dropout(x)\n","        if self.use_activation:\n","            x = self.tanh(x)\n","        return self.linear(x)\n","\n","class xlmRoBerta(XLMRobertaModel):\n","    def __init__(self, config, args):\n","        super(xlmRoBerta, self).__init__(config)\n","        self.xlmroberta = XLMRobertaModel.from_pretrained(\"xlm-roberta-large\", config=config)  # Load pretrained Electra\n","\n","        self.num_labels = config.num_labels\n","\n","        self.cls_fc_layer = FCLayer(config.hidden_size, config.hidden_size, args.dropout_rate)\n","        # l2 norm, similarity add\n","        self.label_classifier = FCLayer(\n","            config.hidden_size,\n","            config.num_labels,\n","            args.dropout_rate,\n","            use_activation=False,\n","        )\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n","        outputs = self.xlmroberta(\n","            input_ids, attention_mask=attention_mask\n","        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n","        sequence_output = outputs[0]\n","        pooled_output = outputs[0][:, 0, :]  # [CLS]\n","\n","        # Average\n","        pooled_output = self.cls_fc_layer(pooled_output)\n","\n","        # Concat -> fc_layer\n","        logits = self.label_classifier(pooled_output)\n","        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n","\n","        # Softmax\n","        if labels is not None:\n","            if self.num_labels == 1:\n","                loss_fct = nn.MSELoss()\n","                loss = loss_fct(logits.view(-1), labels.view(-1))\n","            else:\n","                loss_fct = nn.CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","\n","            outputs = (loss,) + outputs\n","\n","        return outputs  # (loss), logits, (hidden_states), (attentions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JTR68DjYzN3N"},"source":["import easydict"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z0elUUoWxy16"},"source":["args = easydict.EasyDict({ \"dropout_rate\": 0 })"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139,"referenced_widgets":["9a28c34e865f48a2a1573dba5ee0c1e8","479780f7957b4521b6e07808aa7026aa","95b16ef6acfb43c1b0b995e41ee6962e","7ab962dc0f7c4889a5211c82e300156e","4a2a9f00600e4fafaba9f2bdbf55e150","6a8b505f65c745efbb719541bf494af0","38268af62ee943178225e226eb3b8673","8b9eb099ecd1495594d6a7bd278267ae"]},"id":"2Bul5eXHmjSx","executionInfo":{"status":"ok","timestamp":1622637917746,"user_tz":-540,"elapsed":103634,"user":{"displayName":"서석민_T1092","photoUrl":"","userId":"11541575103523287051"}},"outputId":"d252faa0-52b4-4053-de60-09a1334831f3"},"source":["model = xlmRoBerta.from_pretrained('/content/drive/MyDrive/sentimental_analisis/good-news-sentimental-analysis/results/xlm_batch8_kfold_1/1/best', args=args)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a28c34e865f48a2a1573dba5ee0c1e8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2244861551.0, style=ProgressStyle(descr…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n","- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"f02qtWWN08d8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622637924880,"user_tz":-540,"elapsed":7141,"user":{"displayName":"서석민_T1092","photoUrl":"","userId":"11541575103523287051"}},"outputId":"ee8dca05-05e2-4be6-d856-7079b634e4cc"},"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","model.to(device)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["xlmRoBerta(\n","  (embeddings): RobertaEmbeddings(\n","    (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n","    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n","    (token_type_embeddings): Embedding(1, 1024)\n","    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): RobertaEncoder(\n","    (layer): ModuleList(\n","      (0): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (12): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (13): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (14): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (15): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (16): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (17): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (18): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (19): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (20): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (21): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (22): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (23): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): RobertaPooler(\n","    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","    (activation): Tanh()\n","  )\n","  (xlmroberta): XLMRobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n","      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 1024)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (12): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (13): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (14): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (15): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (16): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (17): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (18): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (19): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (20): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (21): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (22): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (23): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): RobertaPooler(\n","      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (cls_fc_layer): FCLayer(\n","    (dropout): Dropout(p=0, inplace=False)\n","    (linear): Linear(in_features=1024, out_features=1024, bias=True)\n","    (tanh): Tanh()\n","  )\n","  (label_classifier): FCLayer(\n","    (dropout): Dropout(p=0, inplace=False)\n","    (linear): Linear(in_features=1024, out_features=2, bias=True)\n","    (tanh): Tanh()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"u39XI48Kx8mD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622637924882,"user_tz":-540,"elapsed":30,"user":{"displayName":"서석민_T1092","photoUrl":"","userId":"11541575103523287051"}},"outputId":"de5dd3ea-7ab2-470d-933c-32ba66f449e3"},"source":["model.eval()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["xlmRoBerta(\n","  (embeddings): RobertaEmbeddings(\n","    (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n","    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n","    (token_type_embeddings): Embedding(1, 1024)\n","    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (encoder): RobertaEncoder(\n","    (layer): ModuleList(\n","      (0): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (1): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (2): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (3): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (4): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (5): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (6): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (7): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (8): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (9): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (10): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (11): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (12): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (13): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (14): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (15): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (16): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (17): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (18): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (19): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (20): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (21): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (22): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","      (23): RobertaLayer(\n","        (attention): RobertaAttention(\n","          (self): RobertaSelfAttention(\n","            (query): Linear(in_features=1024, out_features=1024, bias=True)\n","            (key): Linear(in_features=1024, out_features=1024, bias=True)\n","            (value): Linear(in_features=1024, out_features=1024, bias=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (output): RobertaSelfOutput(\n","            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (intermediate): RobertaIntermediate(\n","          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","        )\n","        (output): RobertaOutput(\n","          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","  )\n","  (pooler): RobertaPooler(\n","    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","    (activation): Tanh()\n","  )\n","  (xlmroberta): XLMRobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n","      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 1024)\n","      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (12): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (13): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (14): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (15): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (16): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (17): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (18): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (19): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (20): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (21): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (22): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (23): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=1024, out_features=1024, bias=True)\n","              (key): Linear(in_features=1024, out_features=1024, bias=True)\n","              (value): Linear(in_features=1024, out_features=1024, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n","            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): RobertaPooler(\n","      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (cls_fc_layer): FCLayer(\n","    (dropout): Dropout(p=0, inplace=False)\n","    (linear): Linear(in_features=1024, out_features=1024, bias=True)\n","    (tanh): Tanh()\n","  )\n","  (label_classifier): FCLayer(\n","    (dropout): Dropout(p=0, inplace=False)\n","    (linear): Linear(in_features=1024, out_features=2, bias=True)\n","    (tanh): Tanh()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"PqnpqG-O29qO","colab":{"base_uri":"https://localhost:8080/","height":166,"referenced_widgets":["d45d43811dc84561bcf88ea56a15033b","f6b2bcf9ec8f40068c9c83e2f0bd5b82","f64af2e9460d4ab5b86a8e5c327e4744","6bb1a68c0911478b9c1646a5e5a37284","bfa2097d9c5544709b681bac0a686d1c","4e179c9d33424d189f571785c3538717","4e67dbb28c484d62b87d7699416d0b17","10b97ca6aca7451b80950efda4619a7e","34c9d405273b49c0afe747a4fc99e932","6715d27dee0e4a118f4523b0d95d2a47","88648a7cc10c4d9582f0fe51e68a2ffb","e73c8afb120340e5b782f8ecee83151f","c8205c97127c4ae397e0a9eb2eec224d","fd193de0a7414d28ace670a9f0eb6027","77543ee5a5224d9b8f9b5f0efc7e608b","370479ef2ba44910b5bebb8bef8d1d9a","8bfa0f30b63149c3b1dd4134ab56ab37","fcf74716b6fd4e4dbe2360fac14cff6f","c8684eca7a1d49dfbb58321390e7806f","cbb3bc5ba56e4edaa0e0b8f86f1089e0","4154908224554006a46f8b6c62d4c567","0e58ca2a015c4918a8c6537ac9ba753b","61c65e5394af4fbc8c88da93b7ef18a4","1a1385b9a775467f9019a427ae2c0884"]},"executionInfo":{"status":"ok","timestamp":1622637927538,"user_tz":-540,"elapsed":2662,"user":{"displayName":"서석민_T1092","photoUrl":"","userId":"11541575103523287051"}},"outputId":"8b488608-9f13-4c53-e1c8-313065ed0e86"},"source":["tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-large\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d45d43811dc84561bcf88ea56a15033b","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=513.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34c9d405273b49c0afe747a4fc99e932","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=5069051.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8bfa0f30b63149c3b1dd4134ab56ab37","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=9096718.0, style=ProgressStyle(descript…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hyQN9Y6H2-eF"},"source":["qurry = ['이런 기사 쓸시간 있으면 잠이나 자라.',\n","         '마음이 따뜻해지는 기사네요.',\n","         '어휴 진짜 아침부터 이런 글 읽으니깐 기분 안좋네요',\n","         '사랑이 싹트는 기사네요',\n","         '와 앞으로 어떻게 진행될지 지켜봐야겠어요',\n","         '뻔하네요',\n","         'fun 하네요',\n","         '펀하네요'\n","         ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yTajUI1Z4yL5"},"source":["qurry = ['와 앞으로 어떻게 진행될지 지켜봐야겠어요',\n","         '뻔하네요',\n","         'fun 하네요',\n","         '펀하네요',\n","         '내가 이럴 줄 알았다']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7A89mLk5X3l"},"source":["qurry = ['살면서 저런 돌I들 안만나게해주세요...',\n","         '그 손주란 애가 불쌍하네요.',\n","         '저런할미랑 애미밑에서 뭘 보고 배울지..',\n","         '애미도 보니 지엄만지 시몬지 몰라도 보고 배운게 딱 수준이 같네요...',\n","         '목사는 길거리의 똥강아지도 하는 것인줄 이제야 바로 알았다 ',\n","         '개나 소나 다 목사 하는게 우리나라구나 ',\n","         '우리나라로 돌아와 값 싼 노동만 원하다가는 큰 코 다쳐',\n","         '베트남이 이렇게 뒷통수를 칠줄이야.. 정말 몰랐네']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bZHniKcv50D3"},"source":["qurry = ['사장님의 마음과 \"사장님처럼 어려운 사람을 도와주는 멋있는 사람이 되겠다”는 다짐과 함께 그동안 동생을 챙겨준 사장님께 감사의 인사를 전한 형의 행동이 너무 아름답네요.저도 이 어려운 시국에 이웃을 도와주며 살아가야겠습니다.마음이 따뜻해지는 기사였습니다.',\n","         '치킨을 너무 먹고 싶어하는 초등학생 동생을 위해 오천어치만 시켜야했던 형의 가슴아픈 사연에 마음이 너무 아팠어요ㅠㅠ 하지만 사장님이 배불리 치킨을 먹게해주시고 그런 따뜻한 마음이 너무 감사했던 형의 편지로 온세상에 알려지게 됐네요.']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oJltcHNf6FuW"},"source":["qurry = ['계속 퇴보하다가 어디까지 가게되는 걸까?']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N75mWIbb83L2"},"source":["qurry = ['무섭기만 하다']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZMPgGyeMwDXG"},"source":["qurry = ['정부합동특별수사본부(합수본)는 2일 정부서울청사에서 수사 중간 결과 브리핑을 열고 지난 3월 10일 출범부터 이날까지 총 646건의 사건과 관련해, 2796명을 내·수사해 20명을 구속했다고 밝혔다. 또 529명을 검찰에 송치했다. 경찰 중심의 합수본은 1560여명의 규모로 구성됐다. 검찰에도 600여 명의 전담수사팀을 편성했다. 또 금융위원회, 금융감독원, 국세청은 부동산 시장에서의 편법대출과 불법 탈세를 살폈다.',\n","         '합수본은 국회의원의 부동산 관련 뇌물수수, 차명거래 혐의, LH 및 서울주택도시공사(SH) 직원들의 뇌물수수 혐의에 대해서도 수사를 진행 중이다. 특히 LH의 경우 경찰은 직원 77명과 친인척·지인 74명등 151명을 적발해 현재까지 4명을 구속하고 126명에 대해 계속 수사를 진행하고 있다.',\n","         '해당 영상은 코로나19가 발생하기 전인 지난 2017년 12월 29일 중국중앙방송(CCTV)을 통해 공개된 영상이다. 이 영상에는 박쥐한테 물린 자국이 심하게 부풀어 오른 사진도 포함됐다. 영상에 등장한 연구진은 박쥐가 자신의 장갑을 뚫고 물었다며 “바늘에 찔린 기분”이라고 설명했다.',\n","         \"이용구 법무부 차관이 차관에 임명되기 한 달 전쯤인 지난해 11월, 술에 취해 택시기사를 폭행하는 장면이 담긴 영상을 SBS가 단독으로 입수했습니다. 37초짜리 이 영상에는 이용구 차관이 택시기사의 목을 조르고 또 욕을 하는 모습이 고스란히 담겨 있습니다. 이용구 차관은 지난주 사표를 냈지만 아직 수리되지 않아서 지금도 '법무부 차관' 신분입니다. 그럼 먼저 지난해 11월 6일 밤, 이용구 차관이 타고 있던 택시 안에서 무슨 일이 있었는지부터 함께 보시겠습니다.\",\n","         '이씨는 지난해 7월 11일 성남시의 한 가정집에서 전 여자친구 A씨를 흉기로 여러 차례 찔러 살해한 혐의로 기소됐다. A씨의 승용차와 신용카드를 훔쳐 달아난 혐의도 받고 있다. 이씨는 범행 후 A씨의 차를 몰고 전남 고흥으로 도주해 극단적 선택을 시도했지만 출동한 경찰에 체포돼 목숨을 건졌다.',\n","         ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FfhfBQg7w0VF"},"source":["qurry = ['불이 난 건물과 이웃한 건물은 지은 지 오래돼 스프링클러나 화재 경보 장치가 없던 상황.30m 떨어진 곳에는 주유소까지 있어서 불이 번져 불티라도 튀었으면 더 큰 참사가 발생할 수도 있었습니다.이 때문에 주변 상인들은 김 씨가 큰불을 막았다며 은인이라며 고맙다고 말합니다.[주변 상인 : 고맙다고 내가 인사를 해야 하는데, 너무 고맙습니다. 그때 그렇게 해주셨으니까 빨리 불이 꺼졌었지.]화재 초기에 소화기 1대의 위력이 얼마나 강력한지 몸소 보여준 김 씨.정작 김 씨는 코로나19 때문에 힘든 상인들을 걱정하는 마음에 당연히 한 일이라며 겸손해했습니다.[김상남 / 화재 진압 버스 기사 : 건물마저 타면 그분들이 진짜 큰일 나겠구나 하는 마음을 갖고 달려든 것 같습니다.]',\n","         \"'대구 키다리 아저씨'가 올해도 이름을 숨긴 채 5천여만 원을 기부했습니다.대구사회복지공동모금회는 10년 가까이 익명으로 기부해온 이른바 '키다리 아저씨'가 올해도 5천여만 원을 기부했다고 밝혔습니다.키다리 아저씨 부부는 저녁 식사를 함께하자며 모금회 직원을 초대했고, 이 자리에서 5천4만 원짜리 수표와 메모지가 든 봉투를 건넸습니다.\",\n","         '전주시는 얼굴없는 천사의 뜻을 기리고 아름다운 기부문화가 확산하도록 노송동주민센터 화단에 “당신은 어둠 속의 촛불처럼 세상을 밝고 아름답게 만드는 참사람입니다. 사랑합니다”라는 글귀가 새겨진 ‘얼굴없는 천사의 비’를 2009년 12월 세웠다. 주민들은 얼굴없는 천사의 선행을 본받자는 뜻에서 숫자 천사(1004)를 본따서 10월4일을 ‘천사의 날’로 정하고 불우이웃을 돕고 있다. ',\n","         ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h6yneol527_D"},"source":["tokenized_sentences = tokenizer(\n","    qurry,\n","    return_tensors=\"pt\",\n","    padding=True,\n","    truncation=True,\n","    max_length=50,\n","    add_special_tokens=True,\n","    return_token_type_ids = True\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_GcjVNVczFnP"},"source":["output = model(input_ids=tokenized_sentences['input_ids'].to(device),\n","      attention_mask=tokenized_sentences['attention_mask'].to(device),\n","      token_type_ids=tokenized_sentences['token_type_ids'].to(device),\n","      labels=None,\n","      )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DWOJy_Lb1SUt","executionInfo":{"status":"ok","timestamp":1622638203759,"user_tz":-540,"elapsed":15,"user":{"displayName":"서석민_T1092","photoUrl":"","userId":"11541575103523287051"}},"outputId":"5b283fd0-5196-42a3-9acd-77398361fc95"},"source":["torch.argmax(output[0], 1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 1, 1], device='cuda:0')"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"jUjvcifT4dfz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622638214373,"user_tz":-540,"elapsed":239,"user":{"displayName":"서석민_T1092","photoUrl":"","userId":"11541575103523287051"}},"outputId":"3fdac1e2-392c-49ce-b6af-881b3104e26a"},"source":["torch.argmax(output[0], 1)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 1, 0, 1, 0], device='cuda:0')"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"gWDGbzUdxjpR"},"source":[""],"execution_count":null,"outputs":[]}]}